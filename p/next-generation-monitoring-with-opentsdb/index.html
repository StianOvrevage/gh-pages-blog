<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="2021 Update: The specific tools discussed in this blog post should be considered obsolete by todays standards. You should investigate Prometheus, InfluxDB and TimescaleDB for your monitoring needs. In this paper we will provide a step by step guide on how to install a single-instance of OpenTSDB using the latest versions of the underlying technology, Hadoop and HBase. We will also provide some background on the state of existing monitoring solutions.\n"><title>Next generation monitoring with OpenTSDB</title>
<link rel=canonical href=https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Next generation monitoring with OpenTSDB"><meta property='og:description' content="2021 Update: The specific tools discussed in this blog post should be considered obsolete by todays standards. You should investigate Prometheus, InfluxDB and TimescaleDB for your monitoring needs. In this paper we will provide a step by step guide on how to install a single-instance of OpenTSDB using the latest versions of the underlying technology, Hadoop and HBase. We will also provide some background on the state of existing monitoring solutions.\n"><meta property='og:url' content='https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/'><meta property='og:site_name' content='blog.stian.omg.lol'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='observability'><meta property='article:published_time' content='2014-06-02T19:56:40+00:00'><meta property='article:modified_time' content='2014-06-02T19:56:40+00:00'><meta property='og:image' content='https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure1.png'><meta name=twitter:title content="Next generation monitoring with OpenTSDB"><meta name=twitter:description content="2021 Update: The specific tools discussed in this blog post should be considered obsolete by todays standards. You should investigate Prometheus, InfluxDB and TimescaleDB for your monitoring needs. In this paper we will provide a step by step guide on how to install a single-instance of OpenTSDB using the latest versions of the underlying technology, Hadoop and HBase. We will also provide some background on the state of existing monitoring solutions.\n"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure1.png'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/profile-picture_hu6892409687065925510.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ü§∑‚Äç‚ôÇÔ∏è</span></figure><div class=site-meta><h1 class=site-name><a href=/>blog.stian.omg.lol</a></h1><h2 class=site-description>Technology. Aviation. Philosophy.</h2></div></header><ol class=menu-social><li><a href=https://github.com/StianOvrevage target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><ol><li><a href=#2021-update-the-specific-tools-discussed-in-this-blog-post-should-be-considered-obsolete-by-todays-standards-you-should-investigate-prometheus-influxdb-and-timescaledb-for-your-monitoring-needs>2021 Update: The specific tools discussed in this blog post should be considered obsolete by todays standards. You should investigate Prometheus, InfluxDB and TimescaleDB for your monitoring needs.</a></li></ol></li><li><a href=#background>Background</a><ol><li><a href=#performance-problems---welcome-to-io-hell>Performance problems - Welcome to I/O-hell</a></li><li><a href=#scaling-problems>Scaling problems</a></li><li><a href=#loss-of-detail>Loss of detail</a></li><li><a href=#lack-of-flexibility>Lack of flexibility</a></li></ol></li><li><a href=#the-monitoring-revolution>The monitoring revolution</a></li><li><a href=#setting-up-a-single-node-opentsdb-instance-on-debian-7-wheezy>Setting up a single node OpenTSDB instance on Debian 7 Wheezy</a><ol><li><ol><li><a href=#hardware-requirements>Hardware requirements</a></li><li><a href=#operating-system-requirements>Operating system requirements</a></li><li><a href=#pre-setup-preparations>Pre-setup preparations</a></li><li><a href=#installing-java-from-packages>Installing java from packages</a></li></ol></li><li><a href=#installing-hbase>Installing HBase</a><ol><li><a href=#install-snappy>Install snappy</a></li><li><a href=#building-native-libhadoop-and-libsnappy>Building native libhadoop and libsnappy</a></li><li><a href=#configuring-hbase>Configuring HBase</a></li><li><a href=#confhbase-envsh><strong>conf/hbase-env.sh</strong></a></li><li><a href=#confhbase-sitexml><strong>conf/hbase-site.xml</strong></a></li><li><a href=#testing-hbase-and-compression>Testing HBase and compression</a></li><li><a href=#starting-hbase>Starting HBase</a></li></ol></li><li><a href=#installing-opentsdb>Installing OpenTSDB</a><ol><li><a href=#configuring-opentsdb>Configuring OpenTSDB</a></li><li><a href=#creating-hbase-tables>Creating HBase tables</a></li><li><a href=#starting-opentsdb>Starting OpenTSDB</a></li></ol></li><li><a href=#feeding-data-into-opentsdb>Feeding data into OpenTSDB</a><ol><li><a href=#tcollector>tcollector</a></li><li><a href=#peritus-tc-tools>peritus-tc-tools</a></li><li><a href=#collectd-opentsdb>collectd-opentsdb</a></li><li><a href=#monitoring-opentsdb>Monitoring OpenTSDB</a></li></ol></li><li><a href=#performance-comparison>Performance comparison</a><ol><li><a href=#collection>Collection</a></li><li><a href=#storage>Storage</a></li><li><a href=#collectd--rrdcached>collectd + rrdcached</a></li><li><a href=#opentsdb--hbase-096--hadoop-1>OpenTSDB + Hbase 0.96 + Hadoop 1</a></li><li><a href=#opentsdb--hbase-098--hadoop-2>OpenTSDB + HBase 0.98 + Hadoop 2</a></li><li><a href=#conclusion>Conclusion</a></li></ol></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/next-generation-monitoring-with-opentsdb/><img src=/p/next-generation-monitoring-with-opentsdb/Figure1_hu8865876268263549244.png srcset="/p/next-generation-monitoring-with-opentsdb/Figure1_hu8865876268263549244.png 800w, /p/next-generation-monitoring-with-opentsdb/Figure1_hu17466521611456145226.png 1600w" width=800 height=368 loading=lazy alt="Featured image of post Next generation monitoring with OpenTSDB"></a></div><div class=article-details><header class=article-category><a href=/categories/technology/ style=background-color:#2a9d8f;color:#fff>Technology</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/next-generation-monitoring-with-opentsdb/>Next generation monitoring with OpenTSDB</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jun 02, 2014</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>18 minute read</time></div></footer></div></header><section class=article-content><h3 id=2021-update-the-specific-tools-discussed-in-this-blog-post-should-be-considered-obsolete-by-todays-standards-you-should-investigate-prometheus-influxdb-and-timescaledb-for-your-monitoring-needs>2021 Update: The specific tools discussed in this blog post should be considered obsolete by todays standards. You should investigate Prometheus, InfluxDB and TimescaleDB for your monitoring needs.</h3><p>In this paper we will provide a step by step guide on how to install a single-instance of <strong>OpenTSDB</strong> using the latest versions of the underlying technology, <strong>Hadoop</strong> and <strong>HBase</strong>. We will also provide some background on the state of existing monitoring solutions.</p><p><a id=Abstract></a></p><p>Table of contents</p><ul><li><a class=link href=#Abstract>Abstract</a></li><li><a class=link href=#Background>Background</a><ul><li><a class=link href=#Performanceproblems>Performance problems - Welcome to I/O-hell</a></li><li><a class=link href=#Scaling>Scaling problems</a></li><li><a class=link href=#Loss>Loss of detail</a></li><li><a class=link href=#flexibility>Lack of flexibility</a></li></ul></li><li><a class=link href=#revolution>The monitoring revolution</a></li><li><a class=link href=#Debian>Setting up a single node OpenTSDB instance on Debian 7 Wheezy</a><ul><li><a class=link href=#Hardware>Hardware requirements</a></li><li><a class=link href=#Operating>Operating system requirements</a></li><li><a class=link href=#preparations>Pre-setup preparations</a></li><li><a class=link href=#java>Installing java from packages</a></li><li><a class=link href=#HBase>Installing HBase</a><ul><li><a class=link href=#snappy>Install snappy</a></li><li><a class=link href=#native>Building native libhadoop and libsnappy</a></li><li><a class=link href=#ConfiguringHBase>Configuring HBase</a></li></ul></li><li><a class=link href=#compression>Testing HBase and compression</a></li><li><a class=link href=#StartingHBase>Starting HBase</a></li></ul></li><li><a class=link href=#InstallingOpenTSDB>Installing OpenTSDB</a><ul><li><a class=link href=#ConfiguringOpenTSDB>Configuring OpenTSDB</a></li><li><a class=link href=#HBasetables>Creating HBase tables</a></li><li><a class=link href=#StartingOpenTSDB>Starting OpenTSDB</a></li></ul></li><li><a class=link href=#Feeding>Feeding data into OpenTSDB</a><ul><li><a class=link href=#tcollector>tcollector</a></li><li><a class=link href=#peritus-tc-tools>peritus-tc-tools</a></li><li><a class=link href=#collectd-opentsdb>collectd-opentsdb</a></li><li><a class=link href=#MonitoringOpenTSDB>Monitoring OpenTSDB</a></li></ul></li><li><a class=link href=#Performancecomparison>Performance comparison</a><ul><li><a class=link href=#Collection>Collection</a></li><li><a class=link href=#Storage>Storage</a></li><li><a class=link href=#Conclusion>Conclusion</a></li></ul></li></ul><p><a id=Background></a></p><h2 id=background>Background</h2><p>Since its inception in 1999 <a class=link href=http://oss.oetiker.ch/rrdtool/ target=_blank rel=noopener><strong>rrdtool</strong></a> (the underlying storage mechanism of once universal <strong>MRTG</strong>) has been the base of many popular monitoring solutions; <strong>Cacti</strong>, <strong>collectd</strong>, <strong>Ganglia</strong>, <strong>Munin</strong>, <strong>Observium</strong>, <strong>OpenNMS</strong> and <strong>Zenoss</strong>, to name a few.</p><p>There are a number of problems with the current approach and we will highlight some of these here.</p><p>Please note that this includes <strong>Graphite</strong> and its backend <strong>Whisper</strong>, which is based on the <a class=link href=http://graphite.readthedocs.org/en/0.9.10/whisper.html target=_blank rel=noopener>same basic design as rrdtool</a> and has <a class=link href=http://dieter.plaetinck.be/on-graphite-whisper-and-influxdb.html target=_blank rel=noopener>some of the same limitations</a>.</p><p><a id=Performanceproblems></a></p><h3 id=performance-problems---welcome-to-io-hell>Performance problems - Welcome to I/O-hell</h3><p>When MRTG and rrdtool was created the preservation of disk space was more important than preservation of disk operations and the default collection interval was 5 minutes (which many are still using). The way rrdtool is designed it requires quite a few random reads and writes per datapoint. It also re-reads, computes the average, and writes old data again according to the RRA rules defined which causes additional I/O load. In 2014 memory is cheap, disk storage is cheap and CPU is fairly cheap. Disk I/O operations (IOPS) however are still very expensive in terms of hardware. The recent maturing of SSD provides extreme amounts of IOPS for a reasonable price, but the drive sizes are fractional. The result is that in order to scale IOPS-wise you currently need many low-space SSDs to get the required space, or many low-IOPS spindle drives to get the required IOPS:</p><p><a class=link href="http://www.newegg.com/Product/Product.aspx?Item=N82E16820147251" target=_blank rel=noopener>Samsung EVO 840 1TB SSD</a> - 98.000 IOPS - 470 USD</p><p><a class=link href="http://www.newegg.com/Product/Product.aspx?Item=N82E16822148844" target=_blank rel=noopener>Seagate Barracuda 3TB</a> - 240 IOPS - 110 USD</p><p>You would need $44.880 (408 drives) worth of spindle drives in order to match a single SSD drive in terms of I/O-performance. On the other hand a $2.000 array of spindle drives would get you a net ~54 TB of space. The cost of SSD to reach the same volume would be $25.380. Not to mention the cost of servers, power, provisioning, etc.</p><p><strong>Note: This is the cheapest available bulk consumer drives and comparable OEM drives (<a class=link href=http://h30094.www3.hp.com/product/sku/10350615/mfg_partno/632494-B21 target=_blank rel=noopener>SSD</a>, <a class=link href=http://h30094.www3.hp.com/product/sku/10389145/mfg_partno/628061-B21 target=_blank rel=noopener>spindle</a>) for a HP server will be 6 to 30 times more expensive.</strong></p><p>In rrdtool version 1.4, released in 2009, <strong>rrdcached</strong> was introduced as a caching daemon for buffering multiple data updates and reducing the number of random I/O operations by writing several related datapoints in sequence. It took a couple of years before this new feature was implemented in most of the common open source monitoring solutions.</p><p>For a good introduction into the internals of rrdtool/rrdcached updates and the problems with I/O scaling look at presentation by Sebastian Harl, <a class=link href="http://www.netways.de/index.php?id=2815" target=_blank rel=noopener>How to Escape the I/O Hell</a></p><p><a id=Scaling></a></p><h3 id=scaling-problems>Scaling problems</h3><p>Most of today&rsquo;s monitoring systems do not easily scale-out. Scale-out, or scaling horizontally, is when you can add new nodes in response to increased load. Scaling up by replacing existing hardware with state-of-the-art hardware is both expensive and only buys you limited time before the next even more expensive necessary hardware upgrade. Many systems offer distributed polling but none offer the option of spreading out the disk load. For example; you can <a class=link href=http://community.zenoss.org/docs/DOC-2485 target=_blank rel=noopener>scale Zenozz for High Availability</a> but not performance.</p><p><a id=Loss></a></p><h3 id=loss-of-detail>Loss of detail</h3><p>Current RRD based systems will aggregate old data into averages in order to save storage space. Most technicians do not have the in depth knowledge in order to tune the rules for aggregation and will leave the default values as is. Using cacti as an example and looking at the <a class=link href=http://docs.cacti.net/manual:088:8_rrdtool#rrd_files target=_blank rel=noopener>cacti documentation</a> we see that in a very short time, 2 months, data is averaged to a single data point PER DAY. For systems such as Internet backbones where traffic vary a lot from bottom (30% utilization for example) to peak (90% utilization for example) during a day only the average of 60% is shown in the graphs. This in turn makes troubleshooting by comparing old data difficult. It makes trending based on peaks/bottoms impossible and it may also lead to wrong or delayed strategic decisions on where to invest in added capacity.</p><p><a id=flexibility></a></p><h3 id=lack-of-flexibility>Lack of flexibility</h3><p>In order to collect, store and graph new kinds of metrics an operator would need a certain level of programming skills and experience with the internals of the monitoring system. Adding new metrics to the systems would range from hours to weeks depending on the skill and experience of the operator. Creating new graphs based on existing metrics is also very difficult on most systems. And not within reach for the average operator.</p><p><a id=revolution></a></p><h2 id=the-monitoring-revolution>The monitoring revolution</h2><p>We are currently at the beginning of a monitoring revolution. The advent of cloud computing and big data has created a need for measuring lots of metrics for thousands of machines at small intervals. This has sparked the creation of completely new monitoring components. One of the components where we now have improved alternatives is for efficient metric storage.</p><p>The first is <strong><a class=link href=http://opentsdb.net/ target=_blank rel=noopener>OpenTSDB</a></strong>, a &ldquo;Scalable, Distributed, Time Series Database&rdquo; that begun development at <a class=link href=https://www.stumbleupon.com/ target=_blank rel=noopener>StumbleUpon</a> in 2011 and aimed at solving some of the problems with existing monitoring systems. OpenTSDB is built in top of Apache HBase which is a scalable and performant database that builds on top of Apache Hadoop. Hadoop is a series of tools for building large and scalable distributed systems. Back in 2010 Facebook already had <a class=link href=http://hadoopblog.blogspot.no/2010/05/facebook-has-worlds-largest-hadoop.html target=_blank rel=noopener>2000 machines in a Hadoop cluster</a> with 21PB (that is 21.000.000 GB) of combined storage.</p><p>The second is an interesting newcommer, <a class=link href=http://influxdb.com/ target=_blank rel=noopener><strong>InfluxDB</strong></a>, that began development in 2013 and has the goal of offering scalability and performance without the requirements of HBase/Hadoop.</p><p>In addition to advances in performance these alternatives also decouple storage of metrics and display of graphs and abstract the interaction in simple and well-defined APIs. This makes it easy for developers to create improved frontends rapidly and this has already resulted in several very attractive open-source frontends such as <strong><a class=link href=https://github.com/Ticketmaster/Metrilyx-2.0 target=_blank rel=noopener>Metrilyx</a></strong> (OpenTSDB), <strong><a class=link href=http://grafana.org/ target=_blank rel=noopener>Grafana</a></strong> (InfluxDB, Graphite, <a class=link href=https://github.com/grafana/grafana/pull/211 target=_blank rel=noopener>soon OpenTSDB</a>), <strong><a class=link href=http://www.statuswolf.com/ target=_blank rel=noopener>StatusWolf</a></strong> (OpenTSDB), <strong><a class=link href=https://github.com/hakobera/influga target=_blank rel=noopener>Influga</a></strong> (InfluxDB).</p><p><a id=Debian></a></p><h2 id=setting-up-a-single-node-opentsdb-instance-on-debian-7-wheezy>Setting up a single node OpenTSDB instance on Debian 7 Wheezy</h2><p>In the rest of this paper we will set up a single node OpenTSDB instance. OpenTSDB builds on top of HBase and Hadoop and scales to very large setups easily. But it also delivers substantial performance on a single node which is deployed in <strong>less than an hour</strong>. There are plenty of guides on installing a Hadoop cluster but here we will focus on the natural first step of getting a single node running using <strong>recent releases</strong> of the relevant software:</p><ul><li>OpenTSDB 2.0.0 - Released 2014-05-05</li><li>HBase 0.98.2 - Released 2014-05-01</li><li>Hadoop 2.4.0 - Released 2014-04-07</li></ul><blockquote><p>If you later require to deploy a larger cluster consider using a framework such as <a class=link href=http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html target=_blank rel=noopener><strong>Cloudera CDH</strong></a> or <a class=link href=http://hortonworks.com/hdp/ target=_blank rel=noopener><strong>Hortonworks HDP</strong></a> which are open-source platforms which package Apache Hadoop components and provides a fully tested environment and easy-to-use graphical frontends for configuration and management. It is <a class=link href=http://opentsdb.net/setup-hbase.html target=_blank rel=noopener>recommended to have at least 5 machines</a> in a HBase cluster supporting OpenTSDB.</p></blockquote><hr><blockquote><p>This guide assumes you are somewhat familiar with using a Linux shell/command prompt.</p></blockquote><p><a id=Hardware></a></p><h4 id=hardware-requirements>Hardware requirements</h4><ul><li>CPU cores: Max (Limit to 50% of your available CPU resources)</li><li>RAM: Min 16 GB</li><li>Disk 1 - OS: 10 GB - Thin provisioned</li><li>Disk 2 - Data: 100 GB - Thin provisioned</li></ul><p><a id=Operating></a></p><h4 id=operating-system-requirements>Operating system requirements</h4><p>This guide is based on a recently installed Debian 7 Wheezy <strong>64bit</strong> installed without any extra packages. See the <a class=link href=https://www.debian.org/releases/stable/amd64/ target=_blank rel=noopener>official documentation</a> for more information.</p><p>All commands are entered as <strong>root</strong> user unless otherwise noted.</p><p><a id=preparations></a></p><h4 id=pre-setup-preparations>Pre-setup preparations</h4><p>We start by installing a few tools that we will need later.</p><pre><code>apt-get install wget make gcc g++ cmake maven
</code></pre><p>Create a new ext3 partition on the data disk <strong>/dev/sdb</strong>:</p><pre><code>(echo &quot;n&quot;; echo &quot;p&quot;; echo &quot;&quot;; echo &quot;&quot;; echo &quot;&quot;; echo &quot;t&quot;; echo &quot;83&quot;; echo &quot;w&quot;) | fdisk /dev/sdb

mkfs.ext3 /dev/sdb1
</code></pre><blockquote><p>ext3 is the <a class=link href=https://wiki.apache.org/hadoop/DiskSetup target=_blank rel=noopener>recommended filesystem for Hadoop</a>.</p></blockquote><p>Create a mountpoint <strong>/mnt/data1</strong> and add it to the file system table and mount the disk:</p><pre><code>mkdir /mnt/data1
echo &quot;/dev/sdb1     /mnt/data1    ext3    auto,noexec,noatime,nodiratime   0   1&quot; | tee -a /etc/fstab
mount /mnt/data1
</code></pre><blockquote><p>Using <strong>noexec</strong> for the data partition will increase security as nothing on the data partition will be allowed to ever execute.<br>Using <strong>noatime</strong> and <strong>nodiratime</strong> increases performance since the read access timestamps are not updated on every file access.</p></blockquote><p><a id=java></a></p><h4 id=installing-java-from-packages>Installing java from packages</h4><p>Installing java on Linux can be quite challenging due to licensing issues, but thanks to the guys over at <a class=link href=https://launchpad.net/ target=_blank rel=noopener>Launchpad.net</a> who are providing a repository with a custom java package this can now be done quite easy.</p><p>We start by adding the launchpad java repository to our <em><strong>/etc/apt/sources.list</strong></em> file:</p><pre><code>echo &quot;deb http://ppa.launchpad.net/webupd8team/java/ubuntu precise main&quot; | tee -a /etc/apt/sources.list
echo &quot;deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu precise main&quot; | tee -a /etc/apt/sources.list
</code></pre><p>Add the signing key and download information from the new repository:</p><pre><code>apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys EEA14886
apt-get update
</code></pre><p>Run the java installer:</p><pre><code>apt-get install oracle-java7-installer
</code></pre><p>Follow the instructions on screen to complete the Java 7 installation.</p><p><a id=HBase></a></p><h3 id=installing-hbase>Installing HBase</h3><p>OpenTSDB has its own HBase installation tutorial <a class=link href=http://opentsdb.net/setup-hbase.html target=_blank rel=noopener>here</a>. It is very brief and does not use the latest versions or snappy compression.</p><p>Download and unpack HBase:</p><pre><code>cd /opt
wget http://apache.vianett.no/hbase/hbase-0.98.2/hbase-0.98.2-hadoop2-bin.tar.gz
tar xvfz hbase-0.98.2-hadoop2-bin.tar.gz
export HBASEDIR=`pwd`/hbase-0.98.2-hadoop2/
</code></pre><p>Increase the system-wide limitations of open files and processes from the default of 1000 to 32000 by adding a few lines to <em><strong>/etc/security/limits.conf</strong></em>:</p><pre><code>echo &quot;root    -               nofile  32768&quot; | tee -a /etc/security/limits.conf
echo &quot;root    soft/hard       nproc   32000&quot; | tee -a /etc/security/limits.conf
echo &quot;*    -               nofile  32768&quot; | tee -a /etc/security/limits.conf
echo &quot;*    soft/hard       nproc   32000&quot; | tee -a /etc/security/limits.conf
</code></pre><p>The settings above will only take effect if we also add a line to <em><strong>/etc/pam.d/common-session</strong></em>:</p><pre><code>echo &quot;session required  pam_limits.so&quot; | tee -a /etc/pam.d/common-session
</code></pre><p><a id=snappy></a></p><h4 id=install-snappy>Install snappy</h4><p><a class=link href=https://code.google.com/p/snappy/ target=_blank rel=noopener>Snappy</a> is a compression algorithm that values speed over compression ratio and this makes it a good choice for high throughput applications such as Hadoop/HBase. Due to licensing issues Snappy does not ship with HBase and need to be installed on top.</p><p>The installation process is a bit complicated and has caused headache for many people (me included). Here we will show a method of installing snappy and getting it to work with the latest version of HBase and Hadoop.</p><blockquote><p><strong>Compression algorithms in HBase</strong>
Compression is the method of reducing the size of a file or text without losing any of the contents. There are many compression algorithms available and some focus on being able to create the smallest compressed file at the cost of time and CPU usage while other achieve <em>reasonable</em> compression ratio while being very fast.<br><br>Out of the box HBase supports gz(gzip/zlib), snappy and lzo. Only gz is included due to licensing issues.
Unfortunately gz is a slow and costly algorithm compared to snappy and lzo. In a test performed by Yahoo (see <a class=link href=http://www.slideshare.net/Hadoop_Summit/singh-kamat-june27425pmroom210c target=_blank rel=noopener>slides here</a>, page 8) gz achieves 64% compression in 32 seconds. lzo 47% in 4.8 seconds and snappy 42% in 4.0 seconds. lz4 is another protocol <a class=link href=http://search-hadoop.com/m/KFLWV1PFVhp1 target=_blank rel=noopener>considered for inclusion</a> that is even faster (2.4 seconds) but requires much more memory.<br><br><em>For more information look at the <a class=link href=https://hbase.apache.org/book/compression.html target=_blank rel=noopener>Apache HBase Handbook - Appendix C - Compression</a></em></p></blockquote><p><a id=native></a></p><h4 id=building-native-libhadoop-and-libsnappy>Building native libhadoop and libsnappy</h4><p>In order to use compression we need the common Hadoop library, libhadoop.so, and the snappy library, libsnappy.so. HBase ships without libhadoop.so and the libhadoop.so that ships in the Hadoop Package is only for 32 bit OS. So we need to compile these files ourself.</p><p>Start by downloading and installing ProtoBuf. Hadoop requres version 2.5+ which is not available as a Debian package unfortunately.</p><pre><code>wget --no-check-certificate https://protobuf.googlecode.com/files/protobuf-2.5.0.tar.gz
tar zxvf protobuf-2.5.0.tar.gz
cd protobuf-2.5.0
./configure; make; make install
export LD_LIBRARY_PATH=/usr/local/lib/
</code></pre><p>Download and compile Hadoop:</p><pre><code>apt-get install zlib1g-dev
wget http://apache.uib.no/hadoop/common/hadoop-2.4.0/hadoop-2.4.0-src.tar.gz
tar zxvf hadoop-2.4.0-src.tar.gz
cd hadoop-2.4.0-src/hadoop-common-project/
mvn package -Pdist,native -Dskiptests -Dtar -Drequire.snappy -DskipTests
</code></pre><p>Copy the newly compiled native libhadoop library into /usr/local/lib, then create the folder in which HBase looks for it and create a shortcut from there to /usr/local/lib/libhadoop.so:</p><pre><code>cp hadoop-common/target/native/target/usr/local/lib/libhadoop.* /usr/local/lib
mkdir -p $HBASEDIR/lib/native/Linux-amd64-64/
cd $HBASEDIR/lib/native/Linux-amd64-64/
ln -s /usr/local/lib/libhadoop.so* .
</code></pre><p>Install snappy from Debian packages:</p><pre><code>apt-get install libsnappy-dev
</code></pre><p><a id=ConfiguringHBase></a></p><h4 id=configuring-hbase>Configuring HBase</h4><p>Now we need to do some basic configuration before we can start HBase. The configuration files are in $HBASEDIR/conf/.</p><p><a id=hbase-env.sh></a></p><h4 id=confhbase-envsh><strong>conf/hbase-env.sh</strong></h4><p>A shell script setting various environment variables related to how HBase and Java should behave. The file contains a lot of options and they are all documented by comments so feel free to look around in it.</p><p>Start by setting the JAVA_HOME, which points to where Java is installed:</p><pre><code>export JAVA_HOME=/usr/lib/jvm/java-7-oracle/
</code></pre><p>Then increase the size of the <a class=link href="http://pubs.vmware.com/vfabric52/index.jsp?topic=/com.vmware.vfabric.em4j.1.2/em4j/conf-heap-management.html" target=_blank rel=noopener>Java Heap</a> from the default of 1000 which is a bit low:</p><pre><code>export HBASE_HEAPSIZE=8000
</code></pre><p><a id=Background></a></p><h4 id=confhbase-sitexml><strong>conf/hbase-site.xml</strong></h4><p>An XML file containing HBase specific configuration parameters.</p><pre><code>&lt;configuration&gt;

   &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;/mnt/data1/hbase&lt;/value&gt;
  &lt;/property&gt;
  
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
    &lt;value&gt;/mnt/data1/zookeeper&lt;/value&gt;
  &lt;/property&gt;

&lt;/configuration&gt;
</code></pre><p><a id=compression></a></p><h4 id=testing-hbase-and-compression>Testing HBase and compression</h4><p>Now that we have installed snappy and configured HBase we can verify that HBase is working and that the compression is loaded by doing:</p><pre><code>$HBASEDIR/bin/hbase org.apache.hadoop.hbase.util.CompressionTest /tmp/test.txt snappy
</code></pre><p>This should output some lines with information and end with <strong>SUCCESS</strong>.</p><p><a id=StartingHBase></a></p><h4 id=starting-hbase>Starting HBase</h4><p>HBase ships with scripts for starting and stopping it, namely start-hbase.sh and stop-hbase.sh. You start HBase with</p><pre><code>$HBASEDIR/bin/start-hbase.sh
</code></pre><p>Then look at the log to ensure it has started without any serious errors:</p><pre><code>tail -fn100 $HBASEDIR/bin/../logs/hbase-root-master-opentsdb.log
</code></pre><p>If you want HBase to start automatically on boot you can use a process management tool such as <a class=link href=http://mmonit.com/monit/ target=_blank rel=noopener>Monit</a> or simply put it in <em><strong>/etc/rc.local</strong></em>:</p><pre><code>/opt/hbase-0.98.2-hadoop2/bin/start-hbase.sh
</code></pre><p><a id=InstallingOpenTSDB></a></p><h3 id=installing-opentsdb>Installing OpenTSDB</h3><p>Start by installing gnuplot, which is used by the native webui to draw graphs:</p><pre><code>apt-get install gnuplot
</code></pre><p>Then download and install OpenTSDB:</p><pre><code>wget https://github.com/OpenTSDB/opentsdb/releases/download/v2.0.0/opentsdb-2.0.0_all.deb
dpkg -i opentsdb-2.0.0_all.deb
</code></pre><p><a id=ConfiguringOpenTSDB></a></p><h4 id=configuring-opentsdb>Configuring OpenTSDB</h4><p>The configuration file is <em><strong>/etc/opentsdb/opentsdb.conf</strong></em>. It has some of the basic configuration parameters but not nearly all of them. <a class=link href=http://opentsdb.net/docs/build/html/user_guide/configuration.html target=_blank rel=noopener>Here is the official documentation with all configuration parameters</a>.</p><p>The defaults are reasonable but we need to make a few tweaks, the first is to add this:</p><pre><code>tsd.core.auto_create_metrics = true
</code></pre><p>This will make OpenTSDB accept previously unseen metrics and add them to the database. This is very useful in the beginning when feeding data into OpenTSDB. Without this you will have to use the command <em><strong>mkmetric</strong></em> for each metric you will store and get errors that might be hard to trace if the metric you create do not match what is actually sent.</p><p>Then we will add support for chunked requests via the HTTP API:</p><pre><code>tsd.http.request.enable_chunked = true
tsd.http.request.max_chunk = 16000
</code></pre><p>Some tools and plugins (such as our own <a class=link href=https://github.com/PeritusConsulting/collectd-opentsdb target=_blank rel=noopener>improved collectd to OpenTSDB plugin</a>) send multiple data points in a single HTTP request for increased efficiency and requires this setting to be enabled.</p><p><a id=HBasetables></a></p><h4 id=creating-hbase-tables>Creating HBase tables</h4><p>Before we start OpenTSDB we need to create the necessary tables in HBase:</p><pre><code>env COMPRESSION=SNAPPY HBASE_HOME=$HBASEDIR /usr/share/opentsdb/tools/create_table.sh
</code></pre><p><a id=StartingOpenTSDB></a></p><h4 id=starting-opentsdb>Starting OpenTSDB</h4><p>Since version 2.0.0 OpenTSDB ships as a Debian package and includes SysV init scripts. To start OpenTSDB as a daemon running in the background we run:</p><pre><code>service opentsdb start
</code></pre><p>And then check the logs for any errors or other relevant information:</p><pre><code>tail -f /var/log/opentsdb/opentsdb.log
</code></pre><p>If the server is started successfully the last line of the log should say:</p><pre><code>13:42:30.900 INFO  [TSDMain.main] - Ready to serve on /0.0.0.0:4242
</code></pre><p>And you can now browse to your new OpenTSDB in a browser using http://hostname:4242 !</p><p><a id=Feeding></a></p><h3 id=feeding-data-into-opentsdb>Feeding data into OpenTSDB</h3><p>It is not within the scope of this paper to go into details about how to feed data into OpenTSDB but we will give a quick introduction here to get you started.</p><blockquote><p><strong>A note on metric naming in OpenTSDB</strong><br><br>Each datapoint has a metric name such as <em><strong>df.bytes.free</strong></em> and one or more tags such as <em><strong>host=server1</strong></em> and <em><strong>mount=/mnt/data1</strong></em>. This is closer to the proposed <a class=link href=http://metrics20.org/ target=_blank rel=noopener>Metrics 2.0</a> standard for naming metrics than the traditional naming of <em><strong>df.bytes.free.server1.mnt-data</strong></em>. This makes it possible to create aggregates across tags and combine data easily using the tags.<br><br>OpenTSDB stores each datapoint with a given metric and tags in one HBase row per hour. But due to a HBase issue it still has to scan every row that matches the metric, ignoring the tags. Even though it will only return the data also matching the tags. This results in very much data being read and it will be very slow to read if there is a large number of data points for a given metric. The default for the collectd-opentsdb plugin is to use the read plugin name as metric, and other values as tags. In my case this results in 72.000.000 datapoints per hour for this metric. When generating a graph all of this data has to be read and evaluated before drawing a graph. 24 hours of data is over 1.7 billion datapoints for this single metric and results in a read performance of 5-15 <strong>minutes</strong> for a simple graph.<br><br>A solution to this is to use <em>shift-to-metric</em>, as <a class=link href=http://opentsdb.net/docs/build/html/user_guide/writing.html target=_blank rel=noopener>mentioned in the OpenTSDB user guide</a>. Shift-to-metric is simply moving one or more data identifiers from tags to the metric in order to reduce the cardinality (number of values) for a metric, and hence the time required to read out the data we want. We have modified the collectd-opentsdb java plugin in order to shift the tags to metrics, and this increases read-performance by ~1000x down to 10-100ms. Read the section about collectd below for more information on our modified plugin.</p></blockquote><p><a id=tcollector></a></p><h4 id=tcollector>tcollector</h4><p><a class=link href=http://opentsdb.net/docs/build/html/user_guide/utilities/tcollector.html target=_blank rel=noopener>tcollector</a> is the default agent for collecting and sending data from a Linux server to a OpenTSDB server. It is based on Python and plugins / addons can be written in any language. It ships with the most common plugins to collect information about disk usage and performance, cpu and memory statistics and also for some specific systems such as mysql, mongodb, riak, varnish, postgresql and others. tcollector is very lightweight and features advanced de-duplication in order to reduce unneeded network traffic.</p><p>The commands for installing dependencies and downloading tcollector are</p><pre><code>aptitude install git python
cd /opt
git clone git://github.com/OpenTSDB/tcollector.git
</code></pre><p>Configuration is in the startup script <em><strong>tcollector/startstop</strong></em>, you will need to uncomment and set the value of TSD_HOST to point to your OpenTSDB server.</p><p>To start it run</p><pre><code>/opt/tcollector/startstop start
</code></pre><p>This is also the command you want to add to <em><strong>/etc/rc.local</strong></em> in order to have the agent automatically start at boot. Logfiles are saved in <em><strong>/var/log/tcollector.log</strong></em> and they are rotated automatically.</p><p><a id=peritus-tc-tools></a></p><h4 id=peritus-tc-tools>peritus-tc-tools</h4><p>We have developed a set of <strong>tcollector</strong> plugins for collecting statistics from</p><ul><li><strong><a class=link href=https://www.isc.org/downloads/dhcp/ target=_blank rel=noopener>ISC DHCPd server</a></strong>, about number of DHCP events and DHCP pool sizes</li><li><strong><a class=link href=http://www.opensips.org/ target=_blank rel=noopener>OpenSIPS</a></strong>, total number of subscribers and registered user agents</li><li><strong><a class=link href=http://atmail.com/ target=_blank rel=noopener>Atmail</a></strong>, number of users, admins, sent and received emails, logins and errors</li></ul><p>As well as a high performance replacement for <strong><a class=link href=http://oss.oetiker.ch/smokeping/ target=_blank rel=noopener>smokeping</a></strong> called <strong>tc-ping</strong>.</p><p>These plugins are available for download from our <strong><a class=link href=https://github.com/PeritusConsulting/peritus-tc-tools target=_blank rel=noopener>GitHub page</a></strong>.</p><p><a id=collectd-opentsdb></a></p><h4 id=collectd-opentsdb>collectd-opentsdb</h4><p><a class=link href=http://collectd.org/ target=_blank rel=noopener>collectd</a> is the <em>system statistics collection daemon</em> and is a widely used system for collecting metrics from various sources. There are several options for sending data from collectd to OpenTSDB but one way that works well is to use the <a class=link href=https://github.com/auxesis/collectd-opentsdb target=_blank rel=noopener>collectd-opentsdb java write plugin</a>.</p><p>Since collectd is a generic metric collection tool the original collectd-opentsdb plugin will use the plugin name (such as <strong>snmp</strong>) as the metric, and use tags such as <strong>host=servername</strong>, <strong>plugin_instance=ifHcInOctets</strong> and <strong>type_instance=FastEthernet0/1</strong>.</p><p>As mentioned in the <em><strong>note on metric naming in OpenTSDB</strong></em> this can be very inefficient when data needs to be read again resulting in read performance potentially thousands of times slower than optimal (&lt;100ms). To alleviate this we have modified the original collectd-opentsdb plugin to store all metadata as part of the metric. This gives metric names such as ifHCInBroadcastPkts.sw01.GigabitEthernet0 and very good read performance.</p><p>The modified collectd-opentsdb plugin can be downloaded from our <a class=link href=https://github.com/PeritusConsulting/collectd-opentsdb target=_blank rel=noopener>GitHub repository</a>.</p><p><a id=MonitoringOpenTSDB></a></p><h4 id=monitoring-opentsdb>Monitoring OpenTSDB</h4><p>To monitor OpenTSDB itself install tcollector as described above on the OpenTSDB server and set <em><strong>TSD_HOST</strong></em> to <em><strong>localhost</strong></em> in <em><strong>/opt/tcollector/startstop</strong></em>.</p><p>You can then go to http://opentsdb-server:4242/#start=1h-ago&amp;end=1s-ago&amp;m=sum:rate:tsd.rpc.received%7Btype=*%7D&amp;o=&amp;yrange=%5B0:%5D&amp;wxh=1200x600 to view a graph of amount of data received in the last hour.</p><p><a id=Performancecomparison></a></p><h3 id=performance-comparison>Performance comparison</h3><p>Lastly we include a little performance comparison between the latest version of OpenTSDB+HBase+Hadoop, a previous version of OpenTSDB+HBase+Hadoop that we have used for a while as well as rrdcached which ran in production for 4 years at a client.</p><p>The workload is gathering and storing metrics from 150 Cisco switches with 8200 ports/interfaces every 5 seconds. This equals about 15.000 points per second.</p><p><img src=/p/next-generation-monitoring-with-opentsdb/Figure1.png width=735 height=338 srcset="/p/next-generation-monitoring-with-opentsdb/Figure1_hu13479656331014733159.png 480w, /p/next-generation-monitoring-with-opentsdb/Figure1_hu12430936122100692572.png 1024w" loading=lazy alt="Figure 1 - Data received by OpenTSDB per second" class=gallery-image data-flex-grow=217 data-flex-basis=521px></p><p><a id=Collection></a></p><h4 id=collection>Collection</h4><p>Even though it is not the primary focus, we include some data about collection performance for completeness. Collection is done using the latest version of <a class=link href=http://collectd.org/ target=_blank rel=noopener>collectd</a> and the builtin SNMP plugin.</p><blockquote><p><strong>NB #1:</strong> There is a <a class=link href=https://github.com/collectd/collectd/issues/610 target=_blank rel=noopener>memory leak</a> in the way collectd&rsquo;s SNMP plugin uses the underlying libsnmp library and you might need to schedule a restart of the collectd service as a workaround for that if handling large workloads.</p></blockquote><blockquote><p><strong>NB #2:</strong> Due to <a class=link href=http://comments.gmane.org/gmane.comp.monitoring.collectd/5061 target=_blank rel=noopener>limitations in the libnetsnmp library</a> you will run into problems if polling many (1000+) devices with a single collectd instance. A workaround is to run multiple collectd instances with fewer hosts. <a class=link href=https://github.com/collectd/collectd/issues/610 target=_blank rel=noopener>memory leak</a></p></blockquote><p>Figure 2 shows that collection through SNMP polling consumes about 2200Mhz. We optimized some of the data types and definitions in collectd when moving to OpenTSDB and achieved a 20% performance increase in the polling as seen in Figure 3.</p><p><img src=/p/next-generation-monitoring-with-opentsdb/Figure2.png width=785 height=616 srcset="/p/next-generation-monitoring-with-opentsdb/Figure2_hu11520703687699822583.png 480w, /p/next-generation-monitoring-with-opentsdb/Figure2_hu6507568741892228121.png 1024w" loading=lazy alt="Figure 2 - CPU Usage - SNMP polling and writing to RRDcached" class=gallery-image data-flex-grow=127 data-flex-basis=305px></p><p><img src=/p/next-generation-monitoring-with-opentsdb/Figure3.png width=787 height=625 srcset="/p/next-generation-monitoring-with-opentsdb/Figure3_hu8758563371351857519.png 480w, /p/next-generation-monitoring-with-opentsdb/Figure3_hu7312580424747238338.png 1024w" loading=lazy alt="Figure 3 - CPU Usage - SNMP polling and sending to OpenTSDB" class=gallery-image data-flex-grow=125 data-flex-basis=302px></p><p>Writing to the native rrdcached write plugin consumes 1300Mhz while our modified collectd-opentsdb plugin consumes 1450Mhz. It is probably possible to create a much more efficient write plugin with more advanced knowledge of concurrency and using a lower level language such as C.</p><p><a id=Storage></a></p><h4 id=storage>Storage</h4><p>When considering storage performance we will look at CPU usage and disk IOPS since these are the primary drivers of cost in today&rsquo;s datacenters.</p><h4 id=collectd--rrdcached>collectd + rrdcached</h4><p>CPU usage - 1300Mhz, see Figure 2 above.</p><p><img src=/p/next-generation-monitoring-with-opentsdb/Figure4.png width=661 height=286 srcset="/p/next-generation-monitoring-with-opentsdb/Figure4_hu9422720109705725120.png 480w, /p/next-generation-monitoring-with-opentsdb/Figure4_hu2897764835097661964.png 1024w" loading=lazy alt="Figure 4 - Disk write IOPS - Fluctuating between 10 and 170 IOPS during the 1 hour flush period." class=gallery-image data-flex-grow=231 data-flex-basis=554px></p><h4 id=opentsdb--hbase-096--hadoop-1>OpenTSDB + Hbase 0.96 + Hadoop 1</h4><p><img src=/p/next-generation-monitoring-with-opentsdb/Figure5.png width=791 height=616 srcset="/p/next-generation-monitoring-with-opentsdb/Figure5_hu17014411734833306734.png 480w, /p/next-generation-monitoring-with-opentsdb/Figure5_hu7917395545853379940.png 1024w" loading=lazy alt="Figure 5 - CPU usage - 1700Mhz baseline with peaks of 7000Mhz during Java Garbage Collection (GC) (untuned)." class=gallery-image data-flex-grow=128 data-flex-basis=308px></p><p><img src=/p/next-generation-monitoring-with-opentsdb/Figure6.png width=800 height=285 srcset="/p/next-generation-monitoring-with-opentsdb/Figure6_hu14624923996941413689.png 480w, /p/next-generation-monitoring-with-opentsdb/Figure6_hu10993571528499142312.png 1024w" loading=lazy alt="Figure 6 - Disk write IOPS - 5 IOPS average with peaks of 25 IOPS during Java GC. We also see that disk read IOPS are much higher and this is due to regular compaction of the database and can be tuned. Reads in general can be reduced by increasing caching with more RAM if necessary." class=gallery-image data-flex-grow=280 data-flex-basis=673px></p><h4 id=opentsdb--hbase-098--hadoop-2>OpenTSDB + HBase 0.98 + Hadoop 2</h4><p><img src=/p/next-generation-monitoring-with-opentsdb/Figure7.png width=925 height=478 srcset="/p/next-generation-monitoring-with-opentsdb/Figure7_hu8040901172863881962.png 480w, /p/next-generation-monitoring-with-opentsdb/Figure7_hu1188736229991262180.png 1024w" loading=lazy alt="Figure 7 - CPU usage - 1200Mhz baseline with peaks of 5000-6000Mhz during Java GC (untuned)." class=gallery-image data-flex-grow=193 data-flex-basis=464px></p><p><img src=/p/next-generation-monitoring-with-opentsdb/Figure8.png width=767 height=395 srcset="/p/next-generation-monitoring-with-opentsdb/Figure8_hu10785440613668549146.png 480w, /p/next-generation-monitoring-with-opentsdb/Figure8_hu12590607926021679589.png 1024w" loading=lazy alt="Figure 8 - Disk write IOPS - < 5 IOPS average with peaks of 25 IOPS during Java GC. Much less read IOPS during compaction compared to HBase 0.96." class=gallery-image data-flex-grow=194 data-flex-basis=466px></p><p><a id=Conclusion></a></p><h4 id=conclusion>Conclusion</h4><p>Even without tuning, a single instance OpenTSDB installation is able to handle significant amounts of data before running into IO problems. This comes at a cost of CPU, currently OpenTSDB will consume > 300% the amount of CPU cycles compared to rrdcached for storage. But this is offset by a 85-95% reduction in disk load. In absolute terms for our particular set up (one 2 year old HP DL360p Gen8 running VMware vSphere 5.5) CPU usage increased from 15% to 25% while reducing IOPS load from 70% to &lt; 10%.</p><p><em>Fine tuning of parameters (such as Java GC) as well as detailed analysis of memory usage is outside the scope of this brief paper and detailed information may be found elsewhere (<a class=link href=https://hbase.apache.org/book/performance.html target=_blank rel=noopener>51</a>,<a class=link href=http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html target=_blank rel=noopener>52</a>,<a class=link href=http://www.cubrid.org/blog/textyle/428187 target=_blank rel=noopener>53</a>) for those interested.</em></p><hr></section><footer class=article-footer><section class=article-tags><a href=/tags/observability/>Observability</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Jun 02, 2014 19:56 UTC</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/><div class=article-details><h2 class=article-title>A side quest in API development, observability, Kubernetes and cloud with a hint of database</h2></div></a></article><article><a href=/p/kubernetes-sidecar-config-drift/><div class=article-details><h2 class=article-title>Kubernetes Sidecar Config Drift</h2></div></a></article><article><a href=/p/yak-shaving-photo-drips-for-my-mom/><div class=article-details><h2 class=article-title>Yak shaving - Photo drips for my mom</h2></div></a></article><article class=has-image><a href=/p/end-of-2020-rough-database-landscape/><div class=article-image><img src=/p/end-of-2020-rough-database-landscape/map-nosql.c9e7a90e6cb46866cf2bd4f82b271187_hu2001732389994507380.png width=250 height=150 loading=lazy alt="Featured image of post End of 2020 rough database landscape" data-hash="md5-yeepDmy0aGbPK9T4KycRhw=="></div><div class=article-details><h2 class=article-title>End of 2020 rough database landscape</h2></div></a></article><article class=has-image><a href=/p/mini-post-down-scaling-azure-kubernetes-service-aks/><div class=article-image><img src=/p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu.ada85c716b0eb235da02aa0a65aa3303_hu16938543950284632476.png width=250 height=150 loading=lazy alt="Featured image of post Mini-post: Down-scaling Azure Kubernetes Service (AKS)" data-hash="md5-rahccWsOsjXaAqoKZaozAw=="></div><div class=article-details><h2 class=article-title>Mini-post: Down-scaling Azure Kubernetes Service (AKS)</h2></div></a></article></div></div></aside><div class=disqus-container></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2024 blog.stian.omg.lol</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>