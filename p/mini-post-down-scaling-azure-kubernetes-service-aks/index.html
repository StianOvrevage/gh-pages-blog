<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Looking into caveats of running Kubernetes on AKS with small node sizes and node counts."><title>Mini-post: Down-scaling Azure Kubernetes Service (AKS)</title>
<link rel=canonical href=https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Mini-post: Down-scaling Azure Kubernetes Service (AKS)"><meta property='og:description' content="Looking into caveats of running Kubernetes on AKS with small node sizes and node counts."><meta property='og:url' content='https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/'><meta property='og:site_name' content='blog.stian.omg.lol'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='kubernetes'><meta property='article:tag' content='azure'><meta property='article:published_time' content='2019-06-04T00:00:00+00:00'><meta property='article:modified_time' content='2019-06-04T00:00:00+00:00'><meta property='og:image' content='https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/2019-06-04-downscaling-aks.png'><meta name=twitter:title content="Mini-post: Down-scaling Azure Kubernetes Service (AKS)"><meta name=twitter:description content="Looking into caveats of running Kubernetes on AKS with small node sizes and node counts."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/2019-06-04-downscaling-aks.png'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/profile-picture_hu6892409687065925510.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ü§∑‚Äç‚ôÇÔ∏è</span></figure><div class=site-meta><h1 class=site-name><a href=/>blog.stian.omg.lol</a></h1><h2 class=site-description>Technology. Aviation. Philosophy.</h2></div></header><ol class=menu-social><li><a href=https://github.com/StianOvrevage target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#node-overhead>Node overhead</a><ol><li><ol><li><a href=#cpu>CPU</a></li><li><a href=#memory>Memory</a></li></ol></li></ol></li><li><a href=#node-pods-daemonsets>Node pods (DaemonSets)</a><ol><li><ol><li><a href=#cpu-1>CPU</a></li><li><a href=#memory-1>Memory</a></li></ol></li></ol></li><li><a href=#kube-system-pods>kube-system pods</a></li><li><a href=#third-party-pods>Third party pods</a></li><li><a href=#radix-platform-pods>Radix platform pods</a></li><li><a href=#pod-scheduling>Pod scheduling</a></li><li><a href=#calico-node>calico-node</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/mini-post-down-scaling-azure-kubernetes-service-aks/><img src=/p/mini-post-down-scaling-azure-kubernetes-service-aks/2019-06-04-downscaling-aks_hu13616209857549925028.png srcset="/p/mini-post-down-scaling-azure-kubernetes-service-aks/2019-06-04-downscaling-aks_hu13616209857549925028.png 800w, /p/mini-post-down-scaling-azure-kubernetes-service-aks/2019-06-04-downscaling-aks_hu13597429594578484734.png 1600w" width=800 height=457 loading=lazy alt="Featured image of post Mini-post: Down-scaling Azure Kubernetes Service (AKS)"></a></div><div class=article-details><header class=article-category><a href=/categories/technology/ style=background-color:#2a9d8f;color:#fff>Technology</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/mini-post-down-scaling-azure-kubernetes-service-aks/>Mini-post: Down-scaling Azure Kubernetes Service (AKS)</a></h2><h3 class=article-subtitle>Looking into caveats of running Kubernetes on AKS with small node sizes and node counts.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jun 04, 2019</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>7 minute read</time></div></footer></div></header><section class=article-content><p>We discovered today that some implicit assumptions we had about AKS at smaller scales were incorrect.</p><p>Suddenly new workloads and jobs in our Radix CI/CD could not start due to insufficient resources (CPU & memory).</p><p>Even though it only caused problems in development environments with smaller node sizes it still surprised some of our developers, since we expected the size of development clusters to have enough resources.</p><p>I thought it would be a good chance to go a bit deeper and verify some of our assumptions and also learn more about various components that usually &ldquo;just works&rdquo; and isn&rsquo;t really given much thought.</p><p>First I do a <code>kubectl describe node &lt;node></code> on 2-3 of the nodes to get an idea of how things are looking:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>Resource                       Requests          Limits
</span></span><span class=line><span class=cl>--------                       --------          ------
</span></span><span class=line><span class=cl>cpu                            930m <span class=o>(</span>98%<span class=o>)</span>        5500m <span class=o>(</span>585%<span class=o>)</span>
</span></span><span class=line><span class=cl>memory                         <span class=m>1659939584</span> <span class=o>(</span>89%<span class=o>)</span>  4250M <span class=o>(</span>228%<span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><p>So we are obviously hitting the roof when it comes to resources. But why?</p><h2 id=node-overhead>Node overhead</h2><p>We use <code>Standard DS1 v2</code> instances as AKS nodes and they have 1 CPU core and 3.5 GiB memory.</p><p>The output of <code>kubectl describe node</code> also gives us info on the Capacity (total node size) and Allocatable (resources available to run Pods).</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Capacity:
</span></span><span class=line><span class=cl> cpu:                            1
</span></span><span class=line><span class=cl> memory:                         3500452Ki
</span></span><span class=line><span class=cl>Allocatable:
</span></span><span class=line><span class=cl> cpu:                            940m
</span></span><span class=line><span class=cl> memory:                         1814948Ki
</span></span></code></pre></td></tr></table></div></div><p>So we have lost <strong>60 millicores / 6%</strong> of CPU and <strong>1685MiB / 48%</strong> of memory. The next question is if this increases linearly with node size (the percentage of resources lost is the same regardless of node size) or is fixed (always reserves 60 millicores and 1685Mi of memory), or a combination.</p><p>I connect to another cluster that has double the node size (<code>Standard DS2 v2</code>) and compare:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Capacity:
</span></span><span class=line><span class=cl> cpu:                            2
</span></span><span class=line><span class=cl> memory:                         7113160Ki
</span></span><span class=line><span class=cl>Allocatable:
</span></span><span class=line><span class=cl> cpu:                            1931m
</span></span><span class=line><span class=cl> memory:                         4667848Ki
</span></span></code></pre></td></tr></table></div></div><p>So for this the loss is <strong>69 millicores / 3.5%</strong> of CPU and <strong>2445MiB / 35%</strong> of memory.</p><p>So CPU reservations are close to fixed regardless of node size while memory reservations are influenced by node size but luckily not linearly.</p><p>What causes this &ldquo;waste&rdquo;? Reading up on <a class=link href=https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ target=_blank rel=noopener>kubernetes.io</a> gives a few clues. Kubelet will reserve CPU and memory resources for itself and other Kubernetes processes. It will also reserve a portion of memory to act as a buffer whenever a Pod is going beyond it&rsquo;s memory limits to avoid risking System OOM, potentially making the whole node unstable.</p><p>To figure out what these are configured to we log in to an actual AKS node&rsquo;s console and run <code>ps ax|grep kube</code> and the output looks like this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=o>/</span><span class=n>usr</span><span class=o>/</span><span class=n>local</span><span class=o>/</span><span class=n>bin</span><span class=o>/</span><span class=n>kubelet</span> <span class=o>--</span><span class=n>enable</span><span class=o>-</span><span class=n>server</span> <span class=o>--</span><span class=n>node</span><span class=o>-</span><span class=n>labels</span><span class=o>=</span><span class=n>node</span><span class=o>-</span><span class=n>role</span><span class=o>.</span><span class=n>kubernetes</span><span class=o>.</span><span class=n>io</span><span class=o>/</span><span class=n>agent</span><span class=o>=</span><span class=p>,</span><span class=n>kubernetes</span><span class=o>.</span><span class=n>io</span><span class=o>/</span><span class=n>role</span><span class=o>=</span><span class=n>agent</span><span class=p>,</span><span class=n>agentpool</span><span class=o>=</span><span class=n>nodepool1</span><span class=p>,</span><span class=n>storageprofile</span><span class=o>=</span><span class=n>managed</span><span class=p>,</span><span class=n>storagetier</span><span class=o>=</span><span class=n>Premium_LRS</span><span class=p>,</span><span class=n>kubernetes</span><span class=o>.</span><span class=n>azure</span><span class=o>.</span><span class=n>com</span><span class=o>/</span><span class=n>cluster</span><span class=o>=</span><span class=n>MC_clusters_weekly</span><span class=o>-</span><span class=mi>22</span><span class=n>_northeurope</span> <span class=o>--</span><span class=n>v</span><span class=o>=</span><span class=mi>2</span> <span class=o>--</span><span class=n>volume</span><span class=o>-</span><span class=n>plugin</span><span class=o>-</span><span class=n>dir</span><span class=o>=/</span><span class=n>etc</span><span class=o>/</span><span class=n>kubernetes</span><span class=o>/</span><span class=n>volumeplugins</span> <span class=o>--</span><span class=n>address</span><span class=o>=</span><span class=mf>0.0</span><span class=o>.</span><span class=mf>0.0</span> <span class=o>--</span><span class=n>allow</span><span class=o>-</span><span class=n>privileged</span><span class=o>=</span><span class=bp>true</span> <span class=o>--</span><span class=n>anonymous</span><span class=o>-</span><span class=n>auth</span><span class=o>=</span><span class=bp>false</span> <span class=o>--</span><span class=n>authorization</span><span class=o>-</span><span class=n>mode</span><span class=o>=</span><span class=n>Webhook</span> <span class=o>--</span><span class=n>azure</span><span class=o>-</span><span class=n>container</span><span class=o>-</span><span class=n>registry</span><span class=o>-</span><span class=n>config</span><span class=o>=/</span><span class=n>etc</span><span class=o>/</span><span class=n>kubernetes</span><span class=o>/</span><span class=n>azure</span><span class=o>.</span><span class=n>json</span> <span class=o>--</span><span class=n>cgroups</span><span class=o>-</span><span class=n>per</span><span class=o>-</span><span class=n>qos</span><span class=o>=</span><span class=bp>true</span> <span class=o>--</span><span class=n>client</span><span class=o>-</span><span class=n>ca</span><span class=o>-</span><span class=n>file</span><span class=o>=/</span><span class=n>etc</span><span class=o>/</span><span class=n>kubernetes</span><span class=o>/</span><span class=n>certs</span><span class=o>/</span><span class=n>ca</span><span class=o>.</span><span class=n>crt</span> <span class=o>--</span><span class=n>cloud</span><span class=o>-</span><span class=n>config</span><span class=o>=/</span><span class=n>etc</span><span class=o>/</span><span class=n>kubernetes</span><span class=o>/</span><span class=n>azure</span><span class=o>.</span><span class=n>json</span> <span class=o>--</span><span class=n>cloud</span><span class=o>-</span><span class=n>provider</span><span class=o>=</span><span class=n>azure</span> <span class=o>--</span><span class=n>cluster</span><span class=o>-</span><span class=n>dns</span><span class=o>=</span><span class=mf>10.2</span><span class=o>.</span><span class=mf>0.10</span> <span class=o>--</span><span class=n>cluster</span><span class=o>-</span><span class=n>domain</span><span class=o>=</span><span class=n>cluster</span><span class=o>.</span><span class=n>local</span> <span class=o>--</span><span class=n>enforce</span><span class=o>-</span><span class=n>node</span><span class=o>-</span><span class=n>allocatable</span><span class=o>=</span><span class=n>pods</span> <span class=o>--</span><span class=n>event</span><span class=o>-</span><span class=n>qps</span><span class=o>=</span><span class=mi>0</span> <span class=o>--</span><span class=n>eviction</span><span class=o>-</span><span class=n>hard</span><span class=o>=</span><span class=n>memory</span><span class=o>.</span><span class=n>available</span><span class=o>&lt;</span><span class=mi>750</span><span class=n>Mi</span><span class=p>,</span><span class=n>nodefs</span><span class=o>.</span><span class=n>available</span><span class=o>&lt;</span><span class=mi>10</span><span class=o>%</span><span class=p>,</span><span class=n>nodefs</span><span class=o>.</span><span class=n>inodesFree</span><span class=o>&lt;</span><span class=mi>5</span><span class=o>%</span> <span class=o>--</span><span class=n>feature</span><span class=o>-</span><span class=n>gates</span><span class=o>=</span><span class=n>PodPriority</span><span class=o>=</span><span class=bp>true</span><span class=p>,</span><span class=n>RotateKubeletServerCertificate</span><span class=o>=</span><span class=bp>true</span> <span class=o>--</span><span class=n>image</span><span class=o>-</span><span class=n>gc</span><span class=o>-</span><span class=n>high</span><span class=o>-</span><span class=n>threshold</span><span class=o>=</span><span class=mi>85</span> <span class=o>--</span><span class=n>image</span><span class=o>-</span><span class=n>gc</span><span class=o>-</span><span class=n>low</span><span class=o>-</span><span class=n>threshold</span><span class=o>=</span><span class=mi>80</span> <span class=o>--</span><span class=n>image</span><span class=o>-</span><span class=n>pull</span><span class=o>-</span><span class=n>progress</span><span class=o>-</span><span class=n>deadline</span><span class=o>=</span><span class=mi>30</span><span class=n>m</span> <span class=o>--</span><span class=n>keep</span><span class=o>-</span><span class=n>terminated</span><span class=o>-</span><span class=n>pod</span><span class=o>-</span><span class=n>volumes</span><span class=o>=</span><span class=bp>false</span> <span class=o>--</span><span class=n>kube</span><span class=o>-</span><span class=n>reserved</span><span class=o>=</span><span class=n>cpu</span><span class=o>=</span><span class=mi>60</span><span class=n>m</span><span class=p>,</span><span class=n>memory</span><span class=o>=</span><span class=mi>896</span><span class=n>Mi</span> <span class=o>--</span><span class=n>kubeconfig</span><span class=o>=/</span><span class=k>var</span><span class=o>/</span><span class=n>lib</span><span class=o>/</span><span class=n>kubelet</span><span class=o>/</span><span class=n>kubeconfig</span> <span class=o>--</span><span class=nb>max</span><span class=o>-</span><span class=n>pods</span><span class=o>=</span><span class=mi>110</span> <span class=o>--</span><span class=n>network</span><span class=o>-</span><span class=n>plugin</span><span class=o>=</span><span class=n>cni</span> <span class=o>--</span><span class=n>node</span><span class=o>-</span><span class=n>status</span><span class=o>-</span><span class=n>update</span><span class=o>-</span><span class=n>frequency</span><span class=o>=</span><span class=mi>10</span><span class=n>s</span> <span class=o>--</span><span class=n>non</span><span class=o>-</span><span class=n>masquerade</span><span class=o>-</span><span class=n>cidr</span><span class=o>=</span><span class=mf>0.0</span><span class=o>.</span><span class=mf>0.0</span><span class=o>/</span><span class=mi>0</span> <span class=o>--</span><span class=n>pod</span><span class=o>-</span><span class=n>infra</span><span class=o>-</span><span class=n>container</span><span class=o>-</span><span class=n>image</span><span class=o>=</span><span class=n>k8s</span><span class=o>.</span><span class=n>gcr</span><span class=o>.</span><span class=n>io</span><span class=o>/</span><span class=n>pause</span><span class=o>-</span><span class=n>amd64</span><span class=p>:</span><span class=mf>3.1</span> <span class=o>--</span><span class=n>pod</span><span class=o>-</span><span class=n>manifest</span><span class=o>-</span><span class=n>path</span><span class=o>=/</span><span class=n>etc</span><span class=o>/</span><span class=n>kubernetes</span><span class=o>/</span><span class=n>manifests</span> <span class=o>--</span><span class=n>pod</span><span class=o>-</span><span class=nb>max</span><span class=o>-</span><span class=n>pids</span><span class=o>=-</span><span class=mi>1</span> <span class=o>--</span><span class=n>rotate</span><span class=o>-</span><span class=n>certificates</span><span class=o>=</span><span class=bp>false</span> <span class=o>--</span><span class=n>streaming</span><span class=o>-</span><span class=n>connection</span><span class=o>-</span><span class=n>idle</span><span class=o>-</span><span class=n>timeout</span><span class=o>=</span><span class=mi>5</span><span class=n>m</span>
</span></span></code></pre></td></tr></table></div></div><blockquote><p>To log in to the console of a node, go to the MC_resourcegroup_clustername_region resource-group and select the VM. Then go to <code>Boot diagnostics</code> and enable it. Go to <code>Reset password</code> to create yourself a user and then <code>Serial console</code> to log in and execute commands.</p></blockquote><p>We can see <code>--kube-reserved=cpu=60m,memory=896Mi</code> and <code>--eviction-hard=memory.available&lt;750Mi</code> which adds up to <code>1646Mi</code> which is pretty close to the <code>1685Mi</code> that was the gap between Capacity and Allocatable.</p><p>We also do this on a <code>Standard DS2 v2</code> node and get <code>--kube-reserved=cpu=69m,memory=1638Mi</code> and <code>--eviction-hard=memory.available&lt;750Mi</code>.</p><p>So we can see that the memory of <code>kube-reserved</code> grows almost linearly and seems to always be about 20-25% while CPU reservations are almost the same. The memory eviction buffer is always fixed at <code>750Mi</code> which would mean bigger resource waste as nodes decrease in size.</p><h4 id=cpu>CPU</h4><div class=table-wrapper><table><thead><tr><th></th><th style=text-align:right>Standard DS1 v2</th><th style=text-align:right>Standard DS2 v2</th></tr></thead><tbody><tr><td>VM capacity</td><td style=text-align:right>1.000m</td><td style=text-align:right>2.000m</td></tr><tr><td>kube-reserved</td><td style=text-align:right>-60m</td><td style=text-align:right>-69m</td></tr><tr><td>Allocatable</td><td style=text-align:right>940m</td><td style=text-align:right>1.931m</td></tr><tr><td>Allocatable %</td><td style=text-align:right>94%</td><td style=text-align:right>96.5%</td></tr></tbody></table></div><h4 id=memory>Memory</h4><div class=table-wrapper><table><thead><tr><th></th><th style=text-align:right>Standard DS1 v2</th><th style=text-align:right>Standard DS2 v2</th></tr></thead><tbody><tr><td>VM capacity</td><td style=text-align:right>3.500Mi</td><td style=text-align:right>7.113Mi</td></tr><tr><td>kube-reserved</td><td style=text-align:right>-896Mi</td><td style=text-align:right>-1.638Mi</td></tr><tr><td>Eviction buf</td><td style=text-align:right>-750Mi</td><td style=text-align:right>-750Mi</td></tr><tr><td>Allocatable</td><td style=text-align:right>1.814Mi</td><td style=text-align:right>4.667Mi</td></tr><tr><td>Allocatable %</td><td style=text-align:right>52%</td><td style=text-align:right>65%</td></tr></tbody></table></div><h2 id=node-pods-daemonsets>Node pods (DaemonSets)</h2><p>We have some Pods that run on every node, and they are installed by default by AKS. We get the resource limits of these by describing either the pods or the daemonsets.</p><h4 id=cpu-1>CPU</h4><div class=table-wrapper><table><thead><tr><th></th><th style=text-align:right>Standard DS1 v2</th><th style=text-align:right>Standard DS2 v2</th></tr></thead><tbody><tr><td>Allocatable</td><td style=text-align:right>940m</td><td style=text-align:right>1.931m</td></tr><tr><td>kube-system/calico-node</td><td style=text-align:right>-250m</td><td style=text-align:right>-250m</td></tr><tr><td>kube-system/kube-proxy</td><td style=text-align:right>-100m</td><td style=text-align:right>-100m</td></tr><tr><td>kube-system/kube-svc-redirect</td><td style=text-align:right>-5m</td><td style=text-align:right>-5m</td></tr><tr><td>Available</td><td style=text-align:right>585m</td><td style=text-align:right>1.576m</td></tr><tr><td>Available %</td><td style=text-align:right>58%</td><td style=text-align:right>81%</td></tr></tbody></table></div><h4 id=memory-1>Memory</h4><div class=table-wrapper><table><thead><tr><th></th><th style=text-align:right>Standard DS1 v2</th><th style=text-align:right>Standard DS2 v2</th></tr></thead><tbody><tr><td>Allocatable</td><td style=text-align:right>1.814Mi</td><td style=text-align:right>4.667Mi</td></tr><tr><td>kube-system/kube-svc-redirect</td><td style=text-align:right>-32Mi</td><td style=text-align:right>-32Mi</td></tr><tr><td>Available</td><td style=text-align:right>1.782Mi</td><td style=text-align:right>4.635Mi</td></tr><tr><td>Available %</td><td style=text-align:right>50%</td><td style=text-align:right>61%</td></tr></tbody></table></div><p>So for <code>Standard DS1 v2</code> nodes we have about 0.5 CPU and 1.7GiB memory per node for pods. And for <code>Standard DS2 v2</code> nodes it&rsquo;s about 1.5 CPU and 4.6GiB memory.</p><h2 id=kube-system-pods>kube-system pods</h2><p>Now lets add some standard Kubernetes pods we need to run. As far as I know these are pretty much fixed for a cluster and not related to node size or count.</p><div class=table-wrapper><table><thead><tr><th>Deployment</th><th style=text-align:right>CPU</th><th style=text-align:right>Memory</th></tr></thead><tbody><tr><td>kube-system/kubernetes-dashboard</td><td style=text-align:right>100m</td><td style=text-align:right>50Mi</td></tr><tr><td>kube-system/tunnelfront</td><td style=text-align:right>10m</td><td style=text-align:right>64Mi</td></tr><tr><td>kube-system/coredns (x2)</td><td style=text-align:right>200m</td><td style=text-align:right>140Mi</td></tr><tr><td>kube-system/coredns-autoscaler</td><td style=text-align:right>20m</td><td style=text-align:right>10Mi</td></tr><tr><td>kube-system/heapster</td><td style=text-align:right>130m</td><td style=text-align:right>230Mi</td></tr><tr><td>Sum</td><td style=text-align:right>460m</td><td style=text-align:right>494Mi</td></tr></tbody></table></div><h2 id=third-party-pods>Third party pods</h2><div class=table-wrapper><table><thead><tr><th>Deployment</th><th style=text-align:right>CPU</th><th style=text-align:right>Memory</th></tr></thead><tbody><tr><td>grafana</td><td style=text-align:right>200m</td><td style=text-align:right>500Mi</td></tr><tr><td>prometheus-operator</td><td style=text-align:right>500m</td><td style=text-align:right>1.000Mi</td></tr><tr><td>prometheus-alertmanager</td><td style=text-align:right>100m</td><td style=text-align:right>225Mi</td></tr><tr><td>flux</td><td style=text-align:right>50m</td><td style=text-align:right>64Mi</td></tr><tr><td>flux-helm-operator</td><td style=text-align:right>50m</td><td style=text-align:right>64Mi</td></tr><tr><td>Sum</td><td style=text-align:right>900m</td><td style=text-align:right>1.853Mi</td></tr></tbody></table></div><h2 id=radix-platform-pods>Radix platform pods</h2><div class=table-wrapper><table><thead><tr><th>Deployment</th><th style=text-align:right>CPU</th><th style=text-align:right>Memory</th></tr></thead><tbody><tr><td>radix-api-prod/server (x2)</td><td style=text-align:right>200m</td><td style=text-align:right>400Mi</td></tr><tr><td>radix-api-qa/server (x2)</td><td style=text-align:right>100m</td><td style=text-align:right>200Mi</td></tr><tr><td>radix-canary-golang-dev/www</td><td style=text-align:right>40m</td><td style=text-align:right>500Mi</td></tr><tr><td>radix-canary-golang-prod/www</td><td style=text-align:right>40m</td><td style=text-align:right>500Mi</td></tr><tr><td>radix-platform-prod/public-site</td><td style=text-align:right>5m</td><td style=text-align:right>10Mi</td></tr><tr><td>radix-web-console-prod/web</td><td style=text-align:right>10m</td><td style=text-align:right>42Mi</td></tr><tr><td>radix-web-console-qa/web</td><td style=text-align:right>5m</td><td style=text-align:right>21Mi</td></tr><tr><td>radix-github-webhook-prod/webhook</td><td style=text-align:right>10m</td><td style=text-align:right>30Mi</td></tr><tr><td>radix-github-webhook-prod/webhook</td><td style=text-align:right>5m</td><td style=text-align:right>15Mi</td></tr><tr><td>Sum</td><td style=text-align:right>415m</td><td style=text-align:right>1.718Mi</td></tr></tbody></table></div><p>If we add up the resource usage of these groups of workloads and see the total available resources on our 4 node Standard DS1 v2 clusters we are left with 0.56 CPU cores (14%) and 3GB of memory (22%):</p><div class=table-wrapper><table><thead><tr><th>Workload</th><th style=text-align:right>CPU</th><th style=text-align:right>Memory</th></tr></thead><tbody><tr><td>kube-system</td><td style=text-align:right>460m</td><td style=text-align:right>494Mi</td></tr><tr><td>third-party</td><td style=text-align:right>900m</td><td style=text-align:right>1.853Mi</td></tr><tr><td>radix-platform</td><td style=text-align:right>415m</td><td style=text-align:right>1.718Mi</td></tr><tr><td>Sum</td><td style=text-align:right>1.760m</td><td style=text-align:right>4.020Mi</td></tr><tr><td>Available on 4x DS1</td><td style=text-align:right>2.340m</td><td style=text-align:right>7.128Mi</td></tr><tr><td>Available for workloads</td><td style=text-align:right>565m</td><td style=text-align:right>3.063Mi</td></tr></tbody></table></div><p>Though surprising that we lost this much resources before being able to deploy our actual customer applications, it should still be a bit of headroom.</p><p>Going further I checked the resource requests on 8 customer pods deployed in 4 environments (namespaces). Even though none of them had a resource configuration in their <code>radixconfig.yaml</code> files they still had resource requests and limits. Not surprising since we use LimitRange to set default resource requests and limits. The surprise was that half of them had 50Mi of memory and the other half 500Mi, seemingly at random.</p><p>It turns out that we did an update to the LimitRange values a few days ago but that only applies to new Pods, so depending on if the Pods got re-created for any reason they may or may not have the old request of 500Mi, which in our case of small clusters will quickly drain the available resources.</p><blockquote><p>Read more about LimitRange here: <a class=link href=https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/ target=_blank rel=noopener>kubernetes.io</a> , and here is the commit that eventually trickled down to reduce memory usage: <a class=link href=https://github.com/equinor/radix-operator/commit/f022fcde993efdf6cbcafb2c6632707a823a2a27 target=_blank rel=noopener>github.com</a></p></blockquote><h2 id=pod-scheduling>Pod scheduling</h2><p>Depending on the weight between CPU and memory requests and how often things get destroyed and re-created you may find yourself in a situation where you have enough resources in your cluster but new workloads are still Pending. This can happen when one resource type (e.g. CPU) is filled before another (e.g. memory), leading one or more resources to be stranded and unlikely to be utilized.</p><p>Imagine for example a cluster that is already utilized like this:</p><div class=table-wrapper><table><thead><tr><th></th><th>CPU</th><th>Memory</th></tr></thead><tbody><tr><td>node0</td><td>94%</td><td>86%</td></tr><tr><td>node1</td><td>80%</td><td>89%</td></tr><tr><td>node2</td><td>98%</td><td>60%</td></tr></tbody></table></div><p>Scheduling a workload that requests 15% CPU and 20% memory cannot be scheduled since there are no nodes fulfilling both requirements. In theory there is probably a CPU intensive Pod on node2 that could be moved to node1 but Kubernetes does not do re-scheduling to optimize utilization. It can do re-scheduling based on Pod priority (<a class=link href=https://medium.com/@dominik.tornow/the-kubernetes-scheduler-cd429abac02f target=_blank rel=noopener>medium.com</a>) and there is an incubator project (<a class=link href=https://akomljen.com/meet-a-kubernetes-descheduler/ target=_blank rel=noopener>akomljen.com</a>) that can try to drain nodes with low utilization.</p><p>So for the foreseable future keeping in mind that resources can get stranded and that looking at the sum of cluster resources and sum of cluster resource demand might be misleading.</p><h2 id=calico-node>calico-node</h2><p>The biggest source of waste on our small clusters is <code>calico-node</code> which is installed on every node and requests 25% of a CPU core while only using 2.5-3% CPU:</p><p><img src=/p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu.png width=1291 height=392 srcset="/p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu_hu1477443119961749415.png 480w, /p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu_hu8581044549333248792.png 1024w" loading=lazy alt="calico-node cpu usage" class=gallery-image data-flex-grow=329 data-flex-basis=790px></p><p>The request is originally set here <a class=link href=https://github.com/Azure/aks-engine/blob/master/parts/k8s/containeraddons/kubernetesmasteraddons-calico-daemonset.yaml target=_blank rel=noopener>github.com</a> but I have not got into why that number was choosen. Next steps would be to do some benchmarking of <code>calico-node</code> to smoke out it&rsquo;s performance characteristics to see if it would be safe to lower the resource requests, but that is out of scope for now.</p><h1 id=conclusion>Conclusion</h1><ul><li>By increasing node size from <code>Standard DS1 v2</code> to <code>Standard DS2 v2</code> we also increase the available CPU from 58% per node to 81% per node. Available memory increases from 50% to 61% per node.</li><li>With a total platform requirement of 3-4GB of memory and 4.6GB available on <code>Standard DS2 v2</code> we might have more resources for actual workloads on a 1-node <code>Standard DS2 v2</code> cluster than a 3-node <code>Standard DS1 v2</code> cluster!</li><li>Beware of stranded resources limiting the utilization you can achieve across a cluster.</li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/kubernetes/>Kubernetes</a>
<a href=/tags/azure/>Azure</a></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Jun 04, 2019 00:00 UTC</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/><div class=article-image><img src=/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/2019-02-23-disk-performance-on-aks-part-1.ab18dd1b8670f56fe898797506d57abc_hu17823714411359167148.png width=250 height=150 loading=lazy alt="Featured image of post Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking" data-hash="md5-qxjdG4Zw9W/omHl1BtV6vA=="></div><div class=article-details><h2 class=article-title>Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking</h2></div></a></article><article class=has-image><a href=/p/managed-kubernetes-on-microsoft-azure-english/><div class=article-image><img src=/p/managed-kubernetes-on-microsoft-azure-english/2017-12-23-managed-kubernetes-on-azure-eng.bf0fcb04c841710921f10d2e885d8555_hu4992157927612583129.png width=250 height=150 loading=lazy alt="Featured image of post Managed Kubernetes on Microsoft Azure (English)" data-hash="md5-vw/LBMhBcQkh8Q0uiF2FVQ=="></div><div class=article-details><h2 class=article-title>Managed Kubernetes on Microsoft Azure (English)</h2></div></a></article><article class=has-image><a href=/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/><div class=article-image><img src=/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/2017-12-23-managed-kubernetes-on-azure.bf0fcb04c841710921f10d2e885d8555_hu4992157927612583129.png width=250 height=150 loading=lazy alt="Featured image of post Managed Kubernetes p√• Microsoft Azure (Norwegian)" data-hash="md5-vw/LBMhBcQkh8Q0uiF2FVQ=="></div><div class=article-details><h2 class=article-title>Managed Kubernetes p√• Microsoft Azure (Norwegian)</h2></div></a></article><article class=has-image><a href=/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/><div class=article-image><img src=/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/cover.b44ff6851020084489a2bbba9c34cdea_hu14327566051345104457.webp width=250 height=150 loading=lazy alt="Featured image of post Kubernetes: Don't use NodeLocal DNSCache on GKE (without Cloud DNS)" data-hash="md5-tE/2hRAgCESJoru6nDTN6g=="></div><div class=article-details><h2 class=article-title>Kubernetes: Don't use NodeLocal DNSCache on GKE (without Cloud DNS)</h2></div></a></article><article class=has-image><a href=/p/kubernetes-sidecar-config-drift/><div class=article-image><img src=/p/kubernetes-sidecar-config-drift/2022-05-03-kubernetes-sidecar-config-drift.099fc947effb12fa5df1b55ff2abbf0f_hu6448980740578202474.png width=250 height=150 loading=lazy alt="Featured image of post Kubernetes Sidecar Config Drift" data-hash="md5-CZ/JR+/7Evpd8bVf8qu/Dw=="></div><div class=article-details><h2 class=article-title>Kubernetes Sidecar Config Drift</h2></div></a></article></div></div></aside><div class=disqus-container></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2014 -
2024 blog.stian.omg.lol</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>