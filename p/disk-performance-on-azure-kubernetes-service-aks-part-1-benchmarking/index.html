<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Understanding the characteristics of disk performance of a platform might be more important than you think. If disk resources are not correctly matched to your workload, your performance will suffer and might lead you to incorrectly diagnose a problem as being related to CPU or memory.\nThe defaults might also not give you the performance you expect.\nIn this first post on troubleshooting some disk performance issues on Azure Kubernetes Service (AKS) we will benchmark Azure Premium SSD to find how workloads affect performance and which metrics to monitor to know when troubleshooting potential disk issues.\n"><title>Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking</title>
<link rel=canonical href=https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking"><meta property='og:description' content="Understanding the characteristics of disk performance of a platform might be more important than you think. If disk resources are not correctly matched to your workload, your performance will suffer and might lead you to incorrectly diagnose a problem as being related to CPU or memory.\nThe defaults might also not give you the performance you expect.\nIn this first post on troubleshooting some disk performance issues on Azure Kubernetes Service (AKS) we will benchmark Azure Premium SSD to find how workloads affect performance and which metrics to monitor to know when troubleshooting potential disk issues.\n"><meta property='og:url' content='https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/'><meta property='og:site_name' content='blog.stian.omg.lol'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='kubernetes'><meta property='article:tag' content='azure'><meta property='article:published_time' content='2019-02-23T00:00:00+00:00'><meta property='article:modified_time' content='2019-02-23T00:00:00+00:00'><meta property='og:image' content='https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1.png'><meta name=twitter:title content="Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking"><meta name=twitter:description content="Understanding the characteristics of disk performance of a platform might be more important than you think. If disk resources are not correctly matched to your workload, your performance will suffer and might lead you to incorrectly diagnose a problem as being related to CPU or memory.\nThe defaults might also not give you the performance you expect.\nIn this first post on troubleshooting some disk performance issues on Azure Kubernetes Service (AKS) we will benchmark Azure Premium SSD to find how workloads affect performance and which metrics to monitor to know when troubleshooting potential disk issues.\n"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1.png'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/profile-picture_hu6892409687065925510.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ü§∑‚Äç‚ôÇÔ∏è</span></figure><div class=site-meta><h1 class=site-name><a href=/>blog.stian.omg.lol</a></h1><h2 class=site-description>Technology. Aviation. Philosophy.</h2></div></header><ol class=menu-social><li><a href=https://github.com/StianOvrevage target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><ol><li><ol><li><a href=#microsoft-azure>Microsoft Azure</a></li></ol></li></ol></li><li><a href=#corrections>Corrections</a></li><li><a href=#background>Background</a><ol><li><a href=#metric-methodologies>Metric Methodologies</a></li><li><a href=#storage-background>Storage Background</a></li></ol></li><li><a href=#what-to-measure>What to measure?</a></li><li><a href=#how-to-measure-disk>How to measure disk</a></li><li><a href=#how-to-measure-disk-on-azure-kubernetes-service>How to measure disk on Azure Kubernetes Service</a></li><li><a href=#test-results>Test results</a><ol><li><a href=#test-1---learning-to-dislike-azure-cache>Test 1 - Learning to dislike Azure Cache</a></li><li><a href=#test-2---disable-azure-cache---enable-os-cache>Test 2 - Disable Azure Cache - enable OS cache</a></li><li><a href=#test-3---disable-os-cache>Test 3 - Disable OS cache</a></li><li><a href=#test-4---increase-io-depth>Test 4 - Increase IO depth</a></li><li><a href=#test-5---larger-block-size-smaller-io-depth>Test 5 - Larger block size, smaller IO depth</a></li><li><a href=#test-6---enable-os-cache>Test 6 - Enable OS cache</a></li><li><a href=#test-7---random-writes-small-block-size>Test 7 - Random writes, small block size</a></li><li><a href=#test-8---large-block-size>Test 8 - Large block size</a></li></ol></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/><img src=/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1_hu3066909101112033904.png srcset="/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1_hu3066909101112033904.png 800w, /p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1_hu7438943432135093458.png 1600w" width=800 height=581 loading=lazy alt="Featured image of post Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking"></a></div><div class=article-details><header class=article-category><a href=/categories/technology/ style=background-color:#2a9d8f;color:#fff>Technology</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/>Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Feb 23, 2019</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>14 minute read</time></div></footer></div></header><section class=article-content><p>Understanding the characteristics of disk performance of a platform might be more important than you think. If disk resources are not correctly matched to your workload, your performance will suffer and might lead you to incorrectly diagnose a problem as being related to CPU or memory.</p><p>The defaults might also not give you the performance you expect.</p><p>In this first post on troubleshooting some disk performance issues on Azure Kubernetes Service (AKS) we will benchmark Azure Premium SSD to find how workloads affect performance and which metrics to monitor to know when troubleshooting potential disk issues.</p><p>TLDR:</p><ul><li>Disable Azure cache for workloads with high number of random writes</li><li>Use a P15 (256GB) or larger Premium SSD even though you might only need a fraction of it.</li></ul><p>Table of contents</p><ul><li><a class=link href=#Background>Background</a><ul><li><a class=link href=#MetricsMethodologies>Metric Methodologies</a></li><li><a class=link href=#StorageBackground>Storage Background</a></li></ul></li><li><a class=link href=#WhatToMeasure>What to measure?</a></li><li><a class=link href=#HowToMeasureDisk>How to measure disk</a></li><li><a class=link href=HowToMeasureDiskOnAKS>How to measure disk on Azure Kubernetes Service</a></li><li><a class=link href=#Tests>Test results</a><ul><li><a class=link href=#Test1>Test 1 - Learning to dislike Azure Cache</a></li><li><a class=link href=#Test2>Test 2 - Disable Azure Cache - enable OS cache</a></li><li><a class=link href=#Test3>Test 3 - Disable OS cache</a></li><li><a class=link href=#Test4>Test 4 - Increase IO depth</a></li><li><a class=link href=#Test5>Test 5 - Larger block size, smaller IO depth</a></li><li><a class=link href=#Test6>Test 6 - Enable OS cache</a></li><li><a class=link href=#Test7>Test 7 - Random writes, small block size</a></li><li><a class=link href=#Test8>Test 8 - Large block size</a></li></ul></li><li><a class=link href=#Conclusion>Conclusion</a></li></ul><h4 id=microsoft-azure>Microsoft Azure</h4><blockquote><p><a class=link href=https://azure.microsoft.com/en-us/free/ target=_blank rel=noopener>If you don&rsquo;t have a Azure subscription already you can try services for $200 for 30 days.</a> The VM size <strong>Standard_B2s</strong> is Burstable, has 2vCPU, 4GB RAM, 8GB temp storage and costs roughly $38 / month. For $200 you can have a cluster of 3-4 B2s nodes plus traffic, loadbalancers and other additional costs.</p></blockquote><blockquote><p>See my blog post <a class=link href=2017-12-23-managed-kubernetes-on-azure.md>Managed Kubernetes on Microsoft Azure (English)</a> for information on how to get up and running with Kubernetes on Azure.</p></blockquote><blockquote><p><em>I have no affiliation with Microsoft Azure except using them through work.</em></p></blockquote><h2 id=corrections>Corrections</h2><p><strong>February 2020</strong>: Some of my previous knowledge and assumptions were not correct when applied to a cloud + Docker environment, as <a class=link href=https://github.com/jnoller/kubernaughty/issues/46 target=_blank rel=noopener>explained by
AKS PM Jesse Noller on GitHub</a>.</p><p>One of the issues is that even accessing a &ldquo;data disk&rdquo; will incur IOPS on the OS disk, and throttling of the OS disk will also constraint IOPS on the data disks.</p><p><a id=Background></a></p><h2 id=background>Background</h2><p>I&rsquo;m part of a team at Equinor building an internal PaaS based on Kubernetes running on AKS (Azure managed Kubernetes). We use Prometheus for monitoring each cluster as well as InfluxDB for collecting metrics from k6io which runs continous tests on our public endpoints.</p><p>A couple of weeks ago we discovered some potential problems with both Prometheus and InfluxDB with memory usage and restarts. High CPU usage of type <code>iowait</code> suggested that there might be some disk issues contributing to the problems.</p><blockquote><p>iowait: &ldquo;Percentage of time that the CPU or CPUs were idle during which the system had an outstanding disk I/O request.&rdquo; (<a class=link href="https://support.hpe.com/hpsc/doc/public/display?docId=c02783994" target=_blank rel=noopener>hpe.com</a>). You can see <code>iowait</code> on your Linux system by running <code>top</code> and looking at the <code>wa</code> percentage.</p><p>PS: You can have a disk IO bottleneck even with low <code>iowait</code>, and a high <code>iowait</code> does not always indicate a disk IO bottleneck (<a class=link href="https://www.ibm.com/developerworks/community/blogs/AIXDownUnder/entry/iowait_a_misleading_indicator_of_i_o_performance54?lang=en" target=_blank rel=noopener>ibm.com</a>).</p></blockquote><p>First off we need to benchmark the underlying disk to get an understanding of it&rsquo;s performance limits and characteristics. That is what we will cover in this post.</p><p><a id=MetricsMethodologies></a></p><h3 id=metric-methodologies>Metric Methodologies</h3><p>There are two helpful methodologies when monitoring information systems. The first one is Utilization, Saturation and Errors (USE) from <a class=link href=http://www.brendangregg.com/usemethod.html target=_blank rel=noopener>Brendan Gregg</a> and the second one is Rate, Errors, Duration (RED) from <a class=link href=https://www.slideshare.net/weaveworks/monitoring-microservices target=_blank rel=noopener>Tom Wilkie</a>. RED is best suited when observing workloads and transactions while USE is best suited for observing resources.</p><p>I&rsquo;ll be using the USE method here. USE can be summarised as:</p><ul><li><strong>For every resource, check utilization, saturation, and errors.</strong><ul><li><strong>resource</strong>: all physical server functional components (CPUs, disks, busses, &mldr;)</li><li><strong>utilization</strong>: the average time that the resource was busy servicing work</li><li><strong>saturation</strong>: the degree to which the resource has extra work which it can&rsquo;t service, often queued</li><li><strong>errors</strong>: the count of error events</li></ul></li></ul><p><a id=StorageBackground></a></p><h3 id=storage-background>Storage Background</h3><p>Disk usage has two dimensions, throughput/bandwidth(BW) and operations per second (IOPS), and the underlying storage system will have upper limits of how much data it can receive (BW) and the number of operations it can perform per second (IOPS).</p><blockquote><p><strong>Background - harddrive types</strong>: harddrives come in two types, Solid State Disks (SSD) and spindle (HDD). A SSD disk is a microship capable of permanently storing data while a HDD uses spinning platters to store data. HDDs have a fixed rate of rotation (RPM), typically 5.400 and 7.200 RPM for lower cost drives for home use and higher cost 10.000 and 15.000 RPM drives for server use. Over the last 20 years of HDDs their storage density has increased, but the RPM has largely stayed the same. A disk with twice the density (500GB to 1TB for example) can read twice as much data on a single rotation and thus increase the bandwidth significantly. However, reading or writing a random block still requires waiting for the disk to spin enough to reach the relevant sector on the disk. So IOPS has not increased much for HDDs and is still a low 125-150 IOPS for a 10.000 RPM enterprise disk. A SSD does not have any moving parts so is able to reach MUCH higher IOPS. A low end Samsung 960 EVO with 500GB capacity costs $150 and can achieve a whopping 330.000 IOPS! (<a class=link href=https://en.wikipedia.org/wiki/IOPS target=_blank rel=noopener>wikipedia.com</a>)</p></blockquote><blockquote><p><strong>Background - access patterns</strong>: The way a program uses storage also has a huge impact on the performance one can achieve. Sequential access is when we read or write a large file. When this happens the operating system and harddrive can optimize and &ldquo;merge&rdquo; operations so that we can read or write a much bigger chunk of data at a time. If we can read 1MB at a time 150 times per second we get 150MB/s of bandwidth. However, fully random access where the smallest chunk we read or write is a 4KB block the same 150 IOPS would only give a bandwidth of 0.6MB/s!</p></blockquote><blockquote><p><strong>Background - cloud vs physical</strong>: Now we know what HDDs are limited to a low IOPS and low IOPS combined with a random access pattern gives us a low overall bandwidth. There is a huge gotcha here when it comes to cloud. On Azure when using Premium Managed SSD the IOPS you are given is a factor of the disk size you provision (<a class=link href=https://azure.microsoft.com/en-us/pricing/details/managed-disks/ target=_blank rel=noopener>microsoft.com</a>). A 512GB disk is limited to 2.300 IOPS and 150MB/s. With 100% random access that only gives about 9MB/s of bandwidth!</p></blockquote><blockquote><p><strong>Background - OS caching</strong>: To overcome some of the limitations of the underlying disk (mostly IOPS) there are potentially several layers of caching involved. Linux file systems can have <code>writeback</code> enabled which causes Linux to temporarily store data that is going to be written to disk in memory. This can give a big performance increase when there are sudden spikes of writes exceeding the performance of the underlying disk. It also increases the chance that operations can be <code>merged</code> where several write operations to areas of the disk that are nearby can be executed as one. This caching works best for sudden peaks and will not necessarily be enough if there is continous random writes to disk. This caching also means that even though an application thinks it has saved some data to disk it can be lost in the case of a power outage or other failure. Applications can also explicitly request <code>direct</code> access where every operation is persisted to disk before receiving a confirmation. This is a trade-off between performance and durability that needs to be decided based on the application itself and the environment.</p></blockquote><blockquote><p><strong>Background - Azure caching</strong>: Azure also provides read and write cache for its <code>disks</code> which is enabled by default. As we will see soon for our use case it&rsquo;s not a good idea to use.</p></blockquote><p><a id=WhatToMeasure></a></p><h2 id=what-to-measure>What to measure?</h2><blockquote><p>These metrics are collected by the Prometheus <code>node-exporter</code> and follows it&rsquo;s naming. I&rsquo;ve also created a dashboard that is available on <a class=link href=https://grafana.com/dashboards/9852 target=_blank rel=noopener>Grafana.com</a>.</p></blockquote><p>With the USE methodology as a guideline and the two separate but related &ldquo;resources&rdquo;, bandwidth and IOPS we can look for some useful metrics.</p><p>Utilization:</p><ul><li><code>rate(node_disk_written_bytes_total)</code> - Write bandwidth. The maximum is given by Azure and is 25MB/s for our disk size.</li><li><code>rate(node_disk_writes_completed_total)</code> - Write operations. The maximum is given by Azure and is 120 IOPS for our disk size.</li><li><code>rate(node_disk_io_time_seconds_total)</code> - Disk active time in percent. The time the disk was busy servicing requests. 100% means fully utilized.</li></ul><p>Saturation:</p><ul><li><code>rate(node_cpu_seconds_total{mode="iowait"}</code> - CPU iowait. The percentage of time a CPU core is blocked from doing useful work because it&rsquo;s waiting for an IO operation to complete (typically disk, but can also be network).</li></ul><p>Useful calculated metrics:</p><ul><li><code>rate(node_disk_write_time_seconds_total) / rate(node_disk_writes_completed_total)</code> - Write latency. How long from a write is requested until it&rsquo;s completed.</li><li><code>rate(node_disk_written_bytes_total) / rate(node_disk_writes_completed_total)</code> - Write size. How big the <strong>average</strong> write operation is. 4KB is minimum and indicates 100% random access while 512KB is maximum and indicates sequential access.</li></ul><p><a id=HowToMeasureDisk></a></p><h2 id=how-to-measure-disk>How to measure disk</h2><p>The best tool for measuring disk performance is <code>fio</code>, even though it might seem a bit intimidating at first due to it&rsquo;s insane number of options.</p><p>Installing <code>fio</code> on Ubuntu:</p><pre><code>apt-get install fio
</code></pre><p><code>fio</code> executes <code>jobs</code> described in a file. Here is the top of our jobs file:</p><pre><code>[global]
ioengine=libaio   # sync|libaio|mmap
group_reporting
thread
size=10g          # Size of test file
cpus_allowed=1    # Only use this CPU core
runtime=300s      # Run test for 5 minutes

[test1]
filename=/tmp/fio-test-file
direct=1          # If value is true, use non-buffered I/O. Non-buffered I/O usually means O_DIRECT
readwrite=write   # read|write|randread|randwrite|readwrite|randrw
iodepth=1         # How many operations to queue to the disk
blocksize=4k
</code></pre><p>The fields we will be changing for the various tests are <code>direct</code>, <code>readwrite</code>, <code>iodepth</code> and <code>blocksize</code>. Save the contents in a file named <code>jobs.fio</code> and we run a test with <code>fio --sector test1 jobs.fio</code> and wait until the test completes.</p><blockquote><p>PS: To run these tests on higher performance hardware and better caching you might want to set <code>runtime</code> to <code>0</code> to have the test run continously and monitor the metrics until performance reaches a steady-state.</p></blockquote><p><a id=HowToMeasureDiskOnAKS></a></p><h2 id=how-to-measure-disk-on-azure-kubernetes-service>How to measure disk on Azure Kubernetes Service</h2><p>For this testing we use a standard Prometheus installation collecting data from <code>node-exporter</code> and visualizing data in Grafana. The dashboard I created for the testing can be found here: <a class=link href=https://grafana.com/dashboards/9852 target=_blank rel=noopener>https://grafana.com/dashboards/9852</a>.</p><p>By default Kubernetes will schedule a Pod to any node that has enough memory and CPU for our workload. Since one of the tests we are going to run are on the OS disk we do not want the Pod to run on the same node as any other disk-intensive application, such as Prometheus.</p><p>Look at which Pods are running with <code>kubectl get pods -o wide</code> and look for a node that does not have any disk-intensive application.</p><p>Then we tag that node with <code>kubectl label nodes aks-nodepool1-37707184-2 tag=disktest</code>. This allows us later to specify that we want to run our testing Pod on that specific node.</p><hr><p>A StorageClass in Kubernetes is a specification of a underlying disk that Pods can request usage of through <code>volumeClaimTemplates</code>. AKS comes with a default StorageClass <code>managed-premium</code> that has caching enabled. Most of these tests require the Azure cache disabled so create a new StorageClass <code>managed-premium-retain-nocache</code>:</p><pre><code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: managed-premium-retain-nocache
provisioner: kubernetes.io/azure-disk
reclaimPolicy: Retain
parameters:
  storageaccounttype: Premium_LRS
  kind: Managed
  cachingmode: None
</code></pre><p>You can add it to your cluster with:</p><pre><code>kubectl apply -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2019-02-23-disk-performance-on-aks-part-1/storageclass.yaml
</code></pre><hr><p>Next we create a StatefulSet that uses a <code>volumeClaimTemplate</code> to request a 250GB Azure disk. This provisions a P15 Azure Premium SSD with 125MB/s bandwidth and 1100 IOPS:</p><pre><code>kubectl apply -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2019-02-23-disk-performance-on-aks-part-1/ubuntu-statefulset.yaml
</code></pre><p>Follow the progress of the Pod creation with <code>kubectl get pods -w</code> and wait until it is <code>Running</code>.</p><hr><p>When the Pod is <code>Running</code> we can start a shell on it with <code>kubectl exec -it disk-test-0 bash</code></p><p>Once inside <code>bash</code> on the Pod, we install <code>fio</code>:</p><pre><code>apt-get update &amp;&amp; apt-get install -y fio wget
</code></pre><p>And save the contents of in the Pod:</p><pre><code>wget https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2019-02-23-disk-performance-on-aks-part-1/jobs.fio
</code></pre><p>Now we can run the different test sections one by one. <strong>PS: If you don&rsquo;t specify a section <code>fio</code> will run all the tests <em>simultaneously</em>, which is not what we want.</strong></p><pre><code>fio --section=test1 jobs.fio
fio --section=test2 jobs.fio
fio --section=test3 jobs.fio
fio --section=test4 jobs.fio
fio --section=test5 jobs.fio
fio --section=test6 jobs.fio
fio --section=test7 jobs.fio
fio --section=test8 jobs.fio
fio --section=test9 jobs.fio
</code></pre><p><a id=Tests></a></p><h2 id=test-results>Test results</h2><p><a id=Test1></a></p><h3 id=test-1---learning-to-dislike-azure-cache>Test 1 - Learning to dislike Azure Cache</h3><p><em>Sequential write, 4K block size, Azure Cache enabled, OS cache disabled. See <a class=link href=/attachments/2019-02-23-disk-performance-on-aks-part-1/test1.md>full fio test results</a>.</em></p><p>I run the first tests on the OS disk of a Kubernetes node. The OS disks have Azure caching enabled.</p><p><img src=/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1.png width=1697 height=1232 srcset="/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1_hu1499039068193334488.png 480w, /p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1_hu13957841185659198382.png 1024w" loading=lazy alt=graph class=gallery-image data-flex-grow=137 data-flex-basis=330px></p><p>The first 1-2 minutes of the test I get very good performance of 45MB/s and ~11.500 IOPS but that drops to 0 very quickly as the cache is full and busy writing things to the underlying disk. When that happens everything freezes and I cannot even execute shell commands. After stopping the test the system still hangs for a bit while the cache empties.</p><p>The maximum latency measured by <code>fio</code> was 108751k usec. Or about 108 seconds!</p><blockquote><p>For the first try of these tests a 20-30 second period of very fast writes (250MB/s) caused a 7-8 minutes hang while the cache emptied. Trying again caused another pattern of lower peak performance with shorter hangs in between. Very unpredictable.
I&rsquo;m not sure what to make of this. It&rsquo;s not acceptable that a Kubernetes node becomes unresponsive for many minutes following a short burst of writing. There are scattered recommendations online of disabling caching for write-heavy applications. Since I have not found any way to measure the Azure cache itself, the results are unpredictable and potentially very impactful as well as making it very hard to use the metrics we do have to evaluate application and storage behaviour I&rsquo;ve concluded that it&rsquo;s best to use data disks with caching disabled for our workloads (you cannot disable caching on an AKS node OS disk).</p></blockquote><p><a id=Test2></a></p><h3 id=test-2---disable-azure-cache---enable-os-cache>Test 2 - Disable Azure Cache - enable OS cache</h3><p><em>Sequential write, 4K block size. <strong>Change: Azure cache disabled, OS caching enabled.</strong> See <a class=link href=/attachments/2019-02-23-disk-performance-on-aks-part-1/test2.md>full fio test results</a>.</em></p><p><img src=/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test2.png width=1703 height=1226 srcset="/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test2_hu9653487243883720288.png 480w, /p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test2_hu12369574169741432704.png 1024w" loading=lazy alt=graph class=gallery-image data-flex-grow=138 data-flex-basis=333px></p><p>If we swap the Azure cache for the Linux OS cache we see that <code>iowait</code> increases while the writing occurs. The application sees high write performance until the number of <code>Dirty bytes</code> reaches a threshold of about 3.7GB of memory. The performance of the underlying disk is 125MB/s and 250 IOPS. Here we are throttled by the 125MB/s limit of the Azure P15 Premium SSD.</p><p>Also notice that on sequential writes of 4K with OS caching the actual blocks written to disk is 512K which saves us a lot of IOPS. This will become important later.</p><p><a id=Test3></a></p><h3 id=test-3---disable-os-cache>Test 3 - Disable OS cache</h3><p><em>Sequential write, 4K block size. <strong>Change: OS caching disabled.</strong> See <a class=link href=/attachments/2019-02-23-disk-performance-on-aks-part-1/test3.md>full fio test results</a>.</em></p><p><img src=/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test3.png width=1698 height=1233 srcset="/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test3_hu9187350742741251895.png 480w, /p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test3_hu14645523390977373096.png 1024w" loading=lazy alt=graph class=gallery-image data-flex-grow=137 data-flex-basis=330px></p><blockquote><p>By disabling the OS cache (<code>direct=1</code>) the results are consistent and predictable. There is no <code>iowait</code> since the application does not have multiple writes pending at the same time. Because of the 2-3ms latency of the disks we are not able to get more than about 400 IOPS. This gives us a meager 1.5MB/s even though the disk is limited to 1100 IOPS and 125MB/s. To reach that we need multiple simultaneous writes or a bigger IO depth (queue). <code>Disk active time</code> is also 0% which indicates that the disk is not saturated.</p></blockquote><p><a id=Test4></a></p><h3 id=test-4---increase-io-depth>Test 4 - Increase IO depth</h3><p><em>Sequential write, 4K block size, OS caching disabled. <strong>Change: IO depth 16.</strong> See <a class=link href=/attachments/2019-02-23-disk-performance-on-aks-part-1/test4.md>full fio test results</a>.</em></p><p><img src=/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test4.png width=1698 height=1237 srcset="/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test4_hu5722226652359263237.png 480w, /p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test4_hu2996385930643672719.png 1024w" loading=lazy alt=graph class=gallery-image data-flex-grow=137 data-flex-basis=329px></p><blockquote><p>For this test we only increase the IO depth from 1 to 16. IO depth is the number of write operations <code>fio</code> will execute simultaneously. Since we are using <code>direct</code> these will be queued by the OS for writing. We are now able to hit the performance limit of 1100 IOPS. <code>Disk active time</code> is now steady at 100% indicating that we have saturated the disk.</p></blockquote><p><a id=Test5></a></p><h3 id=test-5---larger-block-size-smaller-io-depth>Test 5 - Larger block size, smaller IO depth</h3><p><em>Sequential write, OS caching disabled. <strong>Change: 128K block size, IO depth 1.</strong> See <a class=link href=/attachments/2019-02-23-disk-performance-on-aks-part-1/test5.md>full fio test results</a>.</em></p><p><img src=/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test5.png width=1699 height=1229 srcset="/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test5_hu14954921568768718351.png 480w, /p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test5_hu3268361891850381379.png 1024w" loading=lazy alt=graph class=gallery-image data-flex-grow=138 data-flex-basis=331px></p><blockquote><p>We increase the block size to 128KB and reduce the IO depth to 1 again. The write latency for larger blocks increase to ~5ms which gives us 200 IOPS and 28MB/s. The disk is not saturated.</p></blockquote><p><a id=Test6></a></p><h3 id=test-6---enable-os-cache>Test 6 - Enable OS cache</h3><p><em>Sequential write, 256K block size, IO depth 1. <strong>Change: OS caching enabled.</strong> See <a class=link href=/attachments/2019-02-23-disk-performance-on-aks-part-1/test6.md>full fio test results</a>.</em></p><p><img src=/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test6.png width=1697 height=1230 srcset="/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test6_hu5250198874418650337.png 480w, /p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test6_hu6421477374997710977.png 1024w" loading=lazy alt=graph class=gallery-image data-flex-grow=137 data-flex-basis=331px></p><blockquote><p>We have now enabled the OS cache/buffer (<code>direct=0</code>). We can see that the writes hitting the disk are now merged to 512KB blocks. We are hitting the 125MB/s limit with about 250 IOPS. Enabling the cache also has other effects: CPU suddenly shows significant IO wait. The write latency shoots through the roof. Also note that the writing continued for 30-40 seconds after the test was done. <strong>This also means that the bandwidth and IOPS that <code>fio</code> sees and reports is higher than what is actually hitting the disk.</strong></p></blockquote><p><a id=Test7></a></p><h3 id=test-7---random-writes-small-block-size>Test 7 - Random writes, small block size</h3><p><em>IO depth 1, OS caching enabled. <strong>Change: Random write, 4K block size.</strong> See <a class=link href=/attachments/2019-02-23-disk-performance-on-aks-part-1/test7.md>full fio test results</a>.</em></p><p><img src=/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test7.png width=1702 height=1239 srcset="/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test7_hu9566364161109018395.png 480w, /p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test7_hu821024261791687868.png 1024w" loading=lazy alt=graph class=gallery-image data-flex-grow=137 data-flex-basis=329px></p><blockquote><p>Here we go from sequential writes to random writes. We are limited by IOPS. The average size of the blocks actually written to disks, and the IOPS required to hit the bandwidth limit is actually varying a bit throughout the test. The time taken to empty the cache is about as long as I ran the test (4-5 minutes).</p></blockquote><p><a id=Test8></a></p><h3 id=test-8---large-block-size>Test 8 - Large block size</h3><p><em>Random write, OS caching enabled. <strong>Change: 256K block size, IO depth 16.</strong> See <a class=link href=/attachments/2019-02-23-disk-performance-on-aks-part-1/test8.md>full fio test results</a>.</em></p><p><img src=/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test8.png width=1699 height=1235 srcset="/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test8_hu1300341601469829731.png 480w, /p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test8_hu923727430283491984.png 1024w" loading=lazy alt=graph class=gallery-image data-flex-grow=137 data-flex-basis=330px></p><blockquote><p>Increasing the block size to 256K makes us bandwidth limited to 125MB/s.</p></blockquote><p><a id=Conclusion></a></p><h2 id=conclusion>Conclusion</h2><p>Access patterns and block sizes have a tremendous impact on the amount of data we are able to write to disk.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/kubernetes/>Kubernetes</a>
<a href=/tags/azure/>Azure</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Feb 23, 2019 00:00 UTC</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/mini-post-down-scaling-azure-kubernetes-service-aks/><div class=article-image><img src=/p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu.ada85c716b0eb235da02aa0a65aa3303_hu16938543950284632476.png width=250 height=150 loading=lazy alt="Featured image of post Mini-post: Down-scaling Azure Kubernetes Service (AKS)" data-hash="md5-rahccWsOsjXaAqoKZaozAw=="></div><div class=article-details><h2 class=article-title>Mini-post: Down-scaling Azure Kubernetes Service (AKS)</h2></div></a></article><article class=has-image><a href=/p/managed-kubernetes-on-microsoft-azure-english/><div class=article-image><img src=/p/managed-kubernetes-on-microsoft-azure-english/minecraft-k8s.a12cfbcd200722fae02c38e833d8ab36_hu5426546647404073287.png width=250 height=150 loading=lazy alt="Featured image of post Managed Kubernetes on Microsoft Azure (English)" data-hash="md5-oSz7zSAHIvrgLDjoM9irNg=="></div><div class=article-details><h2 class=article-title>Managed Kubernetes on Microsoft Azure (English)</h2></div></a></article><article class=has-image><a href=/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/><div class=article-image><img src=/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/minecraft-k8s.a12cfbcd200722fae02c38e833d8ab36_hu5426546647404073287.png width=250 height=150 loading=lazy alt="Featured image of post Managed Kubernetes p√• Microsoft Azure (Norwegian)" data-hash="md5-oSz7zSAHIvrgLDjoM9irNg=="></div><div class=article-details><h2 class=article-title>Managed Kubernetes p√• Microsoft Azure (Norwegian)</h2></div></a></article><article><a href=/p/kubernetes-sidecar-config-drift/><div class=article-details><h2 class=article-title>Kubernetes Sidecar Config Drift</h2></div></a></article><article><a href=/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/><div class=article-details><h2 class=article-title>A side quest in API development, observability, Kubernetes and cloud with a hint of database</h2></div></a></article></div></div></aside><div class=disqus-container></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2024 blog.stian.omg.lol</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>