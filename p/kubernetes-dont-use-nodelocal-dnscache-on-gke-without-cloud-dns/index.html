<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="A travel diary of how we diagnosed a latency spike problem on GKE and the surprising things we learned along the way."><title>Kubernetes: Don't use NodeLocal DNSCache on GKE (without Cloud DNS)</title>
<link rel=canonical href=https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Kubernetes: Don't use NodeLocal DNSCache on GKE (without Cloud DNS)"><meta property='og:description' content="A travel diary of how we diagnosed a latency spike problem on GKE and the surprising things we learned along the way."><meta property='og:url' content='https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/'><meta property='og:site_name' content='blog.stian.omg.lol'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='kubernetes'><meta property='article:tag' content='gke'><meta property='article:tag' content='dns'><meta property='article:published_time' content='2023-12-13T00:00:00+00:00'><meta property='article:modified_time' content='2023-12-13T00:00:00+00:00'><meta property='og:image' content='https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/cover.webp'><meta name=twitter:title content="Kubernetes: Don't use NodeLocal DNSCache on GKE (without Cloud DNS)"><meta name=twitter:description content="A travel diary of how we diagnosed a latency spike problem on GKE and the surprising things we learned along the way."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/cover.webp'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/profile-picture_hu6892409687065925510.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ü§∑‚Äç‚ôÇÔ∏è</span></figure><div class=site-meta><h1 class=site-name><a href=/>blog.stian.omg.lol</a></h1><h2 class=site-description>Technology. Aviation. Philosophy.</h2></div></header><ol class=menu-social><li><a href=https://github.com/StianOvrevage target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#nodelocal-dnscache--coredns>NodeLocal DNSCache / CoreDNS</a></li><li><a href=#kube-dns--dnsmasq>kube-dns / dnsmasq</a></li><li><a href=#am-i-affected>Am I affected?</a></li></ol><ol><li><a href=#the-initial-problem>The initial Problem</a></li><li><a href=#istio--envoy-metrics>Istio / Envoy metrics</a></li><li><a href=#reproducing-and-troubleshooting-http-connection-problems>Reproducing and troubleshooting HTTP connection problems</a></li><li><a href=#troubleshooting-potentially-slow-or-broken-dns-lookups>Troubleshooting potentially slow or broken DNS lookups</a></li></ol><ol><li><a href=#appendix---enabling-additional-envoy-metrics>Appendix - Enabling additional envoy metrics</a><ol><li><a href=#enabling-additional-envoy-metrics>Enabling additional envoy metrics</a></li><li><a href=#enable-additional-metrics-on-a-deployment-or-pod>Enable additional metrics on a Deployment or Pod</a></li><li><a href=#enable-additional-metrics-globally-please-dont>Enable additional metrics globally (please don‚Äôt!)</a></li><li><a href=#viewing-stats-and-metrics>Viewing stats and metrics</a></li><li><a href=#notes-on-timeout-and-retry-metrics>Notes on timeout and retry metrics</a></li><li><a href=#further-work>Further work</a></li></ol></li><li><a href=#appendix---overview-of-dns-on-gke>Appendix - Overview of DNS on GKE</a></li><li><a href=#appendix---reducing-dns-lookups-in-kubernetes-and-gke>Appendix - Reducing DNS lookups in Kubernetes and GKE</a></li><li><a href=#appendix---overview-of-gke-dns-with-nodelocal-dnscache>Appendix - Overview of GKE DNS with NodeLocal DNSCache</a></li><li><a href=#appendix---enabling-node-local-dns-metrics>Appendix - Enabling node-local-dns metrics</a></li><li><a href=#appendix---using-dnsperf-to-test-dns-performance>Appendix - Using dnsperf to test DNS performance</a></li><li><a href=#appendix---network-packet-capture-without-dependencies>Appendix - Network packet capture without dependencies</a></li><li><a href=#appendix---verbose-logging-on-kube-dns>Appendix - Verbose logging on <code>kube-dns</code></a></li><li><a href=#appendix---analyzing-dnsmasq-logs>Appendix - Analyzing <code>dnsmasq</code> logs</a></li><li><a href=#appendix---analyzing-concurrent-tcp-connections>Appendix - Analyzing concurrent TCP connections</a></li><li><a href=#appendix---analyzing-dns-problems-based-on-packet-captures>Appendix - Analyzing DNS problems based on packet captures</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/><img src=/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/cover_hu7659877757681722832.webp srcset="/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/cover_hu7659877757681722832.webp 800w, /p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/cover_hu18049225249822250835.webp 1600w" width=800 height=457 loading=lazy alt="Featured image of post Kubernetes: Don't use NodeLocal DNSCache on GKE (without Cloud DNS)"></a></div><div class=article-details><header class=article-category><a href=/categories/technology/ style=background-color:#2a9d8f;color:#fff>Technology</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/>Kubernetes: Don't use NodeLocal DNSCache on GKE (without Cloud DNS)</a></h2><h3 class=article-subtitle>A travel diary of how we diagnosed a latency spike problem on GKE and the surprising things we learned along the way.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Dec 13, 2023</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>40 minute read</time></div></footer></div></header><section class=article-content><ul><li><a class=link href=#introduction>Introduction</a></li><li><a class=link href=#tl-dr>TL;DR:</a><ul><li><a class=link href=#nodelocal-dnscache-coredns>NodeLocal DNSCache / CoreDNS</a></li><li><a class=link href=#kubedns-dnsmasq>kube-dns / dnsmasq</a></li><li><a class=link href=#am-i-affected>Am I affected?</a></li></ul></li><li><a class=link href=#investigation>Investigation</a><ul><li><a class=link href=#the-initial-problem>The initial Problem</a></li><li><a class=link href=#istio-envoy-metrics>Istio / Envoy metrics</a></li><li><a class=link href=#reproducing>Reproducing and troubleshooting HTTP connection problems</a></li><li><a class=link href=#troubleshooting-dns>Troubleshooting potentially slow or broken DNS lookups</a></li></ul></li><li><a class=link href=#appendices>Appendices</a><ul><li><a class=link href=#appendix-enabling-additional-envoy-metrics>Appendix - Enabling additional envoy metrics</a></li><li><a class=link href=#appendix-overview-of-dns-on-gke>Appendix - Overview of DNS on GKE</a></li><li><a class=link href=#appendix-reducing-dns-lookups>Appendix - Reducing DNS lookups in Kubernetes and GKE</a></li><li><a class=link href=#appendix-overview-of-gke-dns-with-nodelocal-dnscache>Appendix - Overview of GKE DNS with NodeLocal DNSCache</a></li><li><a class=link href=#appendix-enabling-node-local-dns-metrics>Appendix - Enabling node-local-dns metrics</a></li><li><a class=link href=#appendix-using-dnsperf>Appendix - Using dnsperf to test DNS performance</a></li><li><a class=link href=#appendix-network-packet-capture>Appendix - Network packet capture without dependencies</a></li><li><a class=link href=#appendix-verbose-logging-on-kube-dns>Appendix - Verbose logging on kube-dns</a></li><li><a class=link href=#appendix-analyzing-dnsmasq-logs>Appendix - Analyzing dnsmasq logs</a></li><li><a class=link href=#appendix-analyzing-concurrent-tcp-connections>Appendix - Analyzing concurrent TCP connections</a></li><li><a class=link href=#analyzing-dns-problems-based-on-packet-capture>Appendix - Analyzing DNS problems based on packet captures</a></li></ul></li><li><a class=link href=#footnotes>Footnotes</a></li></ul><p><em>This is a cross-post of a blog post also published on the <a class=link href=https://www.signicat.com/blog/dont-use-nodelocal-dnscache-on-gke target=_blank rel=noopener>Signicat Blog</a></em></p><p><a id=introduction></a></p><h1 id=introduction>Introduction</h1><p>Last year I was freelancing as a consultant in Signicat and recently I returned, now as an actual employee!</p><p>The first week after returning, a tech lead for another team reaches out to me about a technical issue he&rsquo;s been struggling with.</p><p>In this blog post I&rsquo;ll try to guide you through the troubleshooting with actionable take-aways. It appears it&rsquo;s going to be a long one with a lot of detours, so I&rsquo;ve summarized our findings and recommendations here on the top starting right below this introduction. There are a few appendixes at the end that I&rsquo;ve tried to make self-contained as to make them useful in other contexts and without necessarily having to follow the main story.</p><p>If you don&rsquo;t want any spoilers but follow the bumpy journey from start to end, fill your coffee mug and skip ahead to <a class=link href=#investigation>Investigation</a>.</p><p><a id=tl-dr></a></p><h1 id=tldr>TL;DR:</h1><p>Due to an unfortunate combination of behaviour of <a class=link href=https://coredns.io/manual/toc/ target=_blank rel=noopener>CoreDNS</a> (which NodeLocal DNSCache uses) and <a class=link href=https://github.com/kubernetes/dns target=_blank rel=noopener>kube-dns</a> (which is the default on GKE) <strong>I recommend NOT using them in combination.</strong></p><p>Since <a class=link href=https://cloud.google.com/knowledge/kb/how-to-run-coredns-on-kubernetes-engine-000004698 target=_blank rel=noopener>GKE does not offer CoreDNS as a managed option</a> for <code>kube-dns</code> (even though <a class=link href=https://kubernetes.io/blog/2018/12/03/kubernetes-1-13-release-announcement/ target=_blank rel=noopener>Kubernetes made CoreDNS the default in 1.13 in 2018</a>) you are left with two options:</p><ul><li>Not enabling <a class=link href=https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache target=_blank rel=noopener>NodeLocal DNSCache on GKE</a></li><li>Switching from <code>kube-dns</code> <a class=link href=https://cloud.google.com/kubernetes-engine/docs/how-to/cloud-dns target=_blank rel=noopener>to Google Cloud DNS</a></li></ul><p><em>What is this unfortunate combination you ask?</em></p><p><a id=nodelocal-dnscache-coredns></a></p><h2 id=nodelocal-dnscache--coredns>NodeLocal DNSCache / CoreDNS</h2><p>NodeLocal DNSCache (in GKE at least) is configured with:</p><pre><code># kubectl get configmap --namespace kube-system node-local-dns -o yaml | yq .data.Corefile
cluster.local:53 {
  forward . __PILLAR__CLUSTER__DNS__ {
      force_tcp
      expire 1s
</code></pre><p>This means CoreDNS will upgrade any and all incoming DNS requests to TCP connections before connecting to <code>dnsmasq</code> (the first of two containers in a <code>kube-dns</code> Pod). CoreDNS reuses TCP connections if available. Unused connections in the connection pool should be cleaned up every 1 second (in theory). If no connections are available a new will be created, apparently with no upper bound.</p><p>This means new connections may be created en-masse when needed, but old connections can take a while (I&rsquo;ve observed 5-10 seconds) before being cleaned up, and most importantly, closed.</p><p><a id=kubedns-dnsmasq></a></p><h2 id=kube-dns--dnsmasq>kube-dns / dnsmasq</h2><p><code>dnsmasq</code> has a <a class=link href=https://github.com/imp/dnsmasq/blob/master/src/config.h#L18 target=_blank rel=noopener>hardcoded maximum number of 20 workers</a>. For us that means each <code>kube-dns</code> Pod is limited to 20 open connections.</p><p><code>kube-dns</code> scaling is managed by a bespoke kube-dns autoscaler that by default in GKE is <a class=link href=https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache#scaling_up_kube-dns target=_blank rel=noopener>configured</a> like:</p><pre><code># kubectl get configmap --namespace kube-system kube-dns-autoscaler -o yaml | yq .data.linear
{&quot;coresPerReplica&quot;:256, &quot;nodesPerReplica&quot;:16,&quot;preventSinglePointFailure&quot;:true}
</code></pre><ul><li><code>preventSinglePointFailure</code> - Run at least two Pods.</li><li><code>nodesPerReplica</code> - Run one <code>kube-dns</code> Pod for each 16 nodes.</li></ul><p>This default setup will have two <code>kube-dns</code> Pods until your cluster grows beyond 32 nodes.</p><p>Two <code>kube-dns</code> Pods have a limit of 40 open TCP connections in total from the <code>node-local-dns</code> Pods running on each node. The <code>node-local-dns</code> Pods though are happy to try to open many more TCP connections.</p><p>GKE kube-dns docs mention &ldquo;<a class=link href=https://cloud.google.com/kubernetes-engine/docs/how-to/kube-dns#performance_limitations_with_kube-dns target=_blank rel=noopener>Performance limitations with kube-dns</a>&rdquo; as a known issue suggesting enabling NodeLocal DNSCache as a potential fix.</p><p>And GKE NodeLocal DNSCache docs mention &ldquo;<a class=link href=https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache#timeout_issues target=_blank rel=noopener>NodeLocal DNSCache timeout errors</a>&rdquo; as a known issue with the possible reasons being</p><ul><li>An underlying network connectivity problem. (<em>Spoiler: it probably isn&rsquo;t</em>)</li><li>Significantly increased DNS queries from the workload or due to node pool upscaling. (<em>Spoiler: it probably isn&rsquo;t</em>)</li></ul><p>With the fix being</p><blockquote><p><strong>&ldquo;The workaround is to increase the number of kube-dns replicas by tuning the autoscaling parameters.&rdquo;</strong></p></blockquote><p>Increasing the number of <code>kube-dns</code> Pods will reduce the frequency and impact but not eliminate it. Until we migrate to Cloud DNS we run <code>kube-dns</code> at a ratio of 1:1.5 of nodes by configuring autoscaling with <code>"coresPerReplica":24</code> and using 16 core nodes. Resulting in 12 <code>kube-dns</code> Pods in a 17 node cluster.</p><blockquote><p>By looking at the <code>coredns_forward_conn_cache_misses_total</code> metric I observe it at least increasing by more than 300 in a 15 second metric sampling window for <em>one</em> <code>node-local-dns</code> Pod. This means <em>on average</em> during those 15 seconds 20 new TCP connections were attempted since no existing connection could be re-used. (<a class=link href=#footnote-a>Footnote A</a>)</p><p>This means even setting <code>"nodesPerReplica":1</code> thus running one <code>kube-dns</code> Pod for each node may not be enough to guarantee not hitting the 20 process limit in <code>dnsmasq</code> occasionally.</p><p>I guess you could start lowering <code>coresPerReplica</code> to have more than one <code>kube-dns</code> Pod for each node, but now it&rsquo;s getting silly.</p></blockquote><p>To re-iterate; If you&rsquo;re using NodeLocal DNSCache and <code>kube-dns</code> you should plan to migrate to Cloud DNS. You can alleviate the problem in the short term by scaling up <code>kube-dns</code> aggressively but it will not eliminate occasional latency spikes.</p><p><a id=am-i-affected></a></p><h2 id=am-i-affected>Am I affected?</h2><p>How do I know if I&rsquo;m affected by this?</p><ul><li>You are using NodeLocal DNSCache (CoreDNS) and <code>kube-dns</code> (default on GKE)</li><li>You see log lines with <code>i/o timeout</code> in node-local-dns pods (<code>kubectl logs -n kube-system node-local-dns-xxxxx</code>)</li><li>You are hitting dnsmasq max procs. Create a graph <code>container_processes{namespace="kube-system", container="dnsmasq"}</code> or use my <a class=link href=https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/grafana-dashboard-kube-dns.json target=_blank rel=noopener>kube-dns Grafana dashboard</a>. (<a class=link href=#footnote-b>Footnote B</a>).</li></ul><p><a id=investigation></a></p><h1 id=investigation>Investigation</h1><p><a id=the-initial-problem></a></p><h2 id=the-initial-problem>The initial Problem</h2><p>We have a microservice architecture and use <a class=link href=https://www.openpolicyagent.org/ target=_blank rel=noopener>Open Policy Agent (OPA)</a> deployed as <a class=link href=https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns/#example-1-sidecar-containers target=_blank rel=noopener>sidecars</a> to process and validate request tokens.</p><p>However, some services were getting timeouts and he suspected it was an issue with <a class=link href=https://istio.io/ target=_blank rel=noopener>istio</a>, a service mesh that provides security and observability for traffic inside Kubernetes clusters.</p><p>The actual error message showing up in the logs was a generic <code>context deadline exceeded (Client.Timeout exceeded while awaiting headers)</code> which I recognize as a Golang error probably stemming from a <a class=link href=https://pkg.go.dev/net/http target=_blank rel=noopener>http.Get()</a> call or similar.</p><p>The first thing that comes to mind is that last year we made a change in our istio practices. Inside our clusters we call services in other namespaces (crossing team boundaries) using the same FQDN domain and path that our customers would use, not internal Kubernetes names. So internally we&rsquo;re calling <code>api.signicat.com/service</code> and not <code>service.some-team-namespace</code>.</p><p>He had noticed that the problem seemed to have increased when doing the switch from internal DNS names to FQDN.</p><p><a id=istio-envoy-metrics></a></p><h2 id=istio--envoy-metrics>Istio / Envoy metrics</h2><p>We started troubleshooting by enabling additional metrics in envoy to hopefully be able to confirm that the problem was visible as envoy timeouts or some other kind of error.</p><blockquote><p><a class=link href=https://www.envoyproxy.io/ target=_blank rel=noopener>Envoy</a> is the actual software that traffic goes through in an istio service mesh.</p></blockquote><p>This turned out to be a dead end and we couldn‚Äôt find any signs of timeouts or errors in the envoy metrics.</p><blockquote><p><em>Enabling these additional metrics was a bit of a chore and there were a few surprises. Have a look at the <a class=link href=#appendix-enabling-additional-envoy-metrics>Enabling additional envoy metrics</a> appendix for steps and caveats.</em></p></blockquote><p><a id=reproducing></a></p><h2 id=reproducing-and-troubleshooting-http-connection-problems>Reproducing and troubleshooting HTTP connection problems</h2><p>At this point I&rsquo;m thinking the problem may be something else. Failure to acquire a HTTP connection from the pool or a TCP socket maybe?</p><p>Using the <a class=link href=https://pkg.go.dev/net/http/httptrace target=_blank rel=noopener>httptrace</a> golang library I created a small program that would continuously query both the internal <code>some-service.some-namespace</code> hostname and FQDN <code>api.signicat.com/some-service</code>, while logging and counting each phase of the HTTP request as well as it&rsquo;s timings.</p><p>Here is a quick and dirty way of starting the program in an ephemeral Pod:</p><pre><code># Start a Pod named `conn-test` using the `golang:latest` image. Attach to it (`-i --tty`) and start `bash`. Delete it after disconnecting (`--rm`):
kubectl run conn-test --rm -i --tty --image=golang:latest -- bash
# Create and enter a directory for the program:
mkdir conn-test &amp;&amp; cd conn-test
# Initialize a go environment for it:
go mod init conn-test
# Download the source:
wget https://raw.githubusercontent.com/signicat/blog-attachements/main/2023-gke-node-local-dns-cache/files/go-http-connection-test/main.go
# Download dependencies:
go mod tidy
# Configure it (refer to the source for more configuration options):
export URL_1=http://some-service.some-namespace.svc.cluster.local
# Start it:
go run main.go
</code></pre><blockquote><p>If you need to run it for longer it probably makes sense to build it as a Docker image, upload it to an image registry and create a Deployment for it. It also exposes metrics in Prometheus format so you can also add scraping of the metrics either through annotations or a ServiceMonitor.</p></blockquote><p>After a few hours the program crashed, and the last log message indicated that <code>DNSStart</code> was the last thing to happened. (The program crashed since I just <code>log.Fatal</code>ed if the request failed, which it did timing out. I since improved that, even though we already learned what we needed).</p><p>We did some manual DNS lookups using <code>nslookup -debug api.signicat.com</code> which (obviously, in retrospect) shows 6 failing DNS requests before finally getting it right:</p><ul><li><code>api.signicat.com.$namespace.svc.cluster.local - NXDOMAIN</code></li><li><code>api.signicat.com.svc.cluster.local - NXDOMAIN</code></li><li><code>api.signicat.com.cluster.local - NXDOMAIN</code></li><li><code>api.signicat.com.$gcp-region.c.$gcp-project.internal - NXDOMAIN</code></li><li><code>api.signicat.com.c.$gcp-project.internal - NXDOMAIN</code></li><li><code>api.signicat.com.google.internal - NXDOMAIN</code></li><li><code>api.signicat.com - OK</code></li></ul><p><em>See <a class=link href=#appendix-overview-of-dns-on-gke>Overview of DNS on GKE</a> for a lengthier explanation of why this happens.</em></p><p>The default timeout for HTTP requests in Golang is 5 seconds. DNS resolution <em>should</em> be fast enough and it shouldn&rsquo;t be a problem to do 7 DNS lookups in that time. But if there is some sluggishness and variability in DNS resolution times the probability of having one slow lookup out of the 7 being slow increases the risk of causing a timeout. In addition, always doing 7 lookups puts additional strain on the DNS infrastructure potentially further exacerbating the probability of slow lookups.</p><p>From here on we have two courses of action. Figure out if we can reduce the number of DNS lookups as well as figure out if, and why, DNS resolution isn&rsquo;t consistently fast.</p><p><em>See <a class=link href=#appendix-reducing-dns-lookups>Reducing DNS lookups in Kubernetes and GKE</a> for some thoughts on reducing these extraneous DNS lookups.</em></p><p><a id=troubleshooting-dns></a></p><h2 id=troubleshooting-potentially-slow-or-broken-dns-lookups>Troubleshooting potentially slow or broken DNS lookups</h2><p>We use <a class=link href=https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/ target=_blank rel=noopener>NodeLocal DNSCache</a> which on GKE is <a class=link href=https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache target=_blank rel=noopener>enabled by clicking the right button</a>.</p><blockquote><p><strong>NodeLocal DNSCache summary</strong></p><p>It is deployed by a ReplicaSet causing one CoreDNS Pod (named <code>node-local-dns-x</code> in <code>kube-system</code> namespace) to be running on each node.</p><p>It adds a listener on the same IP as the <code>kube-dns</code> Service. Thereby intercepting traffic that would otherwise go to that IP.</p><p>The <code>node-local-dns</code> Pod caches both positive and negative results. Domains ending in <code>.cluster.local</code> are forwarded to <code>kube-dns</code> in the cluster (but through a new Service called <code>kube-dns-upstream</code> with a different IP). Requests for other domains are forwarded to 8.8.8.8 and 8.8.4.4.</p><p>NodeLocal DNSCache instances use a 2 second timeout when querying upstream DNS servers.</p><p><em>See <a class=link href=#appendix-overview-of-gke-dns-with-nodelocal-dnscache>Overview of GKE DNS with NodeLocal DNSCache</a> for additional details.</em></p></blockquote><p>We wanted to look at some metrics from the <code>node-local-dns</code> Pods but found that we didn&rsquo;t have any! We fixed that but didn&rsquo;t at the time learn anything new. <em>See <a class=link href=#appendix-enabling-node-local-dns-metrics>Enabling node-local-dns metrics</a> for how to fix/enable node-local-dns metrics.</em></p><p>Stumbling over the logs of one of the <code>node-local-dns-x</code> Pods I notice:</p><pre><code>[ERROR] plugin/errors: 2 archive.ubuntu.com.some-namespace.svc.cluster.local. A: dial tcp 172.18.165.130:53: i/o timeout
</code></pre><p>This tells us that at least lookups going to <code>kube-dns</code> in the cluster (<code>172.18.165.130:53</code>) are having problems.</p><p>So timeouts are happening on individual lookups, it&rsquo;s not just the total duration of 7 lookups timing out. And lookups are not only happening inside the client application but in <code>node-local-dns</code> as well. We still don&rsquo;t know if these lookups are lost or merely slow. But seen from the application anything longer than 2 seconds, that is timing out on <code>node-local-dns</code>, might as well be lost.</p><p>Just to be sure I checked that packets were not being dropped or lost on the nodes on both sides. And indeed the network seems fine. It&rsquo;s been a long time since I&rsquo;ve actually experienced problems due to packets being lost, but it&rsquo;s always good to rule it out.</p><p>Next step is to capture the packets as they (hopefully) leave the <code>node-local-dns</code> Pod and (maybe) arriving at one of the <code>kube-dns</code> Pods.</p><p><em>See <a class=link href=#appendix-network-packet-capture>Network packet capture without dependencies</a> for how to capture packets in Kubernetes.</em></p><p>The packets are indeed arriving at the <code>dnsmasq</code> container in the <code>kube-dns</code> Pods:</p><pre><code>Query 0x31e8:

Leaves node-local-dns eth0:
10:53:11.134	10.0.0.107	172.20.13.4	DNS	0x31e8	Standard query 0x31e8 A archive.ubuntu.com.some-ns.svc.cluster.local

Arrives at dnsmasq eth0:
10:53:11.134	10.0.0.107	172.20.13.4	DNS	0x31e8	Standard query 0x31e8 A archive.ubuntu.com.some-ns.svc.cluster.local
</code></pre><p>Looking at DNS resolution time from the <code>kube-dns</code> container which <code>dnsmasq</code> forwards to shows that <code>kube-dns</code> is consistently answering queries extremely fast (not shown here).</p><p>But after some head scratching I look at the time between queries arriving at <code>dnsmasq</code> on <code>eth0</code> before leaving again on <code>lo</code> for <code>kube-dns</code> and indeed there is a (relatively) long delay of 952ms between 10:53:11.134 and 10:53:12.086:</p><pre><code>Leaves dnsmasq lo:
10:53:12.086	127.0.0.1	127.0.0.1	DNS	0x31e8	Standard query 0x31e8 A archive.ubuntu.com.some-ns.svc.cluster.local
</code></pre><p>In this case the query just sits around in <code>dnsmasq</code> for almost one second before being forwarded to <code>kube-dns</code>!</p><p>Why? As with most issues we start by checking if there are any issues with memory or CPU usage or CPU throttling (<a class=link href=#footnote-c>Footnote C</a>).</p><p><img src=/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/kube-dns-resource-usage.png width=1377 height=693 srcset="/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/kube-dns-resource-usage_hu15148347036814534097.png 480w, /p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/kube-dns-resource-usage_hu11608676099510034976.png 1024w" loading=lazy alt="Graphs showing DNS response time on node-local-dns Pods and resource usage on kube-dns Pods." class=gallery-image data-flex-grow=198 data-flex-basis=476px></p><p>Nope. CPU and memory usage is both low and stable but the 99 percentile DNS request duration is all over the place.</p><p>We also check and see that there is plenty of unused CPU available on the underlying nodes these pods are running on.</p><blockquote><p>We also used <code>dnsperf</code> to benchmark and stress-test the various components and while fun, didn&rsquo;t teach us anything new. <em>See <a class=link href=#appendix-using-dnsperf>Using dnsperf to test DNS performance</a> for more information on that particular side-quest.</em></p></blockquote><p>Next up I want to increase the logging verbosity of <code>dnsmasq</code> to see if there are any clues to why it was (apparently) delaying processing DNS requests.</p><p><em>Have a look at <a class=link href=#appendix-verbose-logging-on-kube-dns>Verbose logging on kube-dns</a> for how to increase logging and <a class=link href=#appendix-analyzing-dnsmasq-logs>Analyzing dnsmasq logs</a> for how I analyzed the logs.</em></p><p>Analyzing the logs we learn that at least it appears that individual requests are fast and not clogging up the machinery.</p><p>In the meantime Google Cloud Support came back to us asking if we could run a command <code>for i in $(seq 1 1800) ; do echo "$(date) Try: ${i} DnsmasqProcess: $(pidof dnsmasq | wc -w)"; sleep 1; done</code> on the VM of the Pod. It also works to run this in a debug container attached to the <code>dnsmasq</code> container. The output looks like:</p><pre><code>Wed Oct 11 14:50:57 UTC 2023 Try: 1 DnsmasqProcess: 9
Wed Oct 11 14:50:58 UTC 2023 Try: 2 DnsmasqProcess: 14
Wed Oct 11 14:50:59 UTC 2023 Try: 3 DnsmasqProcess: 15
Wed Oct 11 14:51:00 UTC 2023 Try: 4 DnsmasqProcess: 21
Wed Oct 11 14:51:01 UTC 2023 Try: 5 DnsmasqProcess: 21
</code></pre><p>It turns out that <code>dnsmasq</code> has a <a class=link href=https://github.com/imp/dnsmasq/blob/master/src/config.h#L18 target=_blank rel=noopener>hard coded limit of 20 child processes</a>. So every time we see 21 it means it will not spawn a new child to process incoming connections.</p><p>I plotted this on the same graph as the DNS request times observed from raw packet captures (See <a class=link href=#analyzing-dns-problems-based-on-packet-capture>Analyzing DNS problems based on packet captures</a>):</p><p><img src=/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-and-dns-packet-latency.png width=1428 height=489 srcset="/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-and-dns-packet-latency_hu3974894387970546800.png 480w, /p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-and-dns-packet-latency_hu12671590469830325247.png 1024w" loading=lazy alt="Graph showing DNS resolution time from packet captures in blue. Number of dnsmasq processes in orange." class=gallery-image data-flex-grow=292 data-flex-basis=700px></p><p>Graphing the number of <code>dnsmasq</code> processes and observed DNS request latency finally shows a stable correlation between our symptoms and a potential cause.</p><p>You can also get a crude estimation by graphing the <code>container_processes{namespace="kube-system", container="dnsmasq"}</code> metric, or use my <a class=link href=https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/grafana-dashboard-kube-dns.json target=_blank rel=noopener>kube-dns Grafana dashboard</a> if you have cAdvisor/container metrics enabled:</p><p><img src=/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-graph-and-node-local-dns-duration.png width=1631 height=646 srcset="/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-graph-and-node-local-dns-duration_hu5189994088160500401.png 480w, /p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-graph-and-node-local-dns-duration_hu2637781448338722321.png 1024w" loading=lazy alt="Graph showing DNS resolution time from node-local-dns and dnsmasq processes." class=gallery-image data-flex-grow=252 data-flex-basis=605px></p><p>Going back to our packet captures I see connections that are unused for many seconds before finally being closed by <code>node-local-dns</code>.</p><pre><code>08:06:07.849	172.20.9.11	10.0.0.29	53 ‚Üí 46123 [ACK] Seq=1 Ack=80 Win=43648 Len=0
08:06:09.849	10.0.0.29	172.20.9.11	46123 ‚Üí 53 [FIN, ACK] Seq=80 Ack=1 Win=42624 Len=0
08:06:09.890	172.20.9.11	10.0.0.29	53 ‚Üí 46123 [ACK] Seq=1 Ack=81 Win=43648 Len=0
</code></pre><p>However <code>dnsmasq</code> only acknowledges the request to close the connection, it does not actually close it yet. In order for the connection to be properly closed both sides have to <code>FIN, ACK</code>.</p><p>Many seconds later <code>dnsmasq</code> faithfully tries to return a response and finish the closing of the connection, but <code>node-local-dns</code> (CoreDNS) has promptly forgotten about the whole thing and replies with the TCP equivalent of a shrug (<code>RST</code>):</p><pre><code>08:06:15.836	172.20.9.11	10.0.0.29	53	46123	Standard query response 0x7f12 No such name A storage.googleapis.com.some-ns.svc.cluster.local SOA ns.dns.cluster.local
08:06:15.836	172.20.9.11	10.0.0.29	53	46123	53 ‚Üí 46123 [FIN, ACK] Seq=162 Ack=81 Win=43648 Len=0
08:06:15.836	10.0.0.29	172.20.9.11	46123	53	46123 ‚Üí 53 [RST] Seq=81 Win=0 Len=0
</code></pre><p>Then I analyzed the whole packet capture to find the number of open connections at any time as well as the frequency and duration of these lingering TCP connections. <em>See <a class=link href=#appendix-analyzing-concurrent-tcp-connections>Analyzing concurrent TCP connections</a> for details on how.</em></p><p>What we find is that the number of open connections at times surge way past 20 and that coincides with increased latency. In addition unused connections stay open for a problematic long time (5-10 seconds).</p><p>Since the closing of connections is initiated by CoreDNS. Is there any way we can make CoreDNS close connections faster or limit the number of connections it uses?</p><p>NodeLocal DNSCache (in GKE at least) is configured (<code>kubectl get configmap --namespace kube-system node-local-dns -o yaml | yq .data.Corefile</code>) with:</p><pre><code>cluster.local:53 {
  forward . __PILLAR__CLUSTER__DNS__ {
      force_tcp
      expire 1s
</code></pre><p>So it is actually configured to &ldquo;expire&rdquo; connections after 1 second.</p><p>I&rsquo;m not well versed in the CoreDNS codebase but it seems expiring connections is <a class=link href=https://github.com/coredns/coredns/blob/master/plugin/pkg/proxy/persistent.go#L48 target=_blank rel=noopener>handled by a ticker</a>. A ticker in go sends a signal every &ldquo;tick&rdquo; that can be used to trigger events for example. This means connections aren&rsquo;t closed once they pass the 1 second mark, but the cleanup process runs every 1 second and then purges expired connections. So a connection can be idle for almost 2x the time (2 seconds in our case) before being cleaned up.</p><p>I still can&rsquo;t explain why we see connections lingering on for 5-10 seconds though.</p><p>There are no configuration options indicating that it&rsquo;s possible to limit the number of connections. Nor anything in the code that would suggest it&rsquo;s a possibility. Adding it is probably not done in a day either as it would require making decisions on trade-offs about how to handle excess traffic volume. How much to queue, for how long. How to avoid the queue eating too much memory. What to do when the queue is full and so on and so on.</p><p>At this point I feel we have a pretty good grasp of how all of this conspires to cause the problems we observe. But unfortunately I can&rsquo;t see any permanent solution that does not require modifying CoreDNS and ideally <code>dnsmasq</code> code itself. At the moment we don&rsquo;t have the capacity to create and push through such changes upstream. If I could snap my fingers and magically get some new features they would be:</p><ul><li><p>dnsmasq</p><ul><li>Make <code>MAX_PROCS</code> configurable.</li></ul></li><li><p>CoreDNS</p><ul><li>Configurable max TCP connection pool size. Metrics on pool size and usage.</li><li>Configurable queue size for requests waiting for a new connection from the pool. Metrics on queue capacity (max) and current size for tuning.</li></ul></li></ul><p>But until that becomes a reality I think the best option is to avoid using NodeLocal DNSCache in combination with <code>kube-dns</code>, and instead replace <code>kube-dns</code> with Cloud DNS.</p><p><a id=appendices></a></p><h1 id=appendices>Appendices</h1><p><a id=appendix-enabling-additional-envoy-metrics></a></p><h2 id=appendix---enabling-additional-envoy-metrics>Appendix - Enabling additional envoy metrics</h2><p><em>See <a class=link href=https://istio.io/latest/docs/concepts/observability/#proxy-level-metrics target=_blank rel=noopener>istio.io</a> for some high level info on proxy-level metrics and <a class=link href=https://www.envoyproxy.io/docs/envoy/latest/configuration/upstream/cluster_manager/cluster_stats target=_blank rel=noopener>envoyproxy.io</a> for complete list of available metrics.</em></p><p>Most likely we are interested in the <code>upstream_cx_*</code> and <code>upstream_rq_*</code> metrics.</p><p>By default metrics are only gathered and exposed for the <code>xds-grpc</code> cluster, and the full stats name looks like <code>cluster.xds-grpc.upstream_cx_total</code>. The <code>xds-grpc</code> cluster I assume is metrics for traffic between the <code>istio-proxy</code> containers in each Pod and the central istio services (<code>istiod</code>) used for configuration management.</p><blockquote><p>A cluster in envoy is a grouping of backends and endpoints. Typically each Kubernetes <code>Service</code> will be it‚Äôs own cluster. There are also separate clusters named <code>BlackHoleCluster</code>, <code>InboundPassthroughClusterIpv4</code> and <code>PassthroughCluster</code>.</p></blockquote><p>When enabling metrics for other services (or clusters as they&rsquo;re also called) they look like</p><ul><li><code>cluster.outbound|80||my-service.my-namespace.svc.cluster.local.upstream_cx_total</code> for outgoing traffic and</li><li><code>cluster.inbound|8080||.upstream_cx_total</code> for incoming traffic.</li></ul><blockquote><p>Beware that traffic to for example api.signicat.com/some-service will be mapped to the internal Kubernetes Service DNS name in the metric, like some-service.some-team.svc.cluster.local.</p><p>Ports are also mapped. For outgoing traffic it will be the port in the Service. While for incoming traffic it will be the port the Pod is actually listening on, and not the one mapped in the Service.</p></blockquote><h3 id=enabling-additional-envoy-metrics>Enabling additional envoy metrics</h3><p>istio.io documents <a class=link href=https://istio.io/latest/docs/ops/configuration/telemetry/envoy-stats/ target=_blank rel=noopener>how to enable additional envoy metrics</a> both globally in the mesh by configuring the <code>IstioOperator</code> object as well as on a per Deployment/Pod using the <code>proxy.istio.io/config</code> annotation.</p><blockquote><p><strong>WARNING: Be very careful when enabling additional metrics as they have a tendency to expose orders of magnitude more time-series than you might expect.</strong></p><p>I apparently managed to get Prometheus OOMKilled even with my fairly limited testing on two deployments.</p><p>If you happen to have <a class=link href=https://victoriametrics.com/ target=_blank rel=noopener>VictoriaMetrics</a> in your monitoring stack you can monitor cardinality (the number of unique time-series, which is the thing that usually breaks a time-series database) in the VM UI:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>kubectl -n metrics port-forward services/victoria-metrics-cluster-vmselect 8481:8481
</span></span></code></pre></td></tr></table></div></div><p>And going to http://localhost:8481/select/0/vmui/?#/cardinality</p></blockquote><h3 id=enable-additional-metrics-on-a-deployment-or-pod>Enable additional metrics on a Deployment or Pod</h3><p>To enable additional metrics on a Deployment or Pod, add the following annotations:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>proxy.istio.io/config</span><span class=p>:</span><span class=w> </span><span class=p>|-</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>      proxyStatsMatcher:
</span></span></span><span class=line><span class=cl><span class=sd>        inclusionSuffixes:
</span></span></span><span class=line><span class=cl><span class=sd>          - &#34;upstream_rq_timeout&#34;
</span></span></span><span class=line><span class=cl><span class=sd>          - &#34;upstream_rq_retry&#34;
</span></span></span><span class=line><span class=cl><span class=sd>          - &#34;upstream_rq_retry_limit_exceeded&#34;
</span></span></span><span class=line><span class=cl><span class=sd>          - &#34;upstream_rq_retry_success&#34;
</span></span></span><span class=line><span class=cl><span class=sd>          - &#34;upstream_rq_retry_overflow&#34;</span><span class=w>      
</span></span></span></code></pre></td></tr></table></div></div><p>Remember to add this to the <strong>Pod</strong> metadata (<code>spec.template.metadata</code>) if adding to a Deployment.</p><blockquote><p>This results in a new time-series being exposed for each Service and Port (Cluster) that envoy has configured.</p><p>In our case we have ~500 services in our dev cluster and enabling these specific metrics adds ~3.800 new time-series for <em>each Pod</em> in the Deployment we added it to. The test Deployment I&rsquo;m playing with has 6 Pods so ~23.000 new time-series from adding 5 additional metrics to 1 Deployment!</p></blockquote><p>Another option is to use regex to enable additional metrics:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=w>          </span><span class=nt>inclusionRegexps</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span>- <span class=s2>&#34;.*upstream_rq_.*&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span>- <span class=s2>&#34;.*upstream_cx_.*&#34;</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>But again, enabling these ~50 metrics on this specific Deployment will result in ~250.000 new time-series.</p><blockquote><p>Be especially wary of metrics envoy exposes as histograms, such as <code>upstream_cx_connect_ms</code> and <code>upstream_cx_length_ms</code> as they result in many _bucket time-series. During my testing this resulted in 6-7 million new time-series in total.</p></blockquote><p>It&rsquo;s possible to specify only the connections we are interested in, which makes it manageable, cardinality wise. For example to only gather metrics from Services A through F in their corresponding namespaces:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=w>    </span><span class=nt>inclusionRegexps</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s2>&#34;.*(svc-a.ns-a.svc|svc-b.ns-b.svc|svc-c.ns-c.svc|svc-d.ns-d.svc|svc-e.ns-e.svc|svc-f.ns-f.svc).*.upstream_rq.*&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s2>&#34;.*(svc-a.ns-a.svc|svc-b.ns-b.svc|svc-c.ns-c.svc|svc-d.ns-d.svc|svc-e.ns-e.svc|svc-f.ns-f.svc).*.upstream_cx.*&#34;</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><h3 id=enable-additional-metrics-globally-please-dont>Enable additional metrics globally (please don‚Äôt!)</h3><p>You can also enable additional metrics globally across the mesh. But that&rsquo;s probably a very bad idea if you are running at any sort of scale. I estimated that enabling <code>.*upstream_rq_.*</code> and <code>.*upstream_cx_.*</code> in our dev cluster would result in 50M additional time-series at a minimum. Or 5-10x our current Prometheus usage.</p><p>If you are using the old <a class=link href=https://istio.io/latest/docs/setup/install/operator/ target=_blank rel=noopener>Istio Operator</a> way of installing and managing istio it should be enough to update the <code>IstioOperator</code> object in the cluster. If you are using <code>istioctl</code> (<a class=link href=https://istio.io/latest/about/faq/#install-method-selection target=_blank rel=noopener>recommended</a>) you must update the source <code>IstioOperator</code> manifest that is being fed to <code>istioctl</code> and run the appropriate <code>istioctl</code> commands again to update. Note that this also creates an <code>IstioOperator</code> object in the cluster with whatever config is used. But in this case it&rsquo;s never used for anything other than reference. So updating the <code>IstioOperator</code> object in a cluster if managing istio with <code>istioctl</code> does nothing.</p><h3 id=viewing-stats-and-metrics>Viewing stats and metrics</h3><p>Metrics should start becoming available in Prometheus with names like <code>envoy_cluster_upstream_cx_total</code>. Note that by default you&rsquo;ll already see metrics from the <code>xds-grpc</code> cluster.</p><p>You can also get the <a class=link href=https://istio.io/latest/docs/ops/configuration/telemetry/envoy-stats/ target=_blank rel=noopener>stats directly from a sidecar</a>. Either by querying the <code>pilot-agent</code> directly:</p><pre><code>kubectl exec -n my-namespace my-pod -c istio-proxy -- pilot-agent request GET stats
</code></pre><p>Or querying the metrics endpoint exposed to Prometheus:</p><pre><code>kubectl exec -n my-namespace my-pod -c istio-proxy -- curl -sS 'localhost:15000/stats/prometheus'
</code></pre><p>Note that these will not give you any additional metrics compared to those exposed to Prometheus. A metric that is not enabled will not show up, and all metrics that are enabled are also automatically exposed to Prometheus.</p><h3 id=notes-on-timeout-and-retry-metrics>Notes on timeout and retry metrics</h3><p>If istio is configured to be involved with timeouts and retries that is configured on the <a class=link href=https://istio.io/latest/docs/reference/config/networking/virtual-service/#Destination target=_blank rel=noopener>VirtualService</a> level.</p><p>That means it will only take effect if using <code>api.signicat.com/some-service</code> (a route in a Istio <code>VirtualService</code>) and not <code>some-service.some-namespace.svc.cluster.local</code> (Kubernetes <code>Service</code>).</p><p>Istio will only count timeouts and retries for requests to a <code>VirtualService</code> route that has timeouts (and optionally retries) configured.</p><p>Additionally, the timeouts and retries seems to be enforced, and counted, in the Source istio-proxy (envoy), and not the Target.</p><h3 id=further-work>Further work</h3><p>It would be beneficial to be able to have envoy only show/export metrics that are non-zero. Since there is usually only a very small set of all possible Service-to-Service pairs that will actually regularly have traffic.</p><p>It‚Äôs possible to customize metrics using the Telemetry API (<a class=link href=https://istio.io/latest/docs/tasks/observability/metrics/telemetry-api/ target=_blank rel=noopener>Customizing Istio Metrics with Telemetry API</a>) but it seems limited to only working with metrics and their dimensions. Not the time-series values.</p><p><a class=link href=https://istio.io/latest/docs/reference/config/proxy_extensions/wasm-plugin/ target=_blank rel=noopener>WASM plugins</a> are probably not a good fit either and are experimental and causes a severe CPU and memory penalty.</p><blockquote><p><strong>Edit to add in 2024</strong></p><p>We know today that the reason for so many time-series (reporting zero), as well as elevated memory usage in <code>istio-proxy</code>, is because we did not filter exposed Services between namespaces causing every <code>istio-proxy</code> to keep track of every pair of possible workloads in the whole cluster.</p></blockquote><p><a id=appendix-overview-of-dns-on-gke></a></p><h2 id=appendix---overview-of-dns-on-gke>Appendix - Overview of DNS on GKE</h2><blockquote><p><strong>kube-dns and CoreDNS confusion</strong></p><p>The main difference between DNS on upstream Kubernetes and GKE is that Kubernetes <a class=link href=https://kubernetes.io/blog/2018/07/10/coredns-ga-for-kubernetes-cluster-dns/ target=_blank rel=noopener>switched to</a> CoreDNS 5 years ago while GKE <a class=link href=https://cloud.google.com/kubernetes-engine/docs/how-to/kube-dns target=_blank rel=noopener>still uses</a> the old <code>kube-dns</code>. It‚Äôs <a class=link href=https://cloud.google.com/knowledge/kb/how-to-run-coredns-on-kubernetes-engine-000004698 target=_blank rel=noopener>possible to add CoreDNS but it‚Äôs not possible to remove or disable</a> <code>kube-dns</code>.</p><p>In upstream K8s, <a class=link href=https://kubernetes.io/blog/2018/07/10/coredns-ga-for-kubernetes-cluster-dns/ target=_blank rel=noopener>CoreDNS reached GA back in 2018 in K8s 1.11</a> and <code>kube-dns</code> was <a class=link href=https://kubernetes.io/docs/tasks/administer-cluster/coredns/#migrating-to-coredns target=_blank rel=noopener>removed from kubeadm in K8s 1.21</a>.</p><p>However for <a class=link href=https://github.com/coredns/deployment/issues/116 target=_blank rel=noopener>backwards compatibility</a> CoreDNS <a class=link href=https://github.com/coredns/deployment/blob/master/kubernetes/coredns.yaml.sed target=_blank rel=noopener>still uses</a> the name <code>kube-dns</code>, which makes things confusing for sure!</p></blockquote><p><img src=/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/DNS-on-GKE.png width=1313 height=586 srcset="/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/DNS-on-GKE_hu2575099721844054612.png 480w, /p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/DNS-on-GKE_hu15301862917548489956.png 1024w" loading=lazy alt="Diagram showing DNS on GKE" class=gallery-image data-flex-grow=224 data-flex-basis=537px></p><p>Part of the magic of Kubernetes is it&rsquo;s DNS-based Service Discovery. Lets say we deploy an application that has a <code>Service</code> named <code>helloworld</code> to the namespace <code>team-a</code>. Other applications in the same namespace are able to connect to that service by calling for example <code>http://helloworld</code>. This makes it easy to build and configure a group of microservices that talk to each other without needing to know about namespaces or FQDNs. Applications in another namespace can also call that application using <code>http://helloworld.team-a</code>. (<a class=link href=#footnote-d>Footnote D</a>).</p><p>This magic is achieved using the standard DNS <code>search</code> domain list feature. On Linux the <code>/etc/resolv.conf</code> <a class=link href=https://man7.org/linux/man-pages/man5/resolv.conf.5.html target=_blank rel=noopener>file</a> defines which DNS servers to use. It also includes a <code>search</code> option that works together with the <code>ndots</code> option. You can see this by executing <code>cat /etc/resolv.conf</code> in any <code>Pod</code>.</p><p>If a domain name contains fewer &ldquo;dots&rdquo; (".") than <code>ndots</code> is set to, the DNS resolver will first try the hostname with each of the appended domains in <code>search</code>. In Kubernetes (and GKE) ndots is by default set to 5.</p><p>In vanilla Kubernetes the search domains are<code> &lt;namespace>.svc.cluster.local svc.cluster.local cluster.local</code> while on GKE it&rsquo;s <code>&lt;namespace>.svc.cluster.local svc.cluster.local cluster.local &lt;gcp-zone>.c.technology-dev-platform.internal c.&lt;gcp-project>.internal google.internal</code>.</p><p>This means if we call <code>https://api.signicat.com</code> (2 dots) it will first try to resolve <code>api.signicat.com.some-namespace.svc.cluster.local</code> and so on through the whole list, sequentally, before finally trying <code>api.signicat.com</code> and actually succeeding.</p><p><em>See <a class=link href=#appendix-reducing-dns-lookups>Reducing DNS lookups in Kubernetes and GKE</a> for thoughts on avoiding some of this.</em></p><p>Then the requests are sent to the <code>dnsmasq</code> container of a <code>kube-dns</code> Pod. <code>dnsmasq</code> is configured by a combination of command-line arguments defined in the <code>kube-dns</code> Deployment and data from the <code>kube-dns</code> <a class=link href=https://cloud.google.com/kubernetes-engine/docs/how-to/kube-dns target=_blank rel=noopener>ConfigMap</a>. We have not changed the default <code>kube-dns</code> ConfigMap and this results in <code>dnsmasq</code> running with these arguments:</p><pre><code>/usr/sbin/dnsmasq -k \
    --cache-size=1000 \
    --no-negcache \
    --dns-forward-max=1500 \
    --log-facility=- \
    --server=/cluster.local/127.0.0.1#10053 \
    --server=/in-addr.arpa/127.0.0.1#10053 \
    --server=/ip6.arpa/127.0.0.1#10053 \
    --max-ttl=30 \
    --max-cache-ttl=30 \
    --server /internal/169.254.169.254 \
    --server 8.8.8.8 \
    --server 8.8.4.4 \
    --no-resolv
</code></pre><p>This means all lookups for <code>cluster.local</code>, <code>in-addr.arpa</code> and <code>ip6.arpa</code> will be sent to <code>127.0.0.1:10053</code>, which is the <code>kube-dns</code> container. Lookups for <code>internal</code> are sent to <code>169.254.169.254</code>. All others are sent to <code>8.8.8.8</code> and <code>8.8.4.4</code>.</p><p>In addition <code>--no-negcache</code> disables caching for lookups that were not found (<code>NXDOMAIN</code>). This is particularly interesting since that means when looking up for example <code>api.signicat.com</code> and the domains in the <code>search</code> list are first tried, they will all result in <code>NXDOMAIN</code> but <code>dnsmasq</code> will not cache those results but send them on to <code>kube-dns</code>. <em>Every</em>. <em>Time</em>. This may severely reduce the usefulness of the cache and create a constant volume of traffic to <code>kube-dns</code> itself.</p><blockquote><p>It&rsquo;s also worth noting that Google Public DNS servers have a <a class=link href=https://developers.google.com/speed/public-dns/docs/isp target=_blank rel=noopener>default rate limit of 1500 QPS per IP</a>.</p></blockquote><p><a id=appendix-reducing-dns-lookups></a></p><h2 id=appendix---reducing-dns-lookups-in-kubernetes-and-gke>Appendix - Reducing DNS lookups in Kubernetes and GKE</h2><p>There are two reasons for the 7 DNS lookups instead of the ideal 1. First is the <code>ndots</code> which tells the DNS resolver in the container if a domain name has fewer dots than this, it will first try looking up the name by appending each entry in the <code>search</code> list sequentially. For Kubernetes <code>ndots</code> is set to 5 (why is <a class=link href=https://github.com/kubernetes/kubernetes/issues/33554#issuecomment-266251056 target=_blank rel=noopener>explained by Tim Hockin here</a>). So <code>api.signicat.com</code> only has 2 dots, and hence will first go through the list of search domains.</p><p>Secondly GKE adds 3 extra search domains in addition to the 3 standard ones in Kubernetes. Bringing the total to 6 before doing the &ldquo;proper&rdquo; DNS lookup.</p><p>The specific assumption we are deviating from leading to problems in our case is &ldquo;We could mitigate some of the perf penalties by always trying names as upstream FQDNs first, but that means that all intra-cluster lookups get slower. Which do we expect more frequently? I&rsquo;ll argue intra-cluster names, if only because the TTL is so low.&rdquo;</p><p>An option that we‚Äôre currently exploring is using a trailing dot in the FQDN (<code>api.signicat.com.</code>) when calling other services. This explicitly tells DNS that this is a FQDN and should not search through the search domain lists for a hit first. This seems to work on some of our services but not all. Indicating that there isn‚Äôt any inherent problems doing this with regards to istio or other infrastructure. But there may be additional changes needed on some services to support this. I‚Äôm suspecting certain web application frameworks in certain languages not handling this well out of the box.</p><p><a id=appendix-overview-of-gke-dns-with-nodelocal-dnscache></a></p><h2 id=appendix---overview-of-gke-dns-with-nodelocal-dnscache>Appendix - Overview of GKE DNS with NodeLocal DNSCache</h2><p><img src=/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/DNS-on-GKE-with-Node-Local-DNS-Cache.png width=1201 height=855 srcset="/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/DNS-on-GKE-with-Node-Local-DNS-Cache_hu15358002095465690191.png 480w, /p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/DNS-on-GKE-with-Node-Local-DNS-Cache_hu7906030369844017798.png 1024w" loading=lazy alt="Diagram showing DNS on GKE with NodeLocal DNSCache enabled" class=gallery-image data-flex-grow=140 data-flex-basis=337px></p><p><a class=link href=https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/ target=_blank rel=noopener>NodeLocal DNSCache</a> is a feature of Kubernetes that primarily aims to improve performance, scale and latency by:</p><ul><li>Potentially not traversing the network to another node</li><li>Skip iptables DNAT which sometimes caused problems</li><li>Upgrading connections from UDP to TCP which should reduce latency in case of dropped packets</li><li>Enabling negative caching</li></ul><p>NodeLocal DNSCache is <a class=link href=https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache target=_blank rel=noopener>available as a GKE add-on</a>.</p><p>NodeLocal DNSCache is deployed as a <code>DaemonSet</code> named <code>node-local-dns</code> in <code>kube-system</code> namespace. One <code>node-local-dns-x</code> Pod is created on each node in the cluster.</p><p>The <code>node-local-dns-x</code> Pods run CoreDNS and are configured by the <code>node-local-dns</code> ConfigMap (<code>kubectl get cm node-local-dns -o yaml|yq .data.Corefile</code>).</p><p>The configuration is similar to <code>dnsmasq</code> in that requests for <code>cluster.local</code>, <code>in-addr.arpa</code> and <code>ip6.arpa</code> are sent to <code>kube-dns</code> while the rest are sent to <code>8.8.8.8</code> and <code>8.8.4.4</code>.</p><p>It binds to (re-uses) the same IP as the <code>kube-dns</code> Service. That way all requests from Pods on the node towards <code>kube-dns</code> will actually be handled by <code>node-local-dns</code> instead. And to actually communicate with <code>kube-dns</code> another Service named <code>kube-dns-upstream</code> is created that is a clone of the <code>kube-dns</code> Service but with a different IP.</p><p>Even though <code>node-local-dns</code> uses CoreDNS and <a class=link href=https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack target=_blank rel=noopener>kube-prometheus-stack</a> has support for scraping CoreDNS it won&rsquo;t necessarily work for <code>node-local-dns</code>. See the next appendix for how to scrape metrics from <code>node-local-dns</code>.</p><p><a id=appendix-enabling-node-local-dns-metrics></a></p><h2 id=appendix---enabling-node-local-dns-metrics>Appendix - Enabling node-local-dns metrics</h2><p>We wanted to look at some metrics from node-local-dns, but found that we didn&rsquo;t have any!</p><p>The <code>node-local-dns</code> pods do have annotations for scraping metrics:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>prometheus.io/port</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;9253&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>prometheus.io/scrape</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;true&#34;</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>But in our Prometheus we don&rsquo;t use the <code>kubernetes-pods</code> <a class=link href=https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml#L998 target=_blank rel=noopener>scrape job config from the prometheus example</a>. meaning that these targets will not be discovered or scraped by prometheus. (And we don&rsquo;t want to add and allow for scraping this way).</p><p>The <a class=link href=https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack target=_blank rel=noopener>kube-prometheus-stack helm chart</a> comes with both a <a class=link href=https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/templates/exporters/core-dns/service.yaml target=_blank rel=noopener>Service</a> and <a class=link href=https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/templates/exporters/core-dns/servicemonitor.yaml target=_blank rel=noopener>ServiceMonitor</a> to scrape metrics from CoreDNS (which NodeLocal DNSCache uses).</p><p>However the Service and ServiceMonitor uses a hardcoded selector of <code>k8s-app: kube-dns</code> while <code>node-local-dns</code> Pods have <code>k8s-app: node-local-dns</code>.</p><p>We made some changes to these and deployed them to the cluster and now started getting metrics in the <code>coredns_dns_*</code> timeserieses.</p><blockquote><p>You can add our updated Service and ServiceMonitor like this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>kubectl apply -f -n kube-system https://raw.githubusercontent.com/signicat/blog-attachements/main/2023-gke-node-local-dns-cache/files/node-local-dns-metrics-service.yaml
</span></span><span class=line><span class=cl>kubectl apply -f -n metrics https://raw.githubusercontent.com/signicat/blog-attachements/main/2023-gke-node-local-dns-cache/files/node-local-dns-servicemonitor.yaml
</span></span></code></pre></td></tr></table></div></div><p>And metrics should start flowing within a couple of minutes.</p></blockquote><p>In new versions of the <a class=link href=https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/templates/grafana/dashboards-1.14/k8s-coredns.yaml target=_blank rel=noopener>CoreDNS dashboard</a> that comes with <code>kube-prometheus-stack</code> you should be able to select the <code>node-local-dns</code> job.</p><blockquote><p>I added the <code>job</code> template variable to the dashboard in <a class=link href=https://github.com/prometheus-community/helm-charts/pull/3798 target=_blank rel=noopener>this PR</a>, which may not have made it into a new version of the <code>kube-prometheus-stack</code> chart yet. In the mean time you can use <a class=link href=https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/grafana-dashboard-coredns.json target=_blank rel=noopener>our updated CoreDNS dashboard</a> which adds the necessary template variable as well as a couple of other improvements.</p></blockquote><p><a id=appendix-using-dnsperf></a></p><h2 id=appendix---using-dnsperf-to-test-dns-performance>Appendix - Using dnsperf to test DNS performance</h2><p>Trying to tease out more information on the problem we run some DNS load testing using <a class=link href=https://linux.die.net/man/1/dnsperf target=_blank rel=noopener>dnsperf</a>.</p><blockquote><p>An alternative to <code>dnsperf</code> is <a class=link href=https://linux.die.net/man/1/resperf target=_blank rel=noopener>resperf</a> which is &ldquo;a companion tool to dnsperf&rdquo; designed for testing resolution performance of a caching DNS server. Whereas <code>dnsperf</code> is primarily meant to test authoritative DNS servers. Since <code>resperf</code> is more complicated to work with, and we want to test at a fairly low volume, we assume that <code>dnsperf</code> is good enough for now.</p></blockquote><p>We spin up a new pod with Ubuntu to run dnsperf from</p><pre><code>kubectl run dnsperf -n default --image=ubuntu:22.04 -i --tty --restart=Never
</code></pre><p>And install dnsperf</p><pre><code>apt-get update &amp;&amp; apt-get install -y dnsperf
</code></pre><p>We&rsquo;ll be testing three different destinations.</p><ul><li><code>kube-dns</code> Service: First we use the IP of the <code>kube-dns</code> Service (<code>172.18.0.10</code>) which will be intercepted by the <code>node-local-dns</code> Pod on the same node. It will do it&rsquo;s caching and the traffic is visible in our modified CoreDNS Grafana dashboard for that node.</li><li><code>kube-dns-upstream</code> Service: Then we use the IP of the <code>kube-dns-upstream</code> Service (<code>172.18.165.139</code>) which is a copy of the <code>kube-dns</code> Service that <code>node-local-dns</code> uses when looking up <code>.cluster.local</code> domains. It has a different IP than <code>kube-dns</code> so that it won&rsquo;t be intercepted by <code>node-local-dns</code>.</li><li>One <code>kube-dns</code> Pod: Then we use the IP of one specific <code>kube-dns</code> Pod (<code>172.20.14.115</code>) so that we can observe the behaviour of one instance without any load balancing sprinkling the traffic all over the place.</li></ul><p><code>dnsperf</code> takes a list of domains to look up in a file that looks like</p><pre><code>archive.ubuntu.com A
test.salesforce.com A
storage.googleapis.com A
</code></pre><p>To create a file with the domains we actually look up I do a packet capture on one of the <code>node-local-dns</code> Pods for half an hour. Open it in Wireshark. Filter by <code>dns.flags.response == 1</code>. Add DNS -> Queries -> query -> Name as a Column. Export Packet Dissections as CSV. And use Excel and Notepad to get a file of 50.000 DNS names in that format. Upload that file to the newly created <code>dnsperf</code> Pod. Create a separate file with only <code>cluster.local</code> domains as well (<code>cat domains-all.txt | grep "cluster.local" > domains-cluster-local.txt</code>).</p><blockquote><p><code>dnsperf</code> does not expand/multiply domains by adding the domains from <code>search-path</code> in <code>/etc/resolve.conf</code>. The list we extracted from the packet capture already have these additional lookups though so it is representative nonetheless.</p></blockquote><p>Run dnsperf:</p><pre><code>dnsperf -d domains-all.txt -l 60 -Q 100 -S 5 -t 2 -s 172.18.0.10
</code></pre><p>Explanation of arguments:</p><ul><li><code>-d domains-all.txt</code> - Domain list file.</li><li><code>-l 60</code> - Length (duration) of test in seconds.</li><li><code>-Q 100</code> - Queries per second.</li><li><code>-S 5</code> - Print statistics every 5 seconds.</li><li><code>-t 2</code> - Timeout 2 seconds.</li><li><code>-s 172.18.0.18</code> - DNS server to query</li></ul><p>Each test has different values for the arguments.</p><p>Results from running a series of tests ranging from 100 Queries per second (QPS) to 1500 QPS:</p><p><img src=/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/2023-12-14-table-load-tests.png width=889 height=499 srcset="/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/2023-12-14-table-load-tests_hu14351295037840062642.png 480w, /p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/2023-12-14-table-load-tests_hu1398839472891939138.png 1024w" loading=lazy alt="Load test results table" class=gallery-image data-flex-grow=178 data-flex-basis=427px></p><blockquote><p>These results are from a second round of testing a couple of weeks after the first. On the first round I observed a lot (6%) timing out on just 200 QPS directly towards a <code>kube-dns</code> Pod. I&rsquo;m not sure why the results are suddenly much better. Looking at the graphs from <code>node-local-dns</code> there is a significant improvement overall some days before the second round of testing. We have not done any changes that could explain the sudden improvement. I guess it&rsquo;s just one of those things&mldr;</p><p>CoreDNS did <a class=link href=https://coredns.io/2018/11/27/cluster-dns-coredns-vs-kube-dns/ target=_blank rel=noopener>a benchmark</a> of <code>kube-dns</code> and <code>CoreDNS</code> and managed to get ~36.000 QPS on internal names and ~2.200 QPS on external names on <code>kube-dns</code>.</p></blockquote><p><a id=appendix-network-packet-capture></a></p><h2 id=appendix---network-packet-capture-without-dependencies>Appendix - Network packet capture without dependencies</h2><p>I haven&rsquo;t done packet capture in a Kubernetes cluster before. First I tried <a class=link href=https://github.com/eldadru/ksniff target=_blank rel=noopener>ksniff</a> that <a class=link href=https://anythingsimple.medium.com/how-to-do-network-sniff-for-kubernetes-pod-running-on-gke-fb23d0b63e95 target=_blank rel=noopener>this blog post describes</a>. But no dice.</p><blockquote><p>Another method that is much more reliable is running tcpdump in a <a class=link href=https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container target=_blank rel=noopener>debug container</a> as mentioned <a class=link href=https://downey.io/blog/kubernetes-ephemeral-debug-container-tcpdump/ target=_blank rel=noopener>here</a>. He pipes tcpdump from a container directly to wireshark on his own machine. But since I&rsquo;m using <code>kubectl</code> etc inside Ubuntu on WSL2 on Windows that is probably going to require much more setup.</p></blockquote><p>Instead I&rsquo;m opting for just saving packet captures in the container as files and copy them to a directory on my machine accessible by Windows.</p><pre><code># Spin up a new container using the ubuntu:22.04 image in the existing node-local-dns-7vqpp Pod. Try to attach to the process-namespace of the node-cache container if possible.
kubectl debug -c debug -n kube-system -it node-local-dns-7vqpp --image=ubuntu:22.04 --target=node-cache
</code></pre><p>As soon as the image is downloaded and container started your console should give you a new shell inside the container where we can start preparing:</p><pre><code># Update apt and install tshark. Enter &quot;no&quot; for &quot;Should non-superusers be able to capture packets?&quot;
apt-get update &amp;&amp; apt-get install -y tshark

# Capture 100 packets from eth0 to verify that things are working
tshark -i eth0 -n -c 100
</code></pre><p>At first I captured 1 million packets without any filters. That resulted in a 6GB file which may be a bit on the large side when I&rsquo;m just interested in some DNS lookups. So lets add a capture filter <code>port 53</code> going forward.</p><p>Capturing DNS packets to and from a <code>node-local-dns</code> Pod:</p><pre><code># In the shell running in the debug container with tshark installed:
date; tshark -i eth0 -n -c 1000000 -f &quot;port 53&quot; -w node-local-dns-7vqpp-eth0.pcap
</code></pre><p>After either capturing 100.000 packets or I&rsquo;m happy I stop the capture with <code>Ctrl+C</code> and I can download the pcap file in WSL:</p><pre><code># On your local machine:
kubectl cp -c debug kube-system/node-local-dns-7vqpp:node-local-dns-7vqpp-eth0.pcap /mnt/c/Data/node-local-dns-7vqpp-eth0.pcap
</code></pre><p>Then browse to <code>C:\Data\</code> where you can open <code>node-local-dns-7vqpp-eth0.pcap</code> in <a class=link href=https://www.wireshark.org/ target=_blank rel=noopener>Wireshark</a>.</p><blockquote><p>I print the current time on the Pod just before starting packet capture. Most of the time the correct UTC time will be recorded on the packets. But in case it isn&rsquo;t and the time starts from 0, I can use that time to adjust the offset at least roughly. Making it easier to correlate packet captures from different Pods.</p></blockquote><p>To capture on the receiving <code>kube-dns</code> Pod, both incoming DNS traffic to the <code>dnsmasq</code> container on port 53 on <code>eth0</code>, and between <code>dnsmasq</code> and <code>kube-dns</code> containers on <code>lo</code> port 10053:</p><pre><code>kubectl debug -c debug -n kube-system -it kube-dns-d7bc86d4c-d2x8p --image=ubuntu:22.04 --target=dnsmasq

# Install tshark as shown above

# Start two packet captures running in the background:
date; tshark -i eth0 -n -c 1000000 -f &quot;port 53&quot; -w kube-dns-d7bc86d4c-d2x8p-eth0.pcap &amp;
date; tshark -i lo -n -c 1000000 -f &quot;port 10053&quot; -w kube-dns-d7bc86d4c-d2x8p-lo.pcap &amp;

# To stop these type `fg` in the shell to bring a background process to the foreground and stop it with `Ctrl+C`.
# Then `fg` and `Ctrl+C` again to stop the other.
</code></pre><p>Download the files to my local machine as before:</p><pre><code>kubectl cp -c debug kube-system/kube-dns-d7bc86d4c-d2x8p:kube-dns-d7bc86d4c-d2x8p-eth0.pcap /mnt/c/Data/kube-dns-d7bc86d4c-d2x8p-eth0.pcap
kubectl cp -c debug kube-system/kube-dns-d7bc86d4c-d2x8p:kube-dns-d7bc86d4c-d2x8p-lo.pcap /mnt/c/Data/kube-dns-d7bc86d4c-d2x8p-lo.pcap
</code></pre><p><em>Check out <a class=link href=#analyzing-dns-problems-based-on-packet-capture>Analyzing DNS problems based on packet captures</a> for some tips and tricks on analyzing the packet captures.</em></p><p><a id=appendix-verbose-logging-on-kube-dns></a></p><h2 id=appendix---verbose-logging-on-kube-dns>Appendix - Verbose logging on <code>kube-dns</code></h2><p>The <a class=link href=https://dnsmasq.org/docs/dnsmasq-man.html target=_blank rel=noopener>dnsmasq man page</a> lists several interesting options such as <code>--log-queries=extra</code> and <code>--log-debug</code>!</p><p>But it&rsquo;s not possible to make changes to the <code>kube-dns</code> Deployment since it&rsquo;s managed by GKE. Any changes you make will be reverted immediately.</p><p>Instead we take the existing manifests for <code>kube-dns</code>, modify them and create a parallel deployment that we control:</p><p><a class=link href=https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/kube-dns-deployment.yaml target=_blank rel=noopener>kube-dns-deployment.yaml</a></p><ul><li>Deployment named <code>kube-dns-debug</code>.</li><li>A couple of annotations commented out that would otherwise make the Deployment be instantly removed.</li><li>Keep existing <code>k8s-app: kube-dns</code> label on the Pods so they will receive traffic for the <code>kube-dns-upstream</code> Service.</li><li>An additional <code>reason: debug</code> label on the Pods so we can target them specifically.</li><li>Additional logging enabled with <code>--log-queries=extra</code>, <code>--log-debug</code> and <code>--log-async=25</code>. I enabled these one at a time but the log volume with everything enabled isn&rsquo;t overwhelming.</li></ul><p><a class=link href=https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/kube-dns-debug-service.yaml target=_blank rel=noopener>kube-dns-debug-service.yaml</a></p><ul><li>Service named <code>kube-dns-debug</code> with port udp+tcp/53 targeting only the Pods with the added <code>reason: debug</code> label. I used this service to run load tests only towards these Pods.</li></ul><p><a class=link href=https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/kube-dns-metrics-service.yaml target=_blank rel=noopener>kube-dns-metrics-service.yaml</a></p><ul><li>Service named <code>kube-dns-debug-metrics</code> with port tcp/10054 and tcp/10055 targeting the same Pods as above but for exposing metrics.</li></ul><p><a class=link href=https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/kube-dns-servicemonitor.yaml target=_blank rel=noopener>kube-dns-servicemonitor.yaml</a></p><ul><li>ServiceMonitor named <code>kube-dns-debug</code> that targets the <code>kube-dns-debug-metrics</code> Service above.</li></ul><blockquote><p>Strictly speaking the <code>kube-dns-debug</code> Service, <code>kube-dns-debug-metrics</code> Service and <code>kube-dns-debug</code> ServiceMonitor isn&rsquo;t necessary if using the <code>k8s-app: kube-dns</code> label on the new Pods. But it makes it possible to separate the two deployments completely by using another label on the Pods such as <code>k8s-app: kube-dns-debug</code> and thus avoid for example load tests affecting real cluster DNS traffic.</p></blockquote><p>Now some DNS requests should arrive at the <code>kube-dns-debug</code> Pods with additional logging enabled.</p><p>It&rsquo;s also possible to force all DNS traffic to this Pod by manually adding the <code>reason: debug</code> label as a selector on the <code>kube-dns-upstream</code> Service. It will stay that way for &ldquo;a while&rdquo; (hours, maybe days) before being reverted. Plenty of time to play around at least.</p><p><a id=appendix-analyzing-dnsmasq-logs></a></p><h2 id=appendix---analyzing-dnsmasq-logs>Appendix - Analyzing <code>dnsmasq</code> logs</h2><p>Once we have increased the log verbosity of <code>dnsmasq</code> we can see if there&rsquo;s anything to learn there.</p><p>First I used <a class=link href=https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/grafana-dashboard-coredns.json target=_blank rel=noopener>our updated CoreDNS dashboard</a> to identify a time interval where we observed latency spikes. Then using loki, our centralized log storage, I download about 5000 lines of logs spanning about 50 seconds worth of logs. (Our loki is limited to loading 5000 lines, therefore it&rsquo;s important to try to narrow down and find logs where we are actually experiencing issues).</p><pre><code>2023-10-10T10:02:36+02:00 I1010 08:02:36.571446   1 nanny.go:146] dnsmasq[4811]: 315207 10.0.0.83/56382 forwarded metadata.google.internal.cluster.local to 127.0.0.1#10053
2023-10-10T10:02:36+02:00 I1010 08:02:36.571457   1 nanny.go:146] dnsmasq[4811]: 315207 10.0.0.83/56382 reply metadata.google.internal.cluster.local is NXDOMAIN
2023-10-10T10:02:36+02:00 I1010 08:02:36.571488   1 nanny.go:146] dnsmasq[4810]: 315107 10.0.0.83/24595 forwarded metadata.google.internal.cluster.local to 127.0.0.1#10053
2023-10-10T10:02:36+02:00 I1010 08:02:36.571495   1 nanny.go:146] dnsmasq[4810]: 315107 10.0.0.83/24595 reply metadata.google.internal.cluster.local is NXDOMAIN
2023-10-10T10:02:36+02:00 I1010 08:02:36.575810   1 nanny.go:146] dnsmasq[4810]: 315108 10.0.0.83/24595 query[A] metadata.google.internal.some-namespace.svc.cluster.local from 10.0.0.83
2023-10-10T10:02:36+02:00 I1010 08:02:36.576101   1 nanny.go:146] dnsmasq[4810]: 315108 10.0.0.83/24595 forwarded metadata.google.internal.some-namespace.svc.cluster.local to 127.0.0.1#10053
2023-10-10T10:02:36+02:00 I1010 08:02:36.576132   1 nanny.go:146] dnsmasq[4810]: 315108 10.0.0.83/24595 reply metadata.google.internal.some-namespace.svc.cluster.local is NXDOMAIN
</code></pre><p>Here we can see exact time (<code>08:02:36.571446</code>). Which <code>dnsmasq</code> process logged the line, identified by the process ID (<code>dnsmasq[4811]</code>). A number identifying an individual request (<code>315207</code>). And the client IP and port (<code>10.0.0.83/56382</code>). One process always maps to one TCP connection so PID and client IP and port will always match.</p><p>Just from this snippet we can see that multiple DNS requests are handled by each process and we have exact time for when individual requests were received from the client as well as exact time for replies.</p><p>I wrote a small (and ugly) <a class=link href=https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/analyze-dnsmasq-logs/main.go target=_blank rel=noopener>program</a> to analyze the logs. It gathers the duration of each request identified by the request number and each &ldquo;session&rdquo; identified by the process ID.</p><p>It filters out requests faster than 2 milliseconds and sessions shorter than 500 milliseconds. The output looks like:</p><pre><code>Request: PID 5439 RequestNumber 356558 Domain . Duration 2.273ms
Request: PID 4914 RequestNumber 319022 Domain api.statuspage.io.some-namespace.svc.cluster.local Duration 2.619ms
Request: PID 4915 RequestNumber 319123 Domain api.statuspage.io.cluster.local Duration 3.088ms
Session: PID 4890 Duration 1.005229s
Session: PID 5022 Duration 852.861ms
Session: PID 5435 Duration 2.627174s
</code></pre><p>The vast majority of individual requests complete in microseconds, and none are slower than 10 milliseconds. This is an indication that the delays aren&rsquo;t coming from the processing of individual requests.</p><p>Sessions however last much longer, regularly in the 1-3 second range. This isn&rsquo;t necessarily a problem since it&rsquo;s resource efficient to keep sessions longer and re-using them for many requests.</p><p><a id=appendix-analyzing-concurrent-tcp-connections></a></p><h2 id=appendix---analyzing-concurrent-tcp-connections>Appendix - Analyzing concurrent TCP connections</h2><p>I want to see how many connections are open at any point (and not impacted by metric scraping intervals etc) as well as how long they tend to stay idle before being closed.</p><p>I made another small (and probably even uglier) <a class=link href=https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/analyze-tcp-conns/main.go target=_blank rel=noopener>program</a> to analyze the packet captures (exported from Wireshark as CSV) from <code>kube-dns</code> and try to answer those questions.</p><p>While iterating through every packet it keeps a counter on how many distinct connections are observed as well as the time since the previous packet in the same connection was observed:</p><pre><code>Time: 08:08:30.610010781 TimeShort: 08:08:30 Port: 61236 Action: open Result: none Flags: PSHACK IdleGroup: fast ConIdleTime: 564.26¬µs ActiveConnections: 11
Time: 08:08:30.610206151 TimeShort: 08:08:30 Port: 10806 Action: open Result: none Flags: ACK IdleGroup: fast ConIdleTime: 178.4¬µs ActiveConnections: 11
Time: 08:08:30.900267009 TimeShort: 08:08:30 Port: 62083 Action: open Result: none Flags: FINACK IdleGroup: slow ConIdleTime: 6.429595796s ActiveConnections: 11
</code></pre><p>Plotting <code>ActiveConnections</code> in the Excel graph we have from <a class=link href=#analyzing-dns-problems-based-on-packet-capture>Analyzing DNS problems based on packet captures</a>:</p><p><img src=/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-and-tcp-conns-and-packet-latency.png width=1428 height=489 srcset="/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-and-tcp-conns-and-packet-latency_hu6833197055541243622.png 480w, /p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-and-tcp-conns-and-packet-latency_hu1786475887642530671.png 1024w" loading=lazy alt="Graph showing active TCP connections in green. DNS resolution time from packet captures in blue. Number of dnsmasq processes in orange" class=gallery-image data-flex-grow=292 data-flex-basis=700px></p><p>Up to the limit of 20 <code>dnsmasq</code> processes they follow the number of open TCP connections pretty closely. However for long periods of time there are way more open connections than <code>dnsmasq</code> is allowed to spawn new child processes to handle. This also overlaps with the sudden huge increases in latency. Another thing we can infer from this is that new TCP connections are successfully being opened from <code>node-local-dns</code> (CoreDNS) to <code>dnsmasq</code>, even though <code>dnsmasq</code> is unable to handle them yet. Probably the master <code>dnsmasq</code> process accepts the connections but blocking the request until there is room to spawn a new child process.</p><p><code>grep</code>ing for &ldquo;slow&rdquo; we get all packets where the connection was idle for more than 1 second:</p><pre><code>Time: 08:08:18.064874738 TimeShort: 08:08:18 Port: 19956 Action: open Result: none Flags: FINACK IdleGroup: slow ConIdleTime: 7.535928188s ActiveConnections: 9
Time: 08:08:18.064888758 TimeShort: 08:08:18 Port: 30168 Action: open Result: none Flags: FINACK IdleGroup: slow ConIdleTime: 7.577724318s ActiveConnections: 9
Time: 08:08:18.064909758 TimeShort: 08:08:18 Port: 41718 Action: open Result: none Flags: FINACK IdleGroup: slow ConIdleTime: 7.393646609s ActiveConnections: 9
Time: 08:08:18.064911088 TimeShort: 08:08:18 Port: 30386 Action: open Result: none Flags: FINACK IdleGroup: slow ConIdleTime: 7.535962768s ActiveConnections: 9
Time: 08:08:18.064920468 TimeShort: 08:08:18 Port: 21482 Action: open Result: none Flags: FINACK IdleGroup: slow ConIdleTime: 7.535946008s ActiveConnections: 9
</code></pre><p>This is particularly interesting since <code>node-local-dns</code> is configured to expire connections after 1 second. And in theory should be cleaned up after at most 2 seconds. I managed to keep myself from diving head first into that particular rabbit hole though.</p><blockquote><p>The underlying <a class=link href=https://github.com/miekg/dns/ target=_blank rel=noopener>dns Go library</a> that CoreDNS uses also has a <a class=link href=https://github.com/miekg/dns/blob/master/server.go#L18 target=_blank rel=noopener>limit of 128 DNS queries</a> for a single TCP connection before closing it.</p></blockquote><p><a id=analyzing-dns-problems-based-on-packet-capture></a></p><h2 id=appendix---analyzing-dns-problems-based-on-packet-captures>Appendix - Analyzing DNS problems based on packet captures</h2><p>Now that we have some packet captures we can start dissecting and analyzing and looking for the needle in the haystack.</p><p>I&rsquo;ll be using <a class=link href=https://www.wireshark.org/ target=_blank rel=noopener>Wireshark</a> for this.</p><p><a class=link href="https://www.youtube.com/watch?v=RRjutHjGdCY" target=_blank rel=noopener>This youtube video</a> shows a few neat tricks. Thanks!</p><ul><li>If you are looking at the traffic between <code>dnsmasq</code> and <code>kube-dns</code> (<code>lo</code> network interface) go to <strong>Analyze</strong> -> <strong>Decode</strong> as and add a mapping from port 10053 to DNS to have Wireshark decode the packets correctly.</li><li>Start by applying the display filter <code>dns.flags.response == 1</code> to only show DNS responses.</li><li>Find a DNS Response packet and find the <code>[Time: n.nnn seconds]</code> field, right click it and <strong>Apply as Column</strong>.</li><li>You can also add <code>Transaction ID</code> and the <code>Name</code> field (under <strong>Queries</strong>) as columns as well.</li></ul><p>Then you can export as CSV for example in <strong>File</strong> -> <strong>Export Packet Dissections&mldr;</strong> -> As <strong>CSV</strong>.</p><p>Importing the data from the <code>node-local-dns</code> packet capture into Excel (yes, there&rsquo;s no way escaping Excel!) and plotting the duration of each and every DNS lookup over a 20 minute period, colored by upstream server:</p><p><img src=/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dns-lookup-time-by-upstream-server-over-time.png width=1948 height=694 srcset="/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dns-lookup-time-by-upstream-server-over-time_hu1554638069529912429.png 480w, /p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dns-lookup-time-by-upstream-server-over-time_hu8836196662265939971.png 1024w" loading=lazy alt="Graph showing DNS resolution time colored by upstream server based on packet captures on node-local-dns Pod" class=gallery-image data-flex-grow=280 data-flex-basis=673px></p><p>We clearly see huge spikes in latency. But it seems to only affect queries being forwarded to the two <code>kube-dns</code> Pods (<code>172.20.1.38</code> & <code>172.20.7.57</code>).</p><p>Another interesting finding is that it appears to happen at the exact same time on both <code>kube-dns</code> Pods. Weird. If we didn&rsquo;t already know that (for at least some queries) the added duration happens inside the <code>dnsmasq</code> container, I would probably suspect a problem on the network.</p><p>Plotting the duration on requests arriving at <code>dnsmasq</code> on one of the <code>kube-dns</code> Pods shows the same pattern:</p><p><img src=/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-latency.png width=1568 height=711 srcset="/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-latency_hu12456545354413195016.png 480w, /p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-latency_hu7087990665509006188.png 1024w" loading=lazy alt="Graph showing DNS resolution time based on packet captures on kube-dns Pod" class=gallery-image data-flex-grow=220 data-flex-basis=529px></p><p>Another thing I noticed is that sometimes close to when request duration would spike, Wireshark warns about <code>TCP Port numbers reused</code>:</p><pre><code>42878	19:09:03.987437048	127.0.0.1	127.0.0.1	TCP	78	44516	[TCP Port numbers reused] 44516 ‚Üí 10053 [SYN] Seq=0 Win=43690 Len=0
</code></pre><p>However Wireshark doesn&rsquo;t discriminate whether that was 20 minutes or 2 seconds ago. Only that it occurs in the same packet capture.</p><p>One hypothesis I had was that outgoing requests from <code>dnsmasq</code> to <code>kube-dns</code> would be stuck waiting for available TCP ports. I plotted the source port usage over time:</p><p><img src=/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-source-port.png width=1569 height=695 srcset="/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-source-port_hu14734794848197582075.png 480w, /p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-source-port_hu12858696458376013384.png 1024w" loading=lazy alt="Graph showing TCP source port for connections from node-local-dns based on packet captures" class=gallery-image data-flex-grow=225 data-flex-basis=541px></p><p>It did not strengthen my suspicion and some random checking shows that the TCP Port reuse is far enough spaced in time (minutes) to avoid problems. So for the time being I&rsquo;m not digging further into this.</p><p><a id=footnotes></a></p><h1 id=footnotes>Footnotes</h1><p><a id=footnote-a></a><strong>[A]</strong> This doesn&rsquo;t strictly mean that 20 new TCP connections are required since many of them are probably retries.</p><p><a id=footnote-b></a><strong>[B]</strong> Note that even if the graph doesn&rsquo;t hit 22 you may still be affected. The number of processes is counted only at the exact time the metrics are scraped, in our case 10 seconds. You can manually sample the process count by attaching a debug container to <code>dnsmasq</code> (<code>kubectl debug -c debug -n kube-system -it kube-dns-6fb7c8866c-bxj7f --image=ubuntu:22.04 --target=dnsmasq</code>) and running <code>for i in $(seq 1 1800) ; do echo "$(date) Try: ${i} DnsmasqProcess: $(pidof dnsmasq | wc -w)"; sleep 1; done</code>.</p><p><a id=footnote-c></a><strong>[C]</strong> CPU throttling would not be an issue in this case since <code>kube-dns</code> does not have CPU limits set.</p><p><a id=footnote-d></a><strong>[D]</strong> Although you should be cautious of calling services by name in other namespaces if they are owned by different teams. As that introduces coupling on what should be implementation details across team boundaries. In Signicat we always call the full FQDN and path if connecting to services owned by other teams.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/kubernetes/>Kubernetes</a>
<a href=/tags/gke/>Gke</a>
<a href=/tags/dns/>Dns</a></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Dec 13, 2023 00:00 UTC</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/kubernetes-sidecar-config-drift/><div class=article-image><img src=/p/kubernetes-sidecar-config-drift/2022-05-03-kubernetes-sidecar-config-drift.099fc947effb12fa5df1b55ff2abbf0f_hu6448980740578202474.png width=250 height=150 loading=lazy alt="Featured image of post Kubernetes Sidecar Config Drift" data-hash="md5-CZ/JR+/7Evpd8bVf8qu/Dw=="></div><div class=article-details><h2 class=article-title>Kubernetes Sidecar Config Drift</h2></div></a></article><article class=has-image><a href=/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/><div class=article-image><img src=/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2021-03-06-a-side-quest-in-api-dev-operations-cloud-and-database.1109b0baf5d1a0def45dd7a81497e5e5_hu15535093335078461266.png width=250 height=150 loading=lazy alt="Featured image of post A side quest in API development, observability, Kubernetes and cloud with a hint of database" data-hash="md5-EQmwuvXRoN70XdeoFJfl5Q=="></div><div class=article-details><h2 class=article-title>A side quest in API development, observability, Kubernetes and cloud with a hint of database</h2></div></a></article><article class=has-image><a href=/p/mini-post-down-scaling-azure-kubernetes-service-aks/><div class=article-image><img src=/p/mini-post-down-scaling-azure-kubernetes-service-aks/2019-06-04-downscaling-aks.329aba45d5c630b3af497bbb7a43f404_hu14101488926934199810.png width=250 height=150 loading=lazy alt="Featured image of post Mini-post: Down-scaling Azure Kubernetes Service (AKS)" data-hash="md5-Mpq6RdXGMLOvSXu7ekP0BA=="></div><div class=article-details><h2 class=article-title>Mini-post: Down-scaling Azure Kubernetes Service (AKS)</h2></div></a></article><article class=has-image><a href=/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/><div class=article-image><img src=/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/2019-02-23-disk-performance-on-aks-part-1.ab18dd1b8670f56fe898797506d57abc_hu17823714411359167148.png width=250 height=150 loading=lazy alt="Featured image of post Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking" data-hash="md5-qxjdG4Zw9W/omHl1BtV6vA=="></div><div class=article-details><h2 class=article-title>Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking</h2></div></a></article><article class=has-image><a href=/p/managed-kubernetes-on-microsoft-azure-english/><div class=article-image><img src=/p/managed-kubernetes-on-microsoft-azure-english/2017-12-23-managed-kubernetes-on-azure-eng.bf0fcb04c841710921f10d2e885d8555_hu4992157927612583129.png width=250 height=150 loading=lazy alt="Featured image of post Managed Kubernetes on Microsoft Azure (English)" data-hash="md5-vw/LBMhBcQkh8Q0uiF2FVQ=="></div><div class=article-details><h2 class=article-title>Managed Kubernetes on Microsoft Azure (English)</h2></div></a></article></div></div></aside><div class=disqus-container></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2014 -
2024 blog.stian.omg.lol</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>