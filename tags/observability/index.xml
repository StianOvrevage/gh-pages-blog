<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Observability on blog.stian.omg.lol</title><link>https://demo.stack.jimmycai.com/tags/observability/</link><description>Recent content in Observability on blog.stian.omg.lol</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 06 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://demo.stack.jimmycai.com/tags/observability/index.xml" rel="self" type="application/rss+xml"/><item><title>A side quest in API development, observability, Kubernetes and cloud with a hint of database</title><link>https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/</link><pubDate>Sat, 06 Mar 2021 00:00:00 +0000</pubDate><guid>https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/</guid><description>&lt;p>Quite often people ask me what I actually do. I have a hard time giving a short answer. Even to colleagues and friends in the industry.&lt;/p>
&lt;p>Here I will try to show and tell how I spent an evening digging around in a system I helped build for a client.&lt;/p>
&lt;br>
&lt;hr>
&lt;br>
&lt;p>&lt;strong>Table of contents&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="#Background" >Background&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#TheProblem" >The (initial) problem&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#FurtherReading" >Fixing the (initial) problem&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Verifying the (initial) fix&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#FurtherReading" >Baseline simple request - HTTP1 1 connections, 20000 requests&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Baseline complex request - HTTP1 1 connections, 20000 requests&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Verifying the fix for assumed workload&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#FurtherReading" >Complex request - HTTP1 6 connections, 500 requests&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Complex request - HTTP2 500 &amp;ldquo;connections&amp;rdquo;, 500 requests&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Side quest: Database optimizations&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Determining the next bottleneck&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Side quest: Cluster resources and burstable VMs&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Conclusion" >Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="Background">&lt;/a>&lt;/p>
&lt;h1 id="background">Background
&lt;/h1>&lt;p>I&amp;rsquo;m a consultant doing development, DevOps and cloud infrastructure.&lt;/p>
&lt;p>For this specific client I mainly develop APIs using Golang to support new products and features as well as various exporting, importing and processing of data in the background.&lt;/p>
&lt;p>I&amp;rsquo;m also the &amp;ldquo;ops&amp;rdquo; guy handling everything in AWS, setting up and maintaing databases, making sure the &amp;ldquo;DevOps&amp;rdquo; works and the frontend and analytics people can do their work with little friction.
99% of the time things work just fine. No data is lost. The systems very rarely have unforeseen downtime and the users can access the data they want with acceptable latency rarely exceeding 500ms.&lt;/p>
&lt;p>A couple of times a year I assess the status of the architecture and set up new environments from scratch and update any documentation that has drifted. This is also a good time to do changes and add or remove constraints in anticipation of future business needs.&lt;/p>
&lt;p>In short, the current tech stack that has evolved over a couple of years is:&lt;/p>
&lt;ul>
&lt;li>Everything hosted on Amazon Web Services (AWS).&lt;/li>
&lt;li>AWS managed Elastic Kubernetes Service (EKS) currently on K8s 1.18.&lt;/li>
&lt;li>GitHub Actions for building Docker images for frontends, backends and other systems.&lt;/li>
&lt;li>AWS Elastic Container Registry for storing Docker images.&lt;/li>
&lt;li>Deployment of each system defined as a Helm chart alongside source code.&lt;/li>
&lt;li>Actual environment configuration (Helm values) stored in repo along source code. Updated by GitHub Actions.&lt;/li>
&lt;li>ArgoCD in cluster to manage status of all environments and deployments. Development environments usually automatically deployed on change. Push a button to deploy to Production.&lt;/li>
&lt;li>Prometheus for storing metrics from the cluster and nodes itself as well as custom metrics for our own systems.&lt;/li>
&lt;li>Loki for storing logs. Makes it easier to retrieve logs from past Pods and aggregate across multiple Pods.&lt;/li>
&lt;li>Elastic APM server for tracing.&lt;/li>
&lt;li>Pyroscope for live CPU profiling/tracing of Go applications.&lt;/li>
&lt;li>Betteruptime.com for tracking uptime and hosting status pages.&lt;/li>
&lt;/ul>
&lt;p>I might write up a longer post about the details if anyone is interested.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h1 id="the-initial-problem">The (initial) problem
&lt;/h1>&lt;p>A week ago I upgraded our API from version 1, that was deployed in January, to version 2 with new features and better architecture.&lt;/p>
&lt;p>One of the endpoints of the API returns an analysis of an object we track. I have previously reduced the amount of database queries by 90% but it still requires about 50 database calls from three different databases.
Getting and analyzing the data usually completes in about 3-400 milliseconds returning an 11.000 line JSON.&lt;/p>
&lt;p>It&amp;rsquo;s also possible to just call &lt;code>/objects/analysis&lt;/code> to get the analysis for all the 500 objects we are tracking. It takes 20 seconds but is meant for exports to other processes and not interactive use, so not a problem.&lt;/p>
&lt;p>Since the product is under very active development the frontend guys just download the whole analysis for an object to show certain relevant information to users. It&amp;rsquo;s too early to decide on which information is needed more often and how to optimize for that. Not a problem.&lt;/p>
&lt;p>So we need an overview of some fields from multiple objects in a dashboard / list. We can easily pull analysis from 20 objects without any noticable delay.&lt;/p>
&lt;p>But what if we just want to show more, 50? 200? 500? The frontend already have the IDs for all the objects and fetches them from &lt;code>/objects/id/analysis&lt;/code>. So they loop over the IDs and fire of requests simultaneously.&lt;/p>
&lt;p>Analyzing the network waterfall in Chrome DevTools indicated that the requests now took 20-30 seconds to complete! But looking closer most of the time they were actually queued up in the browser. This is because
Chrome only allows 6 concurrent TCP connection to the same origin when using HTTP1 (&lt;a class="link" href="https://developers.google.com/web/tools/chrome-devtools/network/understanding-resource-timing%29" target="_blank" rel="noopener"
>https://developers.google.com/web/tools/chrome-devtools/network/understanding-resource-timing)&lt;/a>.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h2 id="fixing-the-initial-problem">Fixing the (initial) problem
&lt;/h2>&lt;p>HTTP2 should fix this problem easily. By default HTTP2 is disabled in nginx-ingress. I add a couple of lines enabling it and update the Helm deployment of the ingress controller.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h2 id="verifying-the-initial-fix">Verifying the (initial) fix
&lt;/h2>&lt;p>Some common development tools doesn&amp;rsquo;t support HTTP2, such as Postman. So I found &lt;code>h2load&lt;/code> which can both help me verify HTTP2 is working and I also get to measure the improvement, nice!&lt;/p>
&lt;blockquote>
&lt;p>Note that I&amp;rsquo;m not using the analysis endpoint since I want to measure the change from HTTP1 to HTTP2 and it will become apparent later that there are other bottlenecks preventing us from a linear performance increase when just changing from HTTP1 to HTTP2.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Also note that this is somewhat naive since it requests the same URL over and over which can give false results due to any caching. But fortunately we don&amp;rsquo;t do any caching yet.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h3 id="baseline-simple-request---http1-1-connections-20000-requests">Baseline simple request - HTTP1 1 connections, 20000 requests
&lt;/h3>&lt;p>Using 1 concurrent streams, 1 client and HTTP1 I get an estimate of performance pre-http2:&lt;/p>
&lt;pre>&lt;code>h2load --h1 --requests=20000 --clients=1 --max-concurrent-streams=1 https://api.x.com/api/v1/objects/1
&lt;/code>&lt;/pre>
&lt;p>The results are as expected:&lt;/p>
&lt;pre>&lt;code>finished in 1138.99s, 17.56 req/s, 18.41KB/s
requests: 20000 total, 20000 started, 20000 done, 19995 succeeded, 5 failed, 0 errored, 0 timeout
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-apm.png"
width="1816"
height="705"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-apm_hu17076544040877865054.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-apm_hu9690961613720477067.png 1024w"
loading="lazy"
alt="Overview from Elastic APM. Duration is very acceptable at around 20ms. No errors. And about 25% of the time spent doing database queries."
class="gallery-image"
data-flex-grow="257"
data-flex-basis="618px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-cpu.png"
width="1034"
height="314"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-cpu_hu2013984859425220242.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-cpu_hu13236457479005124721.png 1024w"
loading="lazy"
alt="Container CPU usage. Nothing special."
class="gallery-image"
data-flex-grow="329"
data-flex-basis="790px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-latency.png"
width="861"
height="356"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-latency_hu1867474644236441644.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-latency_hu15523458481306920119.png 1024w"
loading="lazy"
alt="Database query latency. The vast majority under 5ms. Acceptable."
class="gallery-image"
data-flex-grow="241"
data-flex-basis="580px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-queries.png"
width="780"
height="348"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-queries_hu17361066750422420350.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-queries_hu10420784785634132053.png 1024w"
loading="lazy"
alt="Number of DB queries per second."
class="gallery-image"
data-flex-grow="224"
data-flex-basis="537px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-latency.png"
width="863"
height="316"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-latency_hu6805455395055610702.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-latency_hu3298924550317512719.png 1024w"
loading="lazy"
alt="HTTP response latency."
class="gallery-image"
data-flex-grow="273"
data-flex-basis="655px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-requests.png"
width="778"
height="217"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-requests_hu5945712598158928612.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-requests_hu419352892190289981.png 1024w"
loading="lazy"
alt="Number of HTTP requests per second. Unsurprisingly the number of database queries are identical to the number of HTTP requests. Latency of HTTP requests also tracks the latency of the (single) database query."
class="gallery-image"
data-flex-grow="358"
data-flex-basis="860px"
>&lt;/p>
&lt;p>For http2 we set max concurrent streams to the same as number of requests:&lt;/p>
&lt;pre>&lt;code>h2load --requests=200 --clients=1 --max-concurrent-streams=200 https://api.x.com/api/v1/objects/1
&lt;/code>&lt;/pre>
&lt;p>Which results in almost half the latency:&lt;/p>
&lt;pre>&lt;code>finished in 1.23s, 162.65 req/s, 158.06KB/s
requests: 200 total, 200 started, 200 done, 200 succeeded, 0 failed, 0 errored, 0 timeout
&lt;/code>&lt;/pre>
&lt;p>So HTTP2 is working and providing significant latency improvements. Success!&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h3 id="baseline-complex-request---http1-1-connections-20000-requests">Baseline complex request - HTTP1 1 connections, 20000 requests
&lt;/h3>&lt;p>We start by establishing a baseline with 1 connection querying over and over.&lt;/p>
&lt;pre>&lt;code>h2load --h1 --requests=20000 --clients=1 --max-concurrent-streams=1
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-apm.png"
width="2087"
height="707"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-apm_hu13565166093859492250.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-apm_hu14225806095910349646.png 1024w"
loading="lazy"
alt="Latency increases as much more computation is done and data is returned. But the latency is consistent which is good. We also see that the database is becomming the bottleneck for where most time is spent."
class="gallery-image"
data-flex-grow="295"
data-flex-basis="708px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-cpu.png"
width="1200"
height="308"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-cpu_hu4067009256511003722.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-cpu_hu17950687200232907900.png 1024w"
loading="lazy"
alt="CPU usage increased to 15%. Lower increase than expected considering the complexity involved in serving the requests."
class="gallery-image"
data-flex-grow="389"
data-flex-basis="935px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-latency.png"
width="985"
height="344"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-latency_hu7089278039337102831.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-latency_hu17575998733902921949.png 1024w"
loading="lazy"
alt="Database query latency still mostly under 5ms."
class="gallery-image"
data-flex-grow="286"
data-flex-basis="687px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-queries.png"
width="894"
height="351"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-queries_hu6509951597429203579.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-queries_hu14549008744426915283.png 1024w"
loading="lazy"
alt="Number of database queries increases by a factor of 10 compared to HTTP requests."
class="gallery-image"
data-flex-grow="254"
data-flex-basis="611px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-latency.png"
width="987"
height="313"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-latency_hu565398038381364000.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-latency_hu7243746795286387547.png 1024w"
loading="lazy"
alt="HTTP latency."
class="gallery-image"
data-flex-grow="315"
data-flex-basis="756px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-requests.png"
width="895"
height="219"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-requests_hu9040739607883373017.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-requests_hu3790465645525383070.png 1024w"
loading="lazy"
alt="HTTP requests per second."
class="gallery-image"
data-flex-grow="408"
data-flex-basis="980px"
>&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h2 id="verifying-the-fix-for-assumed-workload">Verifying the fix for assumed workload
&lt;/h2>&lt;p>So we verified that HTTP2 gives us a performance boost. But what happens when we fire away 500 requests to the much heavier &lt;code>/analysis&lt;/code> endpoint?&lt;/p>
&lt;blockquote>
&lt;p>These graphs are not as pretty since the ones above. This is mainly due to the sampling interval of the metrics and that we need several datapoints to accurately determine the rate() of a counter.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h3 id="complex-request---http1-6-connections-500-requests">Complex request - HTTP1 6 connections, 500 requests
&lt;/h3>&lt;pre>&lt;code>finished in 32.25s, 14.88 req/s, 2.29MB/s
requests: 500 total, 500 started, 500 done, 500 succeeded, 0 failed, 0 errored, 0 timeout
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-apm.png"
width="1484"
height="705"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-apm_hu2573287072721533229.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-apm_hu10620414983979950514.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-apm"
class="gallery-image"
data-flex-grow="210"
data-flex-basis="505px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-cpu.png"
width="847"
height="299"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-cpu_hu1135877464954207689.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-cpu_hu17008451797633252465.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-cpu"
class="gallery-image"
data-flex-grow="283"
data-flex-basis="679px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-latency.png"
width="706"
height="359"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-latency_hu9358974391483687749.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-latency_hu16352819740947538611.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-db-latency"
class="gallery-image"
data-flex-grow="196"
data-flex-basis="471px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-queries.png"
width="637"
height="352"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-queries_hu17797633323803646239.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-queries_hu2207066698060964218.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-db-queries"
class="gallery-image"
data-flex-grow="180"
data-flex-basis="434px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-latency.png"
width="700"
height="321"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-latency_hu13253021454442065078.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-latency_hu13649080090034874975.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-http-latency"
class="gallery-image"
data-flex-grow="218"
data-flex-basis="523px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-requests.png"
width="638"
height="213"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-requests_hu12544065771888019895.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-requests_hu1798439057131787658.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-http-requests"
class="gallery-image"
data-flex-grow="299"
data-flex-basis="718px"
>&lt;/p>
&lt;p>In summary it so far seems to scale linearly with load. Most of the time is spent fetching data from the database. Still very predictable low latency on database queries and the resulting HTTP response.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h3 id="complex-request---http2-500-connections-500-requests">Complex request - HTTP2 500 &amp;ldquo;connections&amp;rdquo;, 500 requests
&lt;/h3>&lt;p>&lt;em>So now we unleash the beast. Firing all 500 requests at the same time.&lt;/em>&lt;/p>
&lt;pre>&lt;code>finished in 16.66s, 30.02 req/s, 3.55MB/s
requests: 500 total, 500 started, 500 done, 500 succeeded, 0 failed, 0 errored, 0 timeout
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-cpu.png"
width="939"
height="307"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-cpu_hu5828488102210360607.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-cpu_hu3343570902582545481.png 1024w"
loading="lazy"
alt="CPU on API still doing good. A slight hint of CPU throttling due to CFS, which is used when you set CPU limits in Kubernetes."
class="gallery-image"
data-flex-grow="305"
data-flex-basis="734px"
>&lt;/p>
&lt;blockquote>
&lt;p>Important about Kubernetes and CPU limits&lt;br />
Even with CPU limits set to 1 (100% of one CPU), your container can still be throttled at much lower CPU usage. Check out &lt;a class="link" href="https://medium.com/omio-engineering/cpu-limits-and-aggressive-throttling-in-kubernetes-c5b20bd8a718" target="_blank" rel="noopener"
>this article&lt;/a> for more information.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-latency.png"
width="782"
height="346"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-latency_hu16596262103242219393.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-latency_hu6743393326456624241.png 1024w"
loading="lazy"
alt="Whopsie. The average database query latency has increased drastically, and we have a long tail of very slow queries. Looks like we are starting to see signs of bottlenecks on the database. This might also be affected by our maximum of 60 concurrent connections to the database, resulting in queries having to wait their turn before executing."
class="gallery-image"
data-flex-grow="226"
data-flex-basis="542px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-queries.png"
width="705"
height="346"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-queries_hu17270843372303526721.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-queries_hu14745640312772792195.png 1024w"
loading="lazy"
alt="Its hard to judge the peak rate of database queries due to limited sampling of the metrics."
class="gallery-image"
data-flex-grow="203"
data-flex-basis="489px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-http-latency.png"
width="783"
height="309"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-http-latency_hu9429010041105922198.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-http-latency_hu14677623071583589615.png 1024w"
loading="lazy"
alt="Now individual HTTP requests are much slower due to waiting for the database."
class="gallery-image"
data-flex-grow="253"
data-flex-basis="608px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-apm-trace.png"
width="1150"
height="1192"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-apm-trace_hu12116834356231894546.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-apm-trace_hu17401424198592652531.png 1024w"
loading="lazy"
alt="Here is just a random trace from Elastic APM to see if the increased database latency is concentrated to specific queries or tables or just general saturation. Indeed there is a single query responsible for half the time taken for the entire query! We better get back to that in a bit and dig further."
class="gallery-image"
data-flex-grow="96"
data-flex-basis="231px"
>&lt;/p>
&lt;p>In an ideal world all 500 requests should start and complete in 2-300ms regardless. Since that is not happening it&amp;rsquo;s an indication that we are now hitting some other bottleneck.&lt;/p>
&lt;p>Looking at the graphs it seems we are starting to saturate the database. The latency for every request is now largely dependent on the slowest of the 10-12 database queries it depends on. And as we are stressing the database the probability of slow queries increase. The latency for the whole process of fetching 500 requests are again largely dependent on the slowest requests.&lt;/p>
&lt;p>So this optimization gives on average better performance, but more variability of the individual requests, when the system is under heavy load.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h1 id="side-quest-database-optimizations">Side quest: Database optimizations
&lt;/h1>&lt;p>It seems we are saturating the database. Before throwing more money at the problem (by increasing database size) I like to know what the bottlenecks are. Looking at the traces from APM
I see one query that is consistently taking 10x longer than the rest. I also confirm this in the AWS RDS Performance Insights that show the top SQL queries by load.&lt;/p>
&lt;p>When designing the database schema I came up with the idea of having immutability for certain data types. So instead of overwriting row with ID 1, we add a row with ID 1 Revision 2. Now we have the history of who did what to the data and can easily track changes and roll back if needed. The most common use case is just fetching the last revision. So for simplicity I created a PostgreSQL view that only shows the last revision. That way clients don&amp;rsquo;t have to worry about the existense of revisions at all. That is now just an implementation detail.&lt;/p>
&lt;p>When it comes to performance that turns out to be an important implementation detail. The view is using &lt;code>SELECT DISTINCT ON (id) ... ORDER BY id, revision DESC&lt;/code>. However many of the queries to the view is ordering the returned data by time, and expect the data returned from database to already be ordered chronologically. Using &lt;code>EXPLAIN ANALYZE&lt;/code> on the queries this always results in a full table scan instead of using indexes, and is what&amp;rsquo;s causing this specific query to be slow. Without going into details it seems there is no simple and efficient way of having a view with the last revision and query that for a subset of rows ordered again by time.&lt;/p>
&lt;p>For the forseable future this does not actually impact real world usage. It&amp;rsquo;s only apparent under artificially large loads under the worst conditions. But now we know where we need to refactor things if performance actually becomes a problem.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h1 id="determining-the-next-bottleneck">Determining the next bottleneck
&lt;/h1>&lt;p>Whenever I fix one problem I like to know where, how and when the next problem or limit is likely to appear. When increasing the number of requests and streams I expected to see increasing latency. But instead I see errors appear like a cliff:&lt;/p>
&lt;pre>&lt;code>finished in 27.33s, 36.59 req/s, 5.64MB/s
requests: 5000 total, 1002 started, 1002 done, 998 succeeded, 4002 failed, 4000 errored, 0 timeout
&lt;/code>&lt;/pre>
&lt;p>Consulting the logs for both the nginx load balancer and the API there are no records of failing requests. Since nginx does not pass the HTTP2 connection directly to the API, but instead &amp;ldquo;unbundles&amp;rdquo; them into HTTP1 requests I suspect there might be issues with connection limits or even available ports from nginx to the API. But maybe it&amp;rsquo;s a configuration issue. By default nginx does &lt;a class="link" href="http://nginx.org/en/docs/http/ngx_http_upstream_module.html#server" target="_blank" rel="noopener"
>not limit the number of connections to a backend&lt;/a> (our API). . But, there is actually a &lt;a class="link" href="https://nginx.org/en/docs/http/ngx_http_v2_module.html#http2_max_requests" target="_blank" rel="noopener"
>default limit to the number of HTTP2 requests that can be served over a single connection&lt;/a> - And it happens to be 1000.&lt;/p>
&lt;p>I leave it at that. It&amp;rsquo;s very unlikely we&amp;rsquo;ll be hitting these limits any time soon.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h1 id="side-quest-cluster-resources-and-burstable-vms">Side quest: Cluster resources and burstable VMs
&lt;/h1>&lt;p>When load testing the first time around sometimes Grafana would also become unresponsive. That&amp;rsquo;s usually a bad sign. It might indicate that the underlying infrastructure is also reaching saturation. That is not good since it can impact what should be independent services.&lt;/p>
&lt;p>Our Kubernetes cluster is composed of 2x t3a.medium on demand nodes and 2x t3a.medium spot nodes. These VM types are burstable. You can use 20% per vCPU sustained over time without problems. If you exceed those 20% you start consuming CPU credits faster than they are granted and once you run out of CPU credits processes will be forcibly throttled.&lt;/p>
&lt;p>Of course Kubernetes does not know about this and expects 1 CPU to actually be 1 CPU. In addition Kubernetes will decide where to place workloads based on their stated resource requirements and limits, and not their actual resource usage.&lt;/p>
&lt;p>When looking at the actual metrics two of our nodes are indeed out of CPU credits and being throttled. The sum of factors leading to this is:&lt;/p>
&lt;ul>
&lt;li>We have not yet set resource requests and limits making it harder for Kubernetes to intelligently place workloads&lt;/li>
&lt;li>Using burstable nodes having some additional constraints not visible to Kubernetes&lt;/li>
&lt;li>Old deployments laying around consuming unnecessary resources&lt;/li>
&lt;li>Adding costly features without assessing the overall impact&lt;/li>
&lt;/ul>
&lt;p>I have not touched on the last point yet. I started adding &lt;a class="link" href="https://pyroscope.io/" target="_blank" rel="noopener"
>Pyroscope&lt;/a> to our systems since I simply love monitoring All The Things. The documentation does not go into specifics but emphasizes that it&amp;rsquo;s &amp;ldquo;low overhead&amp;rdquo;. Remember that our budget for CPU usage is actually 40% per node, not 200%. The Pyroscope server itself consumes 10-15% CPU which seems fair. But investigating further the Pyroscope agent also consumes 5-6% CPU per instance. This graph shows the CPU usage of a single Pod before and after turning off Pyroscope profiling.&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/pyroscope-agent-cpu.png"
width="1029"
height="271"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/pyroscope-agent-cpu_hu15074112692027608241.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/pyroscope-agent-cpu_hu2516833612973000297.png 1024w"
loading="lazy"
alt="pyroscope-agent-cpu"
class="gallery-image"
data-flex-grow="379"
data-flex-basis="911px"
>&lt;/p>
&lt;p>5-6% CPU overhead on a highly utilized service is probably worth it. But when the baseline CPU usage is 0% CPU and we have multiple services and deployments in different environments we are suddenly using 40-60% CPU on profiling and less than 1% on actual work!&lt;/p>
&lt;p>The outcome of this is that we need to separate burstable and stable load deployments. Monitoring and supporting systems are usually more stable resource wise while the actual business systems much more variable, and suitable for burst nodes. In practice we add a node pool of non-burst VMs and use NodeAffinity to stick Prometheus, Pyroscope etc to those nodes. Another benefit of this is that the supporting systems needed to troubleshoot problems are now less likely to be impacted by the problem itself, making troubleshooting much easier.&lt;/p>
&lt;p>&lt;a id="Conclusion">&lt;/a>&lt;/p>
&lt;h1 id="conclusion">Conclusion
&lt;/h1>&lt;p>This whole adventure only took a few hours but resulted in some specific and immediate performance gains. It also highlighted the weakest links in our application, database and infrastructure architecture.&lt;/p></description></item><item><title>Next generation monitoring with OpenTSDB</title><link>https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/</link><pubDate>Mon, 02 Jun 2014 19:56:40 +0000</pubDate><guid>https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/</guid><description>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure1.png" alt="Featured image of post Next generation monitoring with OpenTSDB" />&lt;h3 id="2021-update-the-specific-tools-discussed-in-this-blog-post-should-be-considered-obsolete-by-todays-standards-you-should-investigate-prometheus-influxdb-and-timescaledb-for-your-monitoring-needs">2021 Update: The specific tools discussed in this blog post should be considered obsolete by todays standards. You should investigate Prometheus, InfluxDB and TimescaleDB for your monitoring needs.
&lt;/h3>&lt;p>In this paper we will provide a step by step guide on how to install a single-instance of &lt;strong>OpenTSDB&lt;/strong> using the latest versions of the underlying technology, &lt;strong>Hadoop&lt;/strong> and &lt;strong>HBase&lt;/strong>. We will also provide some background on the state of existing monitoring solutions.&lt;/p>
&lt;p>&lt;a id="Abstract">&lt;/a>&lt;/p>
&lt;p>Table of contents&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="#Abstract" >Abstract&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Background" >Background&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Performanceproblems" >Performance problems - Welcome to I/O-hell&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Scaling" >Scaling problems&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Loss" >Loss of detail&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#flexibility" >Lack of flexibility&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#revolution" >The monitoring revolution&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Debian" >Setting up a single node OpenTSDB instance on Debian 7 Wheezy&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Hardware" >Hardware requirements&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Operating" >Operating system requirements&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#preparations" >Pre-setup preparations&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#java" >Installing java from packages&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#HBase" >Installing HBase&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#snappy" >Install snappy&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#native" >Building native libhadoop and libsnappy&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#ConfiguringHBase" >Configuring HBase&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#compression" >Testing HBase and compression&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#StartingHBase" >Starting HBase&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#InstallingOpenTSDB" >Installing OpenTSDB&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#ConfiguringOpenTSDB" >Configuring OpenTSDB&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#HBasetables" >Creating HBase tables&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#StartingOpenTSDB" >Starting OpenTSDB&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Feeding" >Feeding data into OpenTSDB&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#tcollector" >tcollector&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#peritus-tc-tools" >peritus-tc-tools&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#collectd-opentsdb" >collectd-opentsdb&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#MonitoringOpenTSDB" >Monitoring OpenTSDB&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Performancecomparison" >Performance comparison&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Collection" >Collection&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Storage" >Storage&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Conclusion" >Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="Background">&lt;/a>&lt;/p>
&lt;h2 id="background">Background
&lt;/h2>&lt;p>Since its inception in 1999 &lt;a class="link" href="http://oss.oetiker.ch/rrdtool/" target="_blank" rel="noopener"
>&lt;strong>rrdtool&lt;/strong>&lt;/a> (the underlying storage mechanism of once universal &lt;strong>MRTG&lt;/strong>) has been the base of many popular monitoring solutions; &lt;strong>Cacti&lt;/strong>, &lt;strong>collectd&lt;/strong>, &lt;strong>Ganglia&lt;/strong>, &lt;strong>Munin&lt;/strong>, &lt;strong>Observium&lt;/strong>, &lt;strong>OpenNMS&lt;/strong> and &lt;strong>Zenoss&lt;/strong>, to name a few.&lt;/p>
&lt;p>There are a number of problems with the current approach and we will highlight some of these here.&lt;/p>
&lt;p>Please note that this includes &lt;strong>Graphite&lt;/strong> and its backend &lt;strong>Whisper&lt;/strong>, which is based on the &lt;a class="link" href="http://graphite.readthedocs.org/en/0.9.10/whisper.html" target="_blank" rel="noopener"
>same basic design as rrdtool&lt;/a> and has &lt;a class="link" href="http://dieter.plaetinck.be/on-graphite-whisper-and-influxdb.html" target="_blank" rel="noopener"
>some of the same limitations&lt;/a>.&lt;/p>
&lt;p>&lt;a id="Performanceproblems">&lt;/a>&lt;/p>
&lt;h3 id="performance-problems---welcome-to-io-hell">Performance problems - Welcome to I/O-hell
&lt;/h3>&lt;p>When MRTG and rrdtool was created the preservation of disk space was more important than preservation of disk operations and the default collection interval was 5 minutes (which many are still using). The way rrdtool is designed it requires quite a few random reads and writes per datapoint. It also re-reads, computes the average, and writes old data again according to the RRA rules defined which causes additional I/O load. In 2014 memory is cheap, disk storage is cheap and CPU is fairly cheap. Disk I/O operations (IOPS) however are still very expensive in terms of hardware. The recent maturing of SSD provides extreme amounts of IOPS for a reasonable price, but the drive sizes are fractional. The result is that in order to scale IOPS-wise you currently need many low-space SSDs to get the required space, or many low-IOPS spindle drives to get the required IOPS:&lt;/p>
&lt;p>&lt;a class="link" href="http://www.newegg.com/Product/Product.aspx?Item=N82E16820147251" target="_blank" rel="noopener"
>Samsung EVO 840 1TB SSD&lt;/a> - 98.000 IOPS - 470 USD&lt;/p>
&lt;p>&lt;a class="link" href="http://www.newegg.com/Product/Product.aspx?Item=N82E16822148844" target="_blank" rel="noopener"
>Seagate Barracuda 3TB&lt;/a> - 240 IOPS - 110 USD&lt;/p>
&lt;p>You would need $44.880 (408 drives) worth of spindle drives in order to match a single SSD drive in terms of I/O-performance. On the other hand a $2.000 array of spindle drives would get you a net ~54 TB of space. The cost of SSD to reach the same volume would be $25.380. Not to mention the cost of servers, power, provisioning, etc.&lt;/p>
&lt;p>&lt;strong>Note: This is the cheapest available bulk consumer drives and comparable OEM drives (&lt;a class="link" href="http://h30094.www3.hp.com/product/sku/10350615/mfg_partno/632494-B21" target="_blank" rel="noopener"
>SSD&lt;/a>, &lt;a class="link" href="http://h30094.www3.hp.com/product/sku/10389145/mfg_partno/628061-B21" target="_blank" rel="noopener"
>spindle&lt;/a>) for a HP server will be 6 to 30 times more expensive.&lt;/strong>&lt;/p>
&lt;p>In rrdtool version 1.4, released in 2009, &lt;strong>rrdcached&lt;/strong> was introduced as a caching daemon for buffering multiple data updates and reducing the number of random I/O operations by writing several related datapoints in sequence. It took a couple of years before this new feature was implemented in most of the common open source monitoring solutions.&lt;/p>
&lt;p>For a good introduction into the internals of rrdtool/rrdcached updates and the problems with I/O scaling look at presentation by Sebastian Harl, &lt;a class="link" href="http://www.netways.de/index.php?id=2815" target="_blank" rel="noopener"
>How to Escape the I/O Hell&lt;/a>&lt;/p>
&lt;p>&lt;a id="Scaling">&lt;/a>&lt;/p>
&lt;h3 id="scaling-problems">Scaling problems
&lt;/h3>&lt;p>Most of today&amp;rsquo;s monitoring systems do not easily scale-out. Scale-out, or scaling horizontally, is when you can add new nodes in response to increased load. Scaling up by replacing existing hardware with state-of-the-art hardware is both expensive and only buys you limited time before the next even more expensive necessary hardware upgrade. Many systems offer distributed polling but none offer the option of spreading out the disk load. For example; you can &lt;a class="link" href="http://community.zenoss.org/docs/DOC-2485" target="_blank" rel="noopener"
>scale Zenozz for High Availability&lt;/a> but not performance.&lt;/p>
&lt;p>&lt;a id="Loss">&lt;/a>&lt;/p>
&lt;h3 id="loss-of-detail">Loss of detail
&lt;/h3>&lt;p>Current RRD based systems will aggregate old data into averages in order to save storage space. Most technicians do not have the in depth knowledge in order to tune the rules for aggregation and will leave the default values as is. Using cacti as an example and looking at the &lt;a class="link" href="http://docs.cacti.net/manual:088:8_rrdtool#rrd_files" target="_blank" rel="noopener"
>cacti documentation&lt;/a> we see that in a very short time, 2 months, data is averaged to a single data point PER DAY. For systems such as Internet backbones where traffic vary a lot from bottom (30% utilization for example) to peak (90% utilization for example) during a day only the average of 60% is shown in the graphs. This in turn makes troubleshooting by comparing old data difficult. It makes trending based on peaks/bottoms impossible and it may also lead to wrong or delayed strategic decisions on where to invest in added capacity.&lt;/p>
&lt;p>&lt;a id="flexibility">&lt;/a>&lt;/p>
&lt;h3 id="lack-of-flexibility">Lack of flexibility
&lt;/h3>&lt;p>In order to collect, store and graph new kinds of metrics an operator would need a certain level of programming skills and experience with the internals of the monitoring system. Adding new metrics to the systems would range from hours to weeks depending on the skill and experience of the operator. Creating new graphs based on existing metrics is also very difficult on most systems. And not within reach for the average operator.&lt;/p>
&lt;p>&lt;a id="revolution">&lt;/a>&lt;/p>
&lt;h2 id="the-monitoring-revolution">The monitoring revolution
&lt;/h2>&lt;p>We are currently at the beginning of a monitoring revolution. The advent of cloud computing and big data has created a need for measuring lots of metrics for thousands of machines at small intervals. This has sparked the creation of completely new monitoring components. One of the components where we now have improved alternatives is for efficient metric storage.&lt;/p>
&lt;p>The first is &lt;strong>&lt;a class="link" href="http://opentsdb.net/" target="_blank" rel="noopener"
>OpenTSDB&lt;/a>&lt;/strong>, a &amp;ldquo;Scalable, Distributed, Time Series Database&amp;rdquo; that begun development at &lt;a class="link" href="https://www.stumbleupon.com/" target="_blank" rel="noopener"
>StumbleUpon&lt;/a> in 2011 and aimed at solving some of the problems with existing monitoring systems. OpenTSDB is built in top of Apache HBase which is a scalable and performant database that builds on top of Apache Hadoop. Hadoop is a series of tools for building large and scalable distributed systems. Back in 2010 Facebook already had &lt;a class="link" href="http://hadoopblog.blogspot.no/2010/05/facebook-has-worlds-largest-hadoop.html" target="_blank" rel="noopener"
>2000 machines in a Hadoop cluster&lt;/a> with 21PB (that is 21.000.000 GB) of combined storage.&lt;/p>
&lt;p>The second is an interesting newcommer, &lt;a class="link" href="http://influxdb.com/" target="_blank" rel="noopener"
>&lt;strong>InfluxDB&lt;/strong>&lt;/a>, that began development in 2013 and has the goal of offering scalability and performance without the requirements of HBase/Hadoop.&lt;/p>
&lt;p>In addition to advances in performance these alternatives also decouple storage of metrics and display of graphs and abstract the interaction in simple and well-defined APIs. This makes it easy for developers to create improved frontends rapidly and this has already resulted in several very attractive open-source frontends such as &lt;strong>&lt;a class="link" href="https://github.com/Ticketmaster/Metrilyx-2.0" target="_blank" rel="noopener"
>Metrilyx&lt;/a>&lt;/strong> (OpenTSDB), &lt;strong>&lt;a class="link" href="http://grafana.org/" target="_blank" rel="noopener"
>Grafana&lt;/a>&lt;/strong> (InfluxDB, Graphite, &lt;a class="link" href="https://github.com/grafana/grafana/pull/211" target="_blank" rel="noopener"
>soon OpenTSDB&lt;/a>), &lt;strong>&lt;a class="link" href="http://www.statuswolf.com/" target="_blank" rel="noopener"
>StatusWolf&lt;/a>&lt;/strong> (OpenTSDB), &lt;strong>&lt;a class="link" href="https://github.com/hakobera/influga" target="_blank" rel="noopener"
>Influga&lt;/a>&lt;/strong> (InfluxDB).&lt;/p>
&lt;p>&lt;a id="Debian">&lt;/a>&lt;/p>
&lt;h2 id="setting-up-a-single-node-opentsdb-instance-on-debian-7-wheezy">Setting up a single node OpenTSDB instance on Debian 7 Wheezy
&lt;/h2>&lt;p>In the rest of this paper we will set up a single node OpenTSDB instance. OpenTSDB builds on top of HBase and Hadoop and scales to very large setups easily. But it also delivers substantial performance on a single node which is deployed in &lt;strong>less than an hour&lt;/strong>. There are plenty of guides on installing a Hadoop cluster but here we will focus on the natural first step of getting a single node running using &lt;strong>recent releases&lt;/strong> of the relevant software:&lt;/p>
&lt;ul>
&lt;li>OpenTSDB 2.0.0 - Released 2014-05-05&lt;/li>
&lt;li>HBase 0.98.2 - Released 2014-05-01&lt;/li>
&lt;li>Hadoop 2.4.0 - Released 2014-04-07&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>If you later require to deploy a larger cluster consider using a framework such as &lt;a class="link" href="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html" target="_blank" rel="noopener"
>&lt;strong>Cloudera CDH&lt;/strong>&lt;/a> or &lt;a class="link" href="http://hortonworks.com/hdp/" target="_blank" rel="noopener"
>&lt;strong>Hortonworks HDP&lt;/strong>&lt;/a> which are open-source platforms which package Apache Hadoop components and provides a fully tested environment and easy-to-use graphical frontends for configuration and management. It is &lt;a class="link" href="http://opentsdb.net/setup-hbase.html" target="_blank" rel="noopener"
>recommended to have at least 5 machines&lt;/a> in a HBase cluster supporting OpenTSDB.&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;blockquote>
&lt;p>This guide assumes you are somewhat familiar with using a Linux shell/command prompt.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Hardware">&lt;/a>&lt;/p>
&lt;h4 id="hardware-requirements">Hardware requirements
&lt;/h4>&lt;ul>
&lt;li>CPU cores: Max (Limit to 50% of your available CPU resources)&lt;/li>
&lt;li>RAM: Min 16 GB&lt;/li>
&lt;li>Disk 1 - OS: 10 GB - Thin provisioned&lt;/li>
&lt;li>Disk 2 - Data: 100 GB - Thin provisioned&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="Operating">&lt;/a>&lt;/p>
&lt;h4 id="operating-system-requirements">Operating system requirements
&lt;/h4>&lt;p>This guide is based on a recently installed Debian 7 Wheezy &lt;strong>64bit&lt;/strong> installed without any extra packages. See the &lt;a class="link" href="https://www.debian.org/releases/stable/amd64/" target="_blank" rel="noopener"
>official documentation&lt;/a> for more information.&lt;/p>
&lt;p>All commands are entered as &lt;strong>root&lt;/strong> user unless otherwise noted.&lt;/p>
&lt;p>&lt;a id="preparations">&lt;/a>&lt;/p>
&lt;h4 id="pre-setup-preparations">Pre-setup preparations
&lt;/h4>&lt;p>We start by installing a few tools that we will need later.&lt;/p>
&lt;pre>&lt;code>apt-get install wget make gcc g++ cmake maven
&lt;/code>&lt;/pre>
&lt;p>Create a new ext3 partition on the data disk &lt;strong>/dev/sdb&lt;/strong>:&lt;/p>
&lt;pre>&lt;code>(echo &amp;quot;n&amp;quot;; echo &amp;quot;p&amp;quot;; echo &amp;quot;&amp;quot;; echo &amp;quot;&amp;quot;; echo &amp;quot;&amp;quot;; echo &amp;quot;t&amp;quot;; echo &amp;quot;83&amp;quot;; echo &amp;quot;w&amp;quot;) | fdisk /dev/sdb
mkfs.ext3 /dev/sdb1
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>ext3 is the &lt;a class="link" href="https://wiki.apache.org/hadoop/DiskSetup" target="_blank" rel="noopener"
>recommended filesystem for Hadoop&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>Create a mountpoint &lt;strong>/mnt/data1&lt;/strong> and add it to the file system table and mount the disk:&lt;/p>
&lt;pre>&lt;code>mkdir /mnt/data1
echo &amp;quot;/dev/sdb1 /mnt/data1 ext3 auto,noexec,noatime,nodiratime 0 1&amp;quot; | tee -a /etc/fstab
mount /mnt/data1
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>Using &lt;strong>noexec&lt;/strong> for the data partition will increase security as nothing on the data partition will be allowed to ever execute.
&lt;br />
Using &lt;strong>noatime&lt;/strong> and &lt;strong>nodiratime&lt;/strong> increases performance since the read access timestamps are not updated on every file access.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="java">&lt;/a>&lt;/p>
&lt;h4 id="installing-java-from-packages">Installing java from packages
&lt;/h4>&lt;p>Installing java on Linux can be quite challenging due to licensing issues, but thanks to the guys over at &lt;a class="link" href="https://launchpad.net/" target="_blank" rel="noopener"
>Launchpad.net&lt;/a> who are providing a repository with a custom java package this can now be done quite easy.&lt;/p>
&lt;p>We start by adding the launchpad java repository to our &lt;em>&lt;strong>/etc/apt/sources.list&lt;/strong>&lt;/em> file:&lt;/p>
&lt;pre>&lt;code>echo &amp;quot;deb http://ppa.launchpad.net/webupd8team/java/ubuntu precise main&amp;quot; | tee -a /etc/apt/sources.list
echo &amp;quot;deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu precise main&amp;quot; | tee -a /etc/apt/sources.list
&lt;/code>&lt;/pre>
&lt;p>Add the signing key and download information from the new repository:&lt;/p>
&lt;pre>&lt;code>apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys EEA14886
apt-get update
&lt;/code>&lt;/pre>
&lt;p>Run the java installer:&lt;/p>
&lt;pre>&lt;code>apt-get install oracle-java7-installer
&lt;/code>&lt;/pre>
&lt;p>Follow the instructions on screen to complete the Java 7 installation.&lt;/p>
&lt;p>&lt;a id="HBase">&lt;/a>&lt;/p>
&lt;h3 id="installing-hbase">Installing HBase
&lt;/h3>&lt;p>OpenTSDB has its own HBase installation tutorial &lt;a class="link" href="http://opentsdb.net/setup-hbase.html" target="_blank" rel="noopener"
>here&lt;/a>. It is very brief and does not use the latest versions or snappy compression.&lt;/p>
&lt;p>Download and unpack HBase:&lt;/p>
&lt;pre>&lt;code>cd /opt
wget http://apache.vianett.no/hbase/hbase-0.98.2/hbase-0.98.2-hadoop2-bin.tar.gz
tar xvfz hbase-0.98.2-hadoop2-bin.tar.gz
export HBASEDIR=`pwd`/hbase-0.98.2-hadoop2/
&lt;/code>&lt;/pre>
&lt;p>Increase the system-wide limitations of open files and processes from the default of 1000 to 32000 by adding a few lines to &lt;em>&lt;strong>/etc/security/limits.conf&lt;/strong>&lt;/em>:&lt;/p>
&lt;pre>&lt;code>echo &amp;quot;root - nofile 32768&amp;quot; | tee -a /etc/security/limits.conf
echo &amp;quot;root soft/hard nproc 32000&amp;quot; | tee -a /etc/security/limits.conf
echo &amp;quot;* - nofile 32768&amp;quot; | tee -a /etc/security/limits.conf
echo &amp;quot;* soft/hard nproc 32000&amp;quot; | tee -a /etc/security/limits.conf
&lt;/code>&lt;/pre>
&lt;p>The settings above will only take effect if we also add a line to &lt;em>&lt;strong>/etc/pam.d/common-session&lt;/strong>&lt;/em>:&lt;/p>
&lt;pre>&lt;code>echo &amp;quot;session required pam_limits.so&amp;quot; | tee -a /etc/pam.d/common-session
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="snappy">&lt;/a>&lt;/p>
&lt;h4 id="install-snappy">Install snappy
&lt;/h4>&lt;p>&lt;a class="link" href="https://code.google.com/p/snappy/" target="_blank" rel="noopener"
>Snappy&lt;/a> is a compression algorithm that values speed over compression ratio and this makes it a good choice for high throughput applications such as Hadoop/HBase. Due to licensing issues Snappy does not ship with HBase and need to be installed on top.&lt;/p>
&lt;p>The installation process is a bit complicated and has caused headache for many people (me included). Here we will show a method of installing snappy and getting it to work with the latest version of HBase and Hadoop.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Compression algorithms in HBase&lt;/strong>
Compression is the method of reducing the size of a file or text without losing any of the contents. There are many compression algorithms available and some focus on being able to create the smallest compressed file at the cost of time and CPU usage while other achieve &lt;em>reasonable&lt;/em> compression ratio while being very fast.
&lt;br /> &lt;br />
Out of the box HBase supports gz(gzip/zlib), snappy and lzo. Only gz is included due to licensing issues.
Unfortunately gz is a slow and costly algorithm compared to snappy and lzo. In a test performed by Yahoo (see &lt;a class="link" href="http://www.slideshare.net/Hadoop_Summit/singh-kamat-june27425pmroom210c" target="_blank" rel="noopener"
>slides here&lt;/a>, page 8) gz achieves 64% compression in 32 seconds. lzo 47% in 4.8 seconds and snappy 42% in 4.0 seconds. lz4 is another protocol &lt;a class="link" href="http://search-hadoop.com/m/KFLWV1PFVhp1" target="_blank" rel="noopener"
>considered for inclusion&lt;/a> that is even faster (2.4 seconds) but requires much more memory.
&lt;br /> &lt;br />
&lt;em>For more information look at the &lt;a class="link" href="https://hbase.apache.org/book/compression.html" target="_blank" rel="noopener"
>Apache HBase Handbook - Appendix C - Compression&lt;/a>&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="native">&lt;/a>&lt;/p>
&lt;h4 id="building-native-libhadoop-and-libsnappy">Building native libhadoop and libsnappy
&lt;/h4>&lt;p>In order to use compression we need the common Hadoop library, libhadoop.so, and the snappy library, libsnappy.so. HBase ships without libhadoop.so and the libhadoop.so that ships in the Hadoop Package is only for 32 bit OS. So we need to compile these files ourself.&lt;/p>
&lt;p>Start by downloading and installing ProtoBuf. Hadoop requres version 2.5+ which is not available as a Debian package unfortunately.&lt;/p>
&lt;pre>&lt;code>wget --no-check-certificate https://protobuf.googlecode.com/files/protobuf-2.5.0.tar.gz
tar zxvf protobuf-2.5.0.tar.gz
cd protobuf-2.5.0
./configure; make; make install
export LD_LIBRARY_PATH=/usr/local/lib/
&lt;/code>&lt;/pre>
&lt;p>Download and compile Hadoop:&lt;/p>
&lt;pre>&lt;code>apt-get install zlib1g-dev
wget http://apache.uib.no/hadoop/common/hadoop-2.4.0/hadoop-2.4.0-src.tar.gz
tar zxvf hadoop-2.4.0-src.tar.gz
cd hadoop-2.4.0-src/hadoop-common-project/
mvn package -Pdist,native -Dskiptests -Dtar -Drequire.snappy -DskipTests
&lt;/code>&lt;/pre>
&lt;p>Copy the newly compiled native libhadoop library into /usr/local/lib, then create the folder in which HBase looks for it and create a shortcut from there to /usr/local/lib/libhadoop.so:&lt;/p>
&lt;pre>&lt;code>cp hadoop-common/target/native/target/usr/local/lib/libhadoop.* /usr/local/lib
mkdir -p $HBASEDIR/lib/native/Linux-amd64-64/
cd $HBASEDIR/lib/native/Linux-amd64-64/
ln -s /usr/local/lib/libhadoop.so* .
&lt;/code>&lt;/pre>
&lt;p>Install snappy from Debian packages:&lt;/p>
&lt;pre>&lt;code>apt-get install libsnappy-dev
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="ConfiguringHBase">&lt;/a>&lt;/p>
&lt;h4 id="configuring-hbase">Configuring HBase
&lt;/h4>&lt;p>Now we need to do some basic configuration before we can start HBase. The configuration files are in $HBASEDIR/conf/.&lt;/p>
&lt;p>&lt;a id="hbase-env.sh">&lt;/a>&lt;/p>
&lt;h4 id="confhbase-envsh">&lt;strong>conf/hbase-env.sh&lt;/strong>
&lt;/h4>&lt;p>A shell script setting various environment variables related to how HBase and Java should behave. The file contains a lot of options and they are all documented by comments so feel free to look around in it.&lt;/p>
&lt;p>Start by setting the JAVA_HOME, which points to where Java is installed:&lt;/p>
&lt;pre>&lt;code>export JAVA_HOME=/usr/lib/jvm/java-7-oracle/
&lt;/code>&lt;/pre>
&lt;p>Then increase the size of the &lt;a class="link" href="http://pubs.vmware.com/vfabric52/index.jsp?topic=/com.vmware.vfabric.em4j.1.2/em4j/conf-heap-management.html" target="_blank" rel="noopener"
>Java Heap&lt;/a> from the default of 1000 which is a bit low:&lt;/p>
&lt;pre>&lt;code>export HBASE_HEAPSIZE=8000
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="Background">&lt;/a>&lt;/p>
&lt;h4 id="confhbase-sitexml">&lt;strong>conf/hbase-site.xml&lt;/strong>
&lt;/h4>&lt;p>An XML file containing HBase specific configuration parameters.&lt;/p>
&lt;pre>&lt;code>&amp;lt;configuration&amp;gt;
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;/mnt/data1/hbase&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;hbase.zookeeper.property.dataDir&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;/mnt/data1/zookeeper&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="compression">&lt;/a>&lt;/p>
&lt;h4 id="testing-hbase-and-compression">Testing HBase and compression
&lt;/h4>&lt;p>Now that we have installed snappy and configured HBase we can verify that HBase is working and that the compression is loaded by doing:&lt;/p>
&lt;pre>&lt;code>$HBASEDIR/bin/hbase org.apache.hadoop.hbase.util.CompressionTest /tmp/test.txt snappy
&lt;/code>&lt;/pre>
&lt;p>This should output some lines with information and end with &lt;strong>SUCCESS&lt;/strong>.&lt;/p>
&lt;p>&lt;a id="StartingHBase">&lt;/a>&lt;/p>
&lt;h4 id="starting-hbase">Starting HBase
&lt;/h4>&lt;p>HBase ships with scripts for starting and stopping it, namely start-hbase.sh and stop-hbase.sh. You start HBase with&lt;/p>
&lt;pre>&lt;code>$HBASEDIR/bin/start-hbase.sh
&lt;/code>&lt;/pre>
&lt;p>Then look at the log to ensure it has started without any serious errors:&lt;/p>
&lt;pre>&lt;code>tail -fn100 $HBASEDIR/bin/../logs/hbase-root-master-opentsdb.log
&lt;/code>&lt;/pre>
&lt;p>If you want HBase to start automatically on boot you can use a process management tool such as &lt;a class="link" href="http://mmonit.com/monit/" target="_blank" rel="noopener"
>Monit&lt;/a> or simply put it in &lt;em>&lt;strong>/etc/rc.local&lt;/strong>&lt;/em>:&lt;/p>
&lt;pre>&lt;code>/opt/hbase-0.98.2-hadoop2/bin/start-hbase.sh
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="InstallingOpenTSDB">&lt;/a>&lt;/p>
&lt;h3 id="installing-opentsdb">Installing OpenTSDB
&lt;/h3>&lt;p>Start by installing gnuplot, which is used by the native webui to draw graphs:&lt;/p>
&lt;pre>&lt;code>apt-get install gnuplot
&lt;/code>&lt;/pre>
&lt;p>Then download and install OpenTSDB:&lt;/p>
&lt;pre>&lt;code>wget https://github.com/OpenTSDB/opentsdb/releases/download/v2.0.0/opentsdb-2.0.0_all.deb
dpkg -i opentsdb-2.0.0_all.deb
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="ConfiguringOpenTSDB">&lt;/a>&lt;/p>
&lt;h4 id="configuring-opentsdb">Configuring OpenTSDB
&lt;/h4>&lt;p>The configuration file is &lt;em>&lt;strong>/etc/opentsdb/opentsdb.conf&lt;/strong>&lt;/em>. It has some of the basic configuration parameters but not nearly all of them. &lt;a class="link" href="http://opentsdb.net/docs/build/html/user_guide/configuration.html" target="_blank" rel="noopener"
>Here is the official documentation with all configuration parameters&lt;/a>.&lt;/p>
&lt;p>The defaults are reasonable but we need to make a few tweaks, the first is to add this:&lt;/p>
&lt;pre>&lt;code>tsd.core.auto_create_metrics = true
&lt;/code>&lt;/pre>
&lt;p>This will make OpenTSDB accept previously unseen metrics and add them to the database. This is very useful in the beginning when feeding data into OpenTSDB. Without this you will have to use the command &lt;em>&lt;strong>mkmetric&lt;/strong>&lt;/em> for each metric you will store and get errors that might be hard to trace if the metric you create do not match what is actually sent.&lt;/p>
&lt;p>Then we will add support for chunked requests via the HTTP API:&lt;/p>
&lt;pre>&lt;code>tsd.http.request.enable_chunked = true
tsd.http.request.max_chunk = 16000
&lt;/code>&lt;/pre>
&lt;p>Some tools and plugins (such as our own &lt;a class="link" href="https://github.com/PeritusConsulting/collectd-opentsdb" target="_blank" rel="noopener"
>improved collectd to OpenTSDB plugin&lt;/a>) send multiple data points in a single HTTP request for increased efficiency and requires this setting to be enabled.&lt;/p>
&lt;p>&lt;a id="HBasetables">&lt;/a>&lt;/p>
&lt;h4 id="creating-hbase-tables">Creating HBase tables
&lt;/h4>&lt;p>Before we start OpenTSDB we need to create the necessary tables in HBase:&lt;/p>
&lt;pre>&lt;code>env COMPRESSION=SNAPPY HBASE_HOME=$HBASEDIR /usr/share/opentsdb/tools/create_table.sh
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="StartingOpenTSDB">&lt;/a>&lt;/p>
&lt;h4 id="starting-opentsdb">Starting OpenTSDB
&lt;/h4>&lt;p>Since version 2.0.0 OpenTSDB ships as a Debian package and includes SysV init scripts. To start OpenTSDB as a daemon running in the background we run:&lt;/p>
&lt;pre>&lt;code>service opentsdb start
&lt;/code>&lt;/pre>
&lt;p>And then check the logs for any errors or other relevant information:&lt;/p>
&lt;pre>&lt;code>tail -f /var/log/opentsdb/opentsdb.log
&lt;/code>&lt;/pre>
&lt;p>If the server is started successfully the last line of the log should say:&lt;/p>
&lt;pre>&lt;code>13:42:30.900 INFO [TSDMain.main] - Ready to serve on /0.0.0.0:4242
&lt;/code>&lt;/pre>
&lt;p>And you can now browse to your new OpenTSDB in a browser using http://hostname:4242 !&lt;/p>
&lt;p>&lt;a id="Feeding">&lt;/a>&lt;/p>
&lt;h3 id="feeding-data-into-opentsdb">Feeding data into OpenTSDB
&lt;/h3>&lt;p>It is not within the scope of this paper to go into details about how to feed data into OpenTSDB but we will give a quick introduction here to get you started.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>A note on metric naming in OpenTSDB&lt;/strong>
&lt;br /> &lt;br />
Each datapoint has a metric name such as &lt;em>&lt;strong>df.bytes.free&lt;/strong>&lt;/em> and one or more tags such as &lt;em>&lt;strong>host=server1&lt;/strong>&lt;/em> and &lt;em>&lt;strong>mount=/mnt/data1&lt;/strong>&lt;/em>. This is closer to the proposed &lt;a class="link" href="http://metrics20.org/" target="_blank" rel="noopener"
>Metrics 2.0&lt;/a> standard for naming metrics than the traditional naming of &lt;em>&lt;strong>df.bytes.free.server1.mnt-data&lt;/strong>&lt;/em>. This makes it possible to create aggregates across tags and combine data easily using the tags.
&lt;br /> &lt;br />
OpenTSDB stores each datapoint with a given metric and tags in one HBase row per hour. But due to a HBase issue it still has to scan every row that matches the metric, ignoring the tags. Even though it will only return the data also matching the tags. This results in very much data being read and it will be very slow to read if there is a large number of data points for a given metric. The default for the collectd-opentsdb plugin is to use the read plugin name as metric, and other values as tags. In my case this results in 72.000.000 datapoints per hour for this metric. When generating a graph all of this data has to be read and evaluated before drawing a graph. 24 hours of data is over 1.7 billion datapoints for this single metric and results in a read performance of 5-15 &lt;strong>minutes&lt;/strong> for a simple graph.
&lt;br /> &lt;br />
A solution to this is to use &lt;em>shift-to-metric&lt;/em>, as &lt;a class="link" href="http://opentsdb.net/docs/build/html/user_guide/writing.html" target="_blank" rel="noopener"
>mentioned in the OpenTSDB user guide&lt;/a>. Shift-to-metric is simply moving one or more data identifiers from tags to the metric in order to reduce the cardinality (number of values) for a metric, and hence the time required to read out the data we want. We have modified the collectd-opentsdb java plugin in order to shift the tags to metrics, and this increases read-performance by ~1000x down to 10-100ms. Read the section about collectd below for more information on our modified plugin.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="tcollector">&lt;/a>&lt;/p>
&lt;h4 id="tcollector">tcollector
&lt;/h4>&lt;p>&lt;a class="link" href="http://opentsdb.net/docs/build/html/user_guide/utilities/tcollector.html" target="_blank" rel="noopener"
>tcollector&lt;/a> is the default agent for collecting and sending data from a Linux server to a OpenTSDB server. It is based on Python and plugins / addons can be written in any language. It ships with the most common plugins to collect information about disk usage and performance, cpu and memory statistics and also for some specific systems such as mysql, mongodb, riak, varnish, postgresql and others. tcollector is very lightweight and features advanced de-duplication in order to reduce unneeded network traffic.&lt;/p>
&lt;p>The commands for installing dependencies and downloading tcollector are&lt;/p>
&lt;pre>&lt;code>aptitude install git python
cd /opt
git clone git://github.com/OpenTSDB/tcollector.git
&lt;/code>&lt;/pre>
&lt;p>Configuration is in the startup script &lt;em>&lt;strong>tcollector/startstop&lt;/strong>&lt;/em>, you will need to uncomment and set the value of TSD_HOST to point to your OpenTSDB server.&lt;/p>
&lt;p>To start it run&lt;/p>
&lt;pre>&lt;code>/opt/tcollector/startstop start
&lt;/code>&lt;/pre>
&lt;p>This is also the command you want to add to &lt;em>&lt;strong>/etc/rc.local&lt;/strong>&lt;/em> in order to have the agent automatically start at boot. Logfiles are saved in &lt;em>&lt;strong>/var/log/tcollector.log&lt;/strong>&lt;/em> and they are rotated automatically.&lt;/p>
&lt;p>&lt;a id="peritus-tc-tools">&lt;/a>&lt;/p>
&lt;h4 id="peritus-tc-tools">peritus-tc-tools
&lt;/h4>&lt;p>We have developed a set of &lt;strong>tcollector&lt;/strong> plugins for collecting statistics from&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;a class="link" href="https://www.isc.org/downloads/dhcp/" target="_blank" rel="noopener"
>ISC DHCPd server&lt;/a>&lt;/strong>, about number of DHCP events and DHCP pool sizes&lt;/li>
&lt;li>&lt;strong>&lt;a class="link" href="http://www.opensips.org/" target="_blank" rel="noopener"
>OpenSIPS&lt;/a>&lt;/strong>, total number of subscribers and registered user agents&lt;/li>
&lt;li>&lt;strong>&lt;a class="link" href="http://atmail.com/" target="_blank" rel="noopener"
>Atmail&lt;/a>&lt;/strong>, number of users, admins, sent and received emails, logins and errors&lt;/li>
&lt;/ul>
&lt;p>As well as a high performance replacement for &lt;strong>&lt;a class="link" href="http://oss.oetiker.ch/smokeping/" target="_blank" rel="noopener"
>smokeping&lt;/a>&lt;/strong> called &lt;strong>tc-ping&lt;/strong>.&lt;/p>
&lt;p>These plugins are available for download from our &lt;strong>&lt;a class="link" href="https://github.com/PeritusConsulting/peritus-tc-tools" target="_blank" rel="noopener"
>GitHub page&lt;/a>&lt;/strong>.&lt;/p>
&lt;p>&lt;a id="collectd-opentsdb">&lt;/a>&lt;/p>
&lt;h4 id="collectd-opentsdb">collectd-opentsdb
&lt;/h4>&lt;p>&lt;a class="link" href="http://collectd.org/" target="_blank" rel="noopener"
>collectd&lt;/a> is the &lt;em>system statistics collection daemon&lt;/em> and is a widely used system for collecting metrics from various sources. There are several options for sending data from collectd to OpenTSDB but one way that works well is to use the &lt;a class="link" href="https://github.com/auxesis/collectd-opentsdb" target="_blank" rel="noopener"
>collectd-opentsdb java write plugin&lt;/a>.&lt;/p>
&lt;p>Since collectd is a generic metric collection tool the original collectd-opentsdb plugin will use the plugin name (such as &lt;strong>snmp&lt;/strong>) as the metric, and use tags such as &lt;strong>host=servername&lt;/strong>, &lt;strong>plugin_instance=ifHcInOctets&lt;/strong> and &lt;strong>type_instance=FastEthernet0/1&lt;/strong>.&lt;/p>
&lt;p>As mentioned in the &lt;em>&lt;strong>note on metric naming in OpenTSDB&lt;/strong>&lt;/em> this can be very inefficient when data needs to be read again resulting in read performance potentially thousands of times slower than optimal (&amp;lt;100ms). To alleviate this we have modified the original collectd-opentsdb plugin to store all metadata as part of the metric. This gives metric names such as ifHCInBroadcastPkts.sw01.GigabitEthernet0 and very good read performance.&lt;/p>
&lt;p>The modified collectd-opentsdb plugin can be downloaded from our &lt;a class="link" href="https://github.com/PeritusConsulting/collectd-opentsdb" target="_blank" rel="noopener"
>GitHub repository&lt;/a>.&lt;/p>
&lt;p>&lt;a id="MonitoringOpenTSDB">&lt;/a>&lt;/p>
&lt;h4 id="monitoring-opentsdb">Monitoring OpenTSDB
&lt;/h4>&lt;p>To monitor OpenTSDB itself install tcollector as described above on the OpenTSDB server and set &lt;em>&lt;strong>TSD_HOST&lt;/strong>&lt;/em> to &lt;em>&lt;strong>localhost&lt;/strong>&lt;/em> in &lt;em>&lt;strong>/opt/tcollector/startstop&lt;/strong>&lt;/em>.&lt;/p>
&lt;p>You can then go to http://opentsdb-server:4242/#start=1h-ago&amp;amp;end=1s-ago&amp;amp;m=sum:rate:tsd.rpc.received%7Btype=*%7D&amp;amp;o=&amp;amp;yrange=%5B0:%5D&amp;amp;wxh=1200x600 to view a graph of amount of data received in the last hour.&lt;/p>
&lt;p>&lt;a id="Performancecomparison">&lt;/a>&lt;/p>
&lt;h3 id="performance-comparison">Performance comparison
&lt;/h3>&lt;p>Lastly we include a little performance comparison between the latest version of OpenTSDB+HBase+Hadoop, a previous version of OpenTSDB+HBase+Hadoop that we have used for a while as well as rrdcached which ran in production for 4 years at a client.&lt;/p>
&lt;p>The workload is gathering and storing metrics from 150 Cisco switches with 8200 ports/interfaces every 5 seconds. This equals about 15.000 points per second.&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure1.png"
width="735"
height="338"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure1_hu13479656331014733159.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure1_hu12430936122100692572.png 1024w"
loading="lazy"
alt="Figure 1 - Data received by OpenTSDB per second"
class="gallery-image"
data-flex-grow="217"
data-flex-basis="521px"
>&lt;/p>
&lt;p>&lt;a id="Collection">&lt;/a>&lt;/p>
&lt;h4 id="collection">Collection
&lt;/h4>&lt;p>Even though it is not the primary focus, we include some data about collection performance for completeness. Collection is done using the latest version of &lt;a class="link" href="http://collectd.org/" target="_blank" rel="noopener"
>collectd&lt;/a> and the builtin SNMP plugin.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>NB #1:&lt;/strong> There is a &lt;a class="link" href="https://github.com/collectd/collectd/issues/610" target="_blank" rel="noopener"
>memory leak&lt;/a> in the way collectd&amp;rsquo;s SNMP plugin uses the underlying libsnmp library and you might need to schedule a restart of the collectd service as a workaround for that if handling large workloads.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>NB #2:&lt;/strong> Due to &lt;a class="link" href="http://comments.gmane.org/gmane.comp.monitoring.collectd/5061" target="_blank" rel="noopener"
>limitations in the libnetsnmp library&lt;/a> you will run into problems if polling many (1000+) devices with a single collectd instance. A workaround is to run multiple collectd instances with fewer hosts. &lt;a class="link" href="https://github.com/collectd/collectd/issues/610" target="_blank" rel="noopener"
>memory leak&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>Figure 2 shows that collection through SNMP polling consumes about 2200Mhz. We optimized some of the data types and definitions in collectd when moving to OpenTSDB and achieved a 20% performance increase in the polling as seen in Figure 3.&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure2.png"
width="785"
height="616"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure2_hu11520703687699822583.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure2_hu6507568741892228121.png 1024w"
loading="lazy"
alt="Figure 2 - CPU Usage - SNMP polling and writing to RRDcached"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="305px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure3.png"
width="787"
height="625"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure3_hu8758563371351857519.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure3_hu7312580424747238338.png 1024w"
loading="lazy"
alt="Figure 3 - CPU Usage - SNMP polling and sending to OpenTSDB"
class="gallery-image"
data-flex-grow="125"
data-flex-basis="302px"
>&lt;/p>
&lt;p>Writing to the native rrdcached write plugin consumes 1300Mhz while our modified collectd-opentsdb plugin consumes 1450Mhz. It is probably possible to create a much more efficient write plugin with more advanced knowledge of concurrency and using a lower level language such as C.&lt;/p>
&lt;p>&lt;a id="Storage">&lt;/a>&lt;/p>
&lt;h4 id="storage">Storage
&lt;/h4>&lt;p>When considering storage performance we will look at CPU usage and disk IOPS since these are the primary drivers of cost in today&amp;rsquo;s datacenters.&lt;/p>
&lt;h4 id="collectd--rrdcached">collectd + rrdcached
&lt;/h4>&lt;p>CPU usage - 1300Mhz, see Figure 2 above.&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure4.png"
width="661"
height="286"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure4_hu9422720109705725120.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure4_hu2897764835097661964.png 1024w"
loading="lazy"
alt="Figure 4 - Disk write IOPS - Fluctuating between 10 and 170 IOPS during the 1 hour flush period."
class="gallery-image"
data-flex-grow="231"
data-flex-basis="554px"
>&lt;/p>
&lt;h4 id="opentsdb--hbase-096--hadoop-1">OpenTSDB + Hbase 0.96 + Hadoop 1
&lt;/h4>&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure5.png"
width="791"
height="616"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure5_hu17014411734833306734.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure5_hu7917395545853379940.png 1024w"
loading="lazy"
alt="Figure 5 - CPU usage - 1700Mhz baseline with peaks of 7000Mhz during Java Garbage Collection (GC) (untuned)."
class="gallery-image"
data-flex-grow="128"
data-flex-basis="308px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure6.png"
width="800"
height="285"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure6_hu14624923996941413689.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure6_hu10993571528499142312.png 1024w"
loading="lazy"
alt="Figure 6 - Disk write IOPS - 5 IOPS average with peaks of 25 IOPS during Java GC. We also see that disk read IOPS are much higher and this is due to regular compaction of the database and can be tuned. Reads in general can be reduced by increasing caching with more RAM if necessary."
class="gallery-image"
data-flex-grow="280"
data-flex-basis="673px"
>&lt;/p>
&lt;h4 id="opentsdb--hbase-098--hadoop-2">OpenTSDB + HBase 0.98 + Hadoop 2
&lt;/h4>&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure7.png"
width="925"
height="478"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure7_hu8040901172863881962.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure7_hu1188736229991262180.png 1024w"
loading="lazy"
alt="Figure 7 - CPU usage - 1200Mhz baseline with peaks of 5000-6000Mhz during Java GC (untuned)."
class="gallery-image"
data-flex-grow="193"
data-flex-basis="464px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure8.png"
width="767"
height="395"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure8_hu10785440613668549146.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure8_hu12590607926021679589.png 1024w"
loading="lazy"
alt="Figure 8 - Disk write IOPS - &amp;lt; 5 IOPS average with peaks of 25 IOPS during Java GC. Much less read IOPS during compaction compared to HBase 0.96."
class="gallery-image"
data-flex-grow="194"
data-flex-basis="466px"
>&lt;/p>
&lt;p>&lt;a id="Conclusion">&lt;/a>&lt;/p>
&lt;h4 id="conclusion">Conclusion
&lt;/h4>&lt;p>Even without tuning, a single instance OpenTSDB installation is able to handle significant amounts of data before running into IO problems. This comes at a cost of CPU, currently OpenTSDB will consume &amp;gt; 300% the amount of CPU cycles compared to rrdcached for storage. But this is offset by a 85-95% reduction in disk load. In absolute terms for our particular set up (one 2 year old HP DL360p Gen8 running VMware vSphere 5.5) CPU usage increased from 15% to 25% while reducing IOPS load from 70% to &amp;lt; 10%.&lt;/p>
&lt;p>&lt;em>Fine tuning of parameters (such as Java GC) as well as detailed analysis of memory usage is outside the scope of this brief paper and detailed information may be found elsewhere (&lt;a class="link" href="https://hbase.apache.org/book/performance.html" target="_blank" rel="noopener"
>51&lt;/a>,&lt;a class="link" href="http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html" target="_blank" rel="noopener"
>52&lt;/a>,&lt;a class="link" href="http://www.cubrid.org/blog/textyle/428187" target="_blank" rel="noopener"
>53&lt;/a>) for those interested.&lt;/em>&lt;/p>
&lt;hr></description></item></channel></rss>