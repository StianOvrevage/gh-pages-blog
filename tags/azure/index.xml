<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Azure on blog.stian.omg.lol</title><link>https://blog.stian.omg.lol/tags/azure/</link><description>Recent content in Azure on blog.stian.omg.lol</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 04 Jun 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.stian.omg.lol/tags/azure/index.xml" rel="self" type="application/rss+xml"/><item><title>Mini-post: Down-scaling Azure Kubernetes Service (AKS)</title><link>https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/</link><pubDate>Tue, 04 Jun 2019 00:00:00 +0000</pubDate><guid>https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/</guid><description>&lt;img src="https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/2019-06-04-downscaling-aks.png" alt="Featured image of post Mini-post: Down-scaling Azure Kubernetes Service (AKS)" />&lt;p>We discovered today that some implicit assumptions we had about AKS at smaller scales were incorrect.&lt;/p>
&lt;p>Suddenly new workloads and jobs in our Radix CI/CD could not start due to insufficient resources (CPU &amp;amp; memory).&lt;/p>
&lt;p>Even though it only caused problems in development environments with smaller node sizes it still surprised some of our developers, since we expected the size of development clusters to have enough resources.&lt;/p>
&lt;p>I thought it would be a good chance to go a bit deeper and verify some of our assumptions and also learn more about various components that usually &amp;ldquo;just works&amp;rdquo; and isn&amp;rsquo;t really given much thought.&lt;/p>
&lt;p>First I do a &lt;code>kubectl describe node &amp;lt;node&amp;gt;&lt;/code> on 2-3 of the nodes to get an idea of how things are looking:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">Resource Requests Limits
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-------- -------- ------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cpu 930m &lt;span class="o">(&lt;/span>98%&lt;span class="o">)&lt;/span> 5500m &lt;span class="o">(&lt;/span>585%&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">memory &lt;span class="m">1659939584&lt;/span> &lt;span class="o">(&lt;/span>89%&lt;span class="o">)&lt;/span> 4250M &lt;span class="o">(&lt;/span>228%&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>So we are obviously hitting the roof when it comes to resources. But why?&lt;/p>
&lt;h2 id="node-overhead">Node overhead
&lt;/h2>&lt;p>We use &lt;code>Standard DS1 v2&lt;/code> instances as AKS nodes and they have 1 CPU core and 3.5 GiB memory.&lt;/p>
&lt;p>The output of &lt;code>kubectl describe node&lt;/code> also gives us info on the Capacity (total node size) and Allocatable (resources available to run Pods).&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Capacity:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cpu: 1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> memory: 3500452Ki
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Allocatable:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cpu: 940m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> memory: 1814948Ki
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>So we have lost &lt;strong>60 millicores / 6%&lt;/strong> of CPU and &lt;strong>1685MiB / 48%&lt;/strong> of memory. The next question is if this increases linearly with node size (the percentage of resources lost is the same regardless of node size) or is fixed (always reserves 60 millicores and 1685Mi of memory), or a combination.&lt;/p>
&lt;p>I connect to another cluster that has double the node size (&lt;code>Standard DS2 v2&lt;/code>) and compare:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Capacity:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cpu: 2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> memory: 7113160Ki
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Allocatable:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cpu: 1931m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> memory: 4667848Ki
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>So for this the loss is &lt;strong>69 millicores / 3.5%&lt;/strong> of CPU and &lt;strong>2445MiB / 35%&lt;/strong> of memory.&lt;/p>
&lt;p>So CPU reservations are close to fixed regardless of node size while memory reservations are influenced by node size but luckily not linearly.&lt;/p>
&lt;p>What causes this &amp;ldquo;waste&amp;rdquo;? Reading up on &lt;a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/" target="_blank" rel="noopener"
>kubernetes.io&lt;/a> gives a few clues. Kubelet will reserve CPU and memory resources for itself and other Kubernetes processes. It will also reserve a portion of memory to act as a buffer whenever a Pod is going beyond it&amp;rsquo;s memory limits to avoid risking System OOM, potentially making the whole node unstable.&lt;/p>
&lt;p>To figure out what these are configured to we log in to an actual AKS node&amp;rsquo;s console and run &lt;code>ps ax|grep kube&lt;/code> and the output looks like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="o">/&lt;/span>&lt;span class="n">usr&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">local&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">bin&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubelet&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">enable&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">server&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">labels&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">role&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">agent&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">role&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">agent&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">agentpool&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">nodepool1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">storageprofile&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">managed&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">storagetier&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">Premium_LRS&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">azure&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">com&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">cluster&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">MC_clusters_weekly&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">22&lt;/span>&lt;span class="n">_northeurope&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">volume&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">plugin&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">dir&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">volumeplugins&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">address&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="mf">0.0&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">allow&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">privileged&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">true&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">anonymous&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">auth&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">false&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">authorization&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">mode&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">Webhook&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">azure&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">container&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">azure&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">json&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">cgroups&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">per&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">qos&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">true&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">client&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">ca&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">file&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">certs&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">ca&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">crt&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">cloud&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">azure&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">json&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">cloud&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">provider&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">azure&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">cluster&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">dns&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">10.2&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="mf">0.10&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">cluster&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">domain&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">cluster&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">enforce&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">allocatable&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">pods&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">event&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">qps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">eviction&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">hard&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">available&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="mi">750&lt;/span>&lt;span class="n">Mi&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">nodefs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">available&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="o">%&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">nodefs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inodesFree&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="o">%&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">feature&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">gates&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">PodPriority&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">true&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">RotateKubeletServerCertificate&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">true&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">gc&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">high&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">threshold&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">85&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">gc&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">low&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">threshold&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">80&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">pull&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">progress&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">deadline&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">30&lt;/span>&lt;span class="n">m&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">keep&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">terminated&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">pod&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">volumes&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">false&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">kube&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">reserved&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">cpu&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">60&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">memory&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">896&lt;/span>&lt;span class="n">Mi&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">kubeconfig&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="k">var&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">lib&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubelet&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubeconfig&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="nb">max&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">pods&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">110&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">network&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">plugin&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">cni&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">status&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">update&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">frequency&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="n">s&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">non&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">masquerade&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">cidr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">pod&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">infra&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">container&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">k8s&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gcr&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">pause&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">amd64&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mf">3.1&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">pod&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">manifest&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">path&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">manifests&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">pod&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="nb">max&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">pids&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">rotate&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">certificates&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">false&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">streaming&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">connection&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">idle&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">timeout&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="n">m&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>To log in to the console of a node, go to the MC_resourcegroup_clustername_region resource-group and select the VM. Then go to &lt;code>Boot diagnostics&lt;/code> and enable it. Go to &lt;code>Reset password&lt;/code> to create yourself a user and then &lt;code>Serial console&lt;/code> to log in and execute commands.&lt;/p>
&lt;/blockquote>
&lt;p>We can see &lt;code>--kube-reserved=cpu=60m,memory=896Mi&lt;/code> and &lt;code>--eviction-hard=memory.available&amp;lt;750Mi&lt;/code> which adds up to &lt;code>1646Mi&lt;/code> which is pretty close to the &lt;code>1685Mi&lt;/code> that was the gap between Capacity and Allocatable.&lt;/p>
&lt;p>We also do this on a &lt;code>Standard DS2 v2&lt;/code> node and get &lt;code>--kube-reserved=cpu=69m,memory=1638Mi&lt;/code> and &lt;code>--eviction-hard=memory.available&amp;lt;750Mi&lt;/code>.&lt;/p>
&lt;p>So we can see that the memory of &lt;code>kube-reserved&lt;/code> grows almost linearly and seems to always be about 20-25% while CPU reservations are almost the same. The memory eviction buffer is always fixed at &lt;code>750Mi&lt;/code> which would mean bigger resource waste as nodes decrease in size.&lt;/p>
&lt;h4 id="cpu">CPU
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: right">Standard DS1 v2&lt;/th>
&lt;th style="text-align: right">Standard DS2 v2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>VM capacity&lt;/td>
&lt;td style="text-align: right">1.000m&lt;/td>
&lt;td style="text-align: right">2.000m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-reserved&lt;/td>
&lt;td style="text-align: right">-60m&lt;/td>
&lt;td style="text-align: right">-69m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Allocatable&lt;/td>
&lt;td style="text-align: right">940m&lt;/td>
&lt;td style="text-align: right">1.931m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Allocatable %&lt;/td>
&lt;td style="text-align: right">94%&lt;/td>
&lt;td style="text-align: right">96.5%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="memory">Memory
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: right">Standard DS1 v2&lt;/th>
&lt;th style="text-align: right">Standard DS2 v2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>VM capacity&lt;/td>
&lt;td style="text-align: right">3.500Mi&lt;/td>
&lt;td style="text-align: right">7.113Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-reserved&lt;/td>
&lt;td style="text-align: right">-896Mi&lt;/td>
&lt;td style="text-align: right">-1.638Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Eviction buf&lt;/td>
&lt;td style="text-align: right">-750Mi&lt;/td>
&lt;td style="text-align: right">-750Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Allocatable&lt;/td>
&lt;td style="text-align: right">1.814Mi&lt;/td>
&lt;td style="text-align: right">4.667Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Allocatable %&lt;/td>
&lt;td style="text-align: right">52%&lt;/td>
&lt;td style="text-align: right">65%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="node-pods-daemonsets">Node pods (DaemonSets)
&lt;/h2>&lt;p>We have some Pods that run on every node, and they are installed by default by AKS. We get the resource limits of these by describing either the pods or the daemonsets.&lt;/p>
&lt;h4 id="cpu-1">CPU
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: right">Standard DS1 v2&lt;/th>
&lt;th style="text-align: right">Standard DS2 v2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Allocatable&lt;/td>
&lt;td style="text-align: right">940m&lt;/td>
&lt;td style="text-align: right">1.931m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/calico-node&lt;/td>
&lt;td style="text-align: right">-250m&lt;/td>
&lt;td style="text-align: right">-250m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/kube-proxy&lt;/td>
&lt;td style="text-align: right">-100m&lt;/td>
&lt;td style="text-align: right">-100m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/kube-svc-redirect&lt;/td>
&lt;td style="text-align: right">-5m&lt;/td>
&lt;td style="text-align: right">-5m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available&lt;/td>
&lt;td style="text-align: right">585m&lt;/td>
&lt;td style="text-align: right">1.576m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available %&lt;/td>
&lt;td style="text-align: right">58%&lt;/td>
&lt;td style="text-align: right">81%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="memory-1">Memory
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: right">Standard DS1 v2&lt;/th>
&lt;th style="text-align: right">Standard DS2 v2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Allocatable&lt;/td>
&lt;td style="text-align: right">1.814Mi&lt;/td>
&lt;td style="text-align: right">4.667Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/kube-svc-redirect&lt;/td>
&lt;td style="text-align: right">-32Mi&lt;/td>
&lt;td style="text-align: right">-32Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available&lt;/td>
&lt;td style="text-align: right">1.782Mi&lt;/td>
&lt;td style="text-align: right">4.635Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available %&lt;/td>
&lt;td style="text-align: right">50%&lt;/td>
&lt;td style="text-align: right">61%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>So for &lt;code>Standard DS1 v2&lt;/code> nodes we have about 0.5 CPU and 1.7GiB memory per node for pods. And for &lt;code>Standard DS2 v2&lt;/code> nodes it&amp;rsquo;s about 1.5 CPU and 4.6GiB memory.&lt;/p>
&lt;h2 id="kube-system-pods">kube-system pods
&lt;/h2>&lt;p>Now lets add some standard Kubernetes pods we need to run. As far as I know these are pretty much fixed for a cluster and not related to node size or count.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Deployment&lt;/th>
&lt;th style="text-align: right">CPU&lt;/th>
&lt;th style="text-align: right">Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>kube-system/kubernetes-dashboard&lt;/td>
&lt;td style="text-align: right">100m&lt;/td>
&lt;td style="text-align: right">50Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/tunnelfront&lt;/td>
&lt;td style="text-align: right">10m&lt;/td>
&lt;td style="text-align: right">64Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/coredns (x2)&lt;/td>
&lt;td style="text-align: right">200m&lt;/td>
&lt;td style="text-align: right">140Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/coredns-autoscaler&lt;/td>
&lt;td style="text-align: right">20m&lt;/td>
&lt;td style="text-align: right">10Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/heapster&lt;/td>
&lt;td style="text-align: right">130m&lt;/td>
&lt;td style="text-align: right">230Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sum&lt;/td>
&lt;td style="text-align: right">460m&lt;/td>
&lt;td style="text-align: right">494Mi&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="third-party-pods">Third party pods
&lt;/h2>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Deployment&lt;/th>
&lt;th style="text-align: right">CPU&lt;/th>
&lt;th style="text-align: right">Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>grafana&lt;/td>
&lt;td style="text-align: right">200m&lt;/td>
&lt;td style="text-align: right">500Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>prometheus-operator&lt;/td>
&lt;td style="text-align: right">500m&lt;/td>
&lt;td style="text-align: right">1.000Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>prometheus-alertmanager&lt;/td>
&lt;td style="text-align: right">100m&lt;/td>
&lt;td style="text-align: right">225Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>flux&lt;/td>
&lt;td style="text-align: right">50m&lt;/td>
&lt;td style="text-align: right">64Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>flux-helm-operator&lt;/td>
&lt;td style="text-align: right">50m&lt;/td>
&lt;td style="text-align: right">64Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sum&lt;/td>
&lt;td style="text-align: right">900m&lt;/td>
&lt;td style="text-align: right">1.853Mi&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="radix-platform-pods">Radix platform pods
&lt;/h2>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Deployment&lt;/th>
&lt;th style="text-align: right">CPU&lt;/th>
&lt;th style="text-align: right">Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>radix-api-prod/server (x2)&lt;/td>
&lt;td style="text-align: right">200m&lt;/td>
&lt;td style="text-align: right">400Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-api-qa/server (x2)&lt;/td>
&lt;td style="text-align: right">100m&lt;/td>
&lt;td style="text-align: right">200Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-canary-golang-dev/www&lt;/td>
&lt;td style="text-align: right">40m&lt;/td>
&lt;td style="text-align: right">500Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-canary-golang-prod/www&lt;/td>
&lt;td style="text-align: right">40m&lt;/td>
&lt;td style="text-align: right">500Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-platform-prod/public-site&lt;/td>
&lt;td style="text-align: right">5m&lt;/td>
&lt;td style="text-align: right">10Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-web-console-prod/web&lt;/td>
&lt;td style="text-align: right">10m&lt;/td>
&lt;td style="text-align: right">42Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-web-console-qa/web&lt;/td>
&lt;td style="text-align: right">5m&lt;/td>
&lt;td style="text-align: right">21Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-github-webhook-prod/webhook&lt;/td>
&lt;td style="text-align: right">10m&lt;/td>
&lt;td style="text-align: right">30Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-github-webhook-prod/webhook&lt;/td>
&lt;td style="text-align: right">5m&lt;/td>
&lt;td style="text-align: right">15Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sum&lt;/td>
&lt;td style="text-align: right">415m&lt;/td>
&lt;td style="text-align: right">1.718Mi&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>If we add up the resource usage of these groups of workloads and see the total available resources on our 4 node Standard DS1 v2 clusters we are left with 0.56 CPU cores (14%) and 3GB of memory (22%):&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Workload&lt;/th>
&lt;th style="text-align: right">CPU&lt;/th>
&lt;th style="text-align: right">Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>kube-system&lt;/td>
&lt;td style="text-align: right">460m&lt;/td>
&lt;td style="text-align: right">494Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>third-party&lt;/td>
&lt;td style="text-align: right">900m&lt;/td>
&lt;td style="text-align: right">1.853Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-platform&lt;/td>
&lt;td style="text-align: right">415m&lt;/td>
&lt;td style="text-align: right">1.718Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sum&lt;/td>
&lt;td style="text-align: right">1.760m&lt;/td>
&lt;td style="text-align: right">4.020Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available on 4x DS1&lt;/td>
&lt;td style="text-align: right">2.340m&lt;/td>
&lt;td style="text-align: right">7.128Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available for workloads&lt;/td>
&lt;td style="text-align: right">565m&lt;/td>
&lt;td style="text-align: right">3.063Mi&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Though surprising that we lost this much resources before being able to deploy our actual customer applications, it should still be a bit of headroom.&lt;/p>
&lt;p>Going further I checked the resource requests on 8 customer pods deployed in 4 environments (namespaces). Even though none of them had a resource configuration in their &lt;code>radixconfig.yaml&lt;/code> files they still had resource requests and limits. Not surprising since we use LimitRange to set default resource requests and limits. The surprise was that half of them had 50Mi of memory and the other half 500Mi, seemingly at random.&lt;/p>
&lt;p>It turns out that we did an update to the LimitRange values a few days ago but that only applies to new Pods, so depending on if the Pods got re-created for any reason they may or may not have the old request of 500Mi, which in our case of small clusters will quickly drain the available resources.&lt;/p>
&lt;blockquote>
&lt;p>Read more about LimitRange here: &lt;a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/" target="_blank" rel="noopener"
>kubernetes.io&lt;/a> , and here is the commit that eventually trickled down to reduce memory usage: &lt;a class="link" href="https://github.com/equinor/radix-operator/commit/f022fcde993efdf6cbcafb2c6632707a823a2a27" target="_blank" rel="noopener"
>github.com&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h2 id="pod-scheduling">Pod scheduling
&lt;/h2>&lt;p>Depending on the weight between CPU and memory requests and how often things get destroyed and re-created you may find yourself in a situation where you have enough resources in your cluster but new workloads are still Pending. This can happen when one resource type (e.g. CPU) is filled before another (e.g. memory), leading one or more resources to be stranded and unlikely to be utilized.&lt;/p>
&lt;p>Imagine for example a cluster that is already utilized like this:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>CPU&lt;/th>
&lt;th>Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>node0&lt;/td>
&lt;td>94%&lt;/td>
&lt;td>86%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>node1&lt;/td>
&lt;td>80%&lt;/td>
&lt;td>89%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>node2&lt;/td>
&lt;td>98%&lt;/td>
&lt;td>60%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Scheduling a workload that requests 15% CPU and 20% memory cannot be scheduled since there are no nodes fulfilling both requirements. In theory there is probably a CPU intensive Pod on node2 that could be moved to node1 but Kubernetes does not do re-scheduling to optimize utilization. It can do re-scheduling based on Pod priority (&lt;a class="link" href="https://medium.com/@dominik.tornow/the-kubernetes-scheduler-cd429abac02f" target="_blank" rel="noopener"
>medium.com&lt;/a>) and there is an incubator project (&lt;a class="link" href="https://akomljen.com/meet-a-kubernetes-descheduler/" target="_blank" rel="noopener"
>akomljen.com&lt;/a>) that can try to drain nodes with low utilization.&lt;/p>
&lt;p>So for the foreseable future keeping in mind that resources can get stranded and that looking at the sum of cluster resources and sum of cluster resource demand might be misleading.&lt;/p>
&lt;h2 id="calico-node">calico-node
&lt;/h2>&lt;p>The biggest source of waste on our small clusters is &lt;code>calico-node&lt;/code> which is installed on every node and requests 25% of a CPU core while only using 2.5-3% CPU:&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu.png"
width="1291"
height="392"
srcset="https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu_hu1477443119961749415.png 480w, https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu_hu8581044549333248792.png 1024w"
loading="lazy"
alt="calico-node cpu usage"
class="gallery-image"
data-flex-grow="329"
data-flex-basis="790px"
>&lt;/p>
&lt;p>The request is originally set here &lt;a class="link" href="https://github.com/Azure/aks-engine/blob/master/parts/k8s/containeraddons/kubernetesmasteraddons-calico-daemonset.yaml" target="_blank" rel="noopener"
>github.com&lt;/a> but I have not got into why that number was choosen. Next steps would be to do some benchmarking of &lt;code>calico-node&lt;/code> to smoke out it&amp;rsquo;s performance characteristics to see if it would be safe to lower the resource requests, but that is out of scope for now.&lt;/p>
&lt;h1 id="conclusion">Conclusion
&lt;/h1>&lt;ul>
&lt;li>By increasing node size from &lt;code>Standard DS1 v2&lt;/code> to &lt;code>Standard DS2 v2&lt;/code> we also increase the available CPU from 58% per node to 81% per node. Available memory increases from 50% to 61% per node.&lt;/li>
&lt;li>With a total platform requirement of 3-4GB of memory and 4.6GB available on &lt;code>Standard DS2 v2&lt;/code> we might have more resources for actual workloads on a 1-node &lt;code>Standard DS2 v2&lt;/code> cluster than a 3-node &lt;code>Standard DS1 v2&lt;/code> cluster!&lt;/li>
&lt;li>Beware of stranded resources limiting the utilization you can achieve across a cluster.&lt;/li>
&lt;/ul></description></item><item><title>Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking</title><link>https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/</link><pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate><guid>https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/</guid><description>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/2019-02-23-disk-performance-on-aks-part-1.png" alt="Featured image of post Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking" />&lt;p>Understanding the characteristics of disk performance of a platform might be more important than you think. If disk resources are not correctly matched to your workload, your performance will suffer and might lead you to incorrectly diagnose a problem as being related to CPU or memory.&lt;/p>
&lt;p>The defaults might also not give you the performance you expect.&lt;/p>
&lt;p>In this first post on troubleshooting some disk performance issues on Azure Kubernetes Service (AKS) we will benchmark Azure Premium SSD to find how workloads affect performance and which metrics to monitor to know when troubleshooting potential disk issues.&lt;/p>
&lt;p>TLDR:&lt;/p>
&lt;ul>
&lt;li>Disable Azure cache for workloads with high number of random writes&lt;/li>
&lt;li>Use a P15 (256GB) or larger Premium SSD even though you might only need a fraction of it.&lt;/li>
&lt;/ul>
&lt;p>Table of contents&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="#Background" >Background&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#MetricsMethodologies" >Metric Methodologies&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#StorageBackground" >Storage Background&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#WhatToMeasure" >What to measure?&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#HowToMeasureDisk" >How to measure disk&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="HowToMeasureDiskOnAKS" >How to measure disk on Azure Kubernetes Service&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Tests" >Test results&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Test1" >Test 1 - Learning to dislike Azure Cache&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test2" >Test 2 - Disable Azure Cache - enable OS cache&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test3" >Test 3 - Disable OS cache&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test4" >Test 4 - Increase IO depth&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test5" >Test 5 - Larger block size, smaller IO depth&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test6" >Test 6 - Enable OS cache&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test7" >Test 7 - Random writes, small block size&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test8" >Test 8 - Large block size&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Conclusion" >Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="microsoft-azure">Microsoft Azure
&lt;/h4>&lt;blockquote>
&lt;p>&lt;a class="link" href="https://azure.microsoft.com/en-us/free/" target="_blank" rel="noopener"
>If you don&amp;rsquo;t have a Azure subscription already you can try services for $200 for 30 days.&lt;/a> The VM size &lt;strong>Standard_B2s&lt;/strong> is Burstable, has 2vCPU, 4GB RAM, 8GB temp storage and costs roughly $38 / month. For $200 you can have a cluster of 3-4 B2s nodes plus traffic, loadbalancers and other additional costs.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>See my blog post &lt;a class="link" href="2017-12-23-managed-kubernetes-on-azure.md" >Managed Kubernetes on Microsoft Azure (English)&lt;/a> for information on how to get up and running with Kubernetes on Azure.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;em>I have no affiliation with Microsoft Azure except using them through work.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="corrections">Corrections
&lt;/h2>&lt;p>&lt;strong>February 2020&lt;/strong>: Some of my previous knowledge and assumptions were not correct when applied to a cloud + Docker environment, as &lt;a class="link" href="https://github.com/jnoller/kubernaughty/issues/46" target="_blank" rel="noopener"
>explained by
AKS PM Jesse Noller on GitHub&lt;/a>.&lt;/p>
&lt;p>One of the issues is that even accessing a &amp;ldquo;data disk&amp;rdquo; will incur IOPS on the OS disk, and throttling of the OS disk will also constraint IOPS on the data disks.&lt;/p>
&lt;p>&lt;a id="Background">&lt;/a>&lt;/p>
&lt;h2 id="background">Background
&lt;/h2>&lt;p>I&amp;rsquo;m part of a team at Equinor building an internal PaaS based on Kubernetes running on AKS (Azure managed Kubernetes). We use Prometheus for monitoring each cluster as well as InfluxDB for collecting metrics from k6io which runs continous tests on our public endpoints.&lt;/p>
&lt;p>A couple of weeks ago we discovered some potential problems with both Prometheus and InfluxDB with memory usage and restarts. High CPU usage of type &lt;code>iowait&lt;/code> suggested that there might be some disk issues contributing to the problems.&lt;/p>
&lt;blockquote>
&lt;p>iowait: &amp;ldquo;Percentage of time that the CPU or CPUs were idle during which the system had an outstanding disk I/O request.&amp;rdquo; (&lt;a class="link" href="https://support.hpe.com/hpsc/doc/public/display?docId=c02783994" target="_blank" rel="noopener"
>hpe.com&lt;/a>). You can see &lt;code>iowait&lt;/code> on your Linux system by running &lt;code>top&lt;/code> and looking at the &lt;code>wa&lt;/code> percentage.&lt;/p>
&lt;p>PS: You can have a disk IO bottleneck even with low &lt;code>iowait&lt;/code>, and a high &lt;code>iowait&lt;/code> does not always indicate a disk IO bottleneck (&lt;a class="link" href="https://www.ibm.com/developerworks/community/blogs/AIXDownUnder/entry/iowait_a_misleading_indicator_of_i_o_performance54?lang=en" target="_blank" rel="noopener"
>ibm.com&lt;/a>).&lt;/p>
&lt;/blockquote>
&lt;p>First off we need to benchmark the underlying disk to get an understanding of it&amp;rsquo;s performance limits and characteristics. That is what we will cover in this post.&lt;/p>
&lt;p>&lt;a id="MetricsMethodologies">&lt;/a>&lt;/p>
&lt;h3 id="metric-methodologies">Metric Methodologies
&lt;/h3>&lt;p>There are two helpful methodologies when monitoring information systems. The first one is Utilization, Saturation and Errors (USE) from &lt;a class="link" href="http://www.brendangregg.com/usemethod.html" target="_blank" rel="noopener"
>Brendan Gregg&lt;/a> and the second one is Rate, Errors, Duration (RED) from &lt;a class="link" href="https://www.slideshare.net/weaveworks/monitoring-microservices" target="_blank" rel="noopener"
>Tom Wilkie&lt;/a>. RED is best suited when observing workloads and transactions while USE is best suited for observing resources.&lt;/p>
&lt;p>I&amp;rsquo;ll be using the USE method here. USE can be summarised as:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>For every resource, check utilization, saturation, and errors.&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>resource&lt;/strong>: all physical server functional components (CPUs, disks, busses, &amp;hellip;)&lt;/li>
&lt;li>&lt;strong>utilization&lt;/strong>: the average time that the resource was busy servicing work&lt;/li>
&lt;li>&lt;strong>saturation&lt;/strong>: the degree to which the resource has extra work which it can&amp;rsquo;t service, often queued&lt;/li>
&lt;li>&lt;strong>errors&lt;/strong>: the count of error events&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="StorageBackground">&lt;/a>&lt;/p>
&lt;h3 id="storage-background">Storage Background
&lt;/h3>&lt;p>Disk usage has two dimensions, throughput/bandwidth(BW) and operations per second (IOPS), and the underlying storage system will have upper limits of how much data it can receive (BW) and the number of operations it can perform per second (IOPS).&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Background - harddrive types&lt;/strong>: harddrives come in two types, Solid State Disks (SSD) and spindle (HDD). A SSD disk is a microship capable of permanently storing data while a HDD uses spinning platters to store data. HDDs have a fixed rate of rotation (RPM), typically 5.400 and 7.200 RPM for lower cost drives for home use and higher cost 10.000 and 15.000 RPM drives for server use. Over the last 20 years of HDDs their storage density has increased, but the RPM has largely stayed the same. A disk with twice the density (500GB to 1TB for example) can read twice as much data on a single rotation and thus increase the bandwidth significantly. However, reading or writing a random block still requires waiting for the disk to spin enough to reach the relevant sector on the disk. So IOPS has not increased much for HDDs and is still a low 125-150 IOPS for a 10.000 RPM enterprise disk. A SSD does not have any moving parts so is able to reach MUCH higher IOPS. A low end Samsung 960 EVO with 500GB capacity costs $150 and can achieve a whopping 330.000 IOPS! (&lt;a class="link" href="https://en.wikipedia.org/wiki/IOPS" target="_blank" rel="noopener"
>wikipedia.com&lt;/a>)&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Background - access patterns&lt;/strong>: The way a program uses storage also has a huge impact on the performance one can achieve. Sequential access is when we read or write a large file. When this happens the operating system and harddrive can optimize and &amp;ldquo;merge&amp;rdquo; operations so that we can read or write a much bigger chunk of data at a time. If we can read 1MB at a time 150 times per second we get 150MB/s of bandwidth. However, fully random access where the smallest chunk we read or write is a 4KB block the same 150 IOPS would only give a bandwidth of 0.6MB/s!&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Background - cloud vs physical&lt;/strong>: Now we know what HDDs are limited to a low IOPS and low IOPS combined with a random access pattern gives us a low overall bandwidth. There is a huge gotcha here when it comes to cloud. On Azure when using Premium Managed SSD the IOPS you are given is a factor of the disk size you provision (&lt;a class="link" href="https://azure.microsoft.com/en-us/pricing/details/managed-disks/" target="_blank" rel="noopener"
>microsoft.com&lt;/a>). A 512GB disk is limited to 2.300 IOPS and 150MB/s. With 100% random access that only gives about 9MB/s of bandwidth!&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Background - OS caching&lt;/strong>: To overcome some of the limitations of the underlying disk (mostly IOPS) there are potentially several layers of caching involved. Linux file systems can have &lt;code>writeback&lt;/code> enabled which causes Linux to temporarily store data that is going to be written to disk in memory. This can give a big performance increase when there are sudden spikes of writes exceeding the performance of the underlying disk. It also increases the chance that operations can be &lt;code>merged&lt;/code> where several write operations to areas of the disk that are nearby can be executed as one. This caching works best for sudden peaks and will not necessarily be enough if there is continous random writes to disk. This caching also means that even though an application thinks it has saved some data to disk it can be lost in the case of a power outage or other failure. Applications can also explicitly request &lt;code>direct&lt;/code> access where every operation is persisted to disk before receiving a confirmation. This is a trade-off between performance and durability that needs to be decided based on the application itself and the environment.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Background - Azure caching&lt;/strong>: Azure also provides read and write cache for its &lt;code>disks&lt;/code> which is enabled by default. As we will see soon for our use case it&amp;rsquo;s not a good idea to use.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="WhatToMeasure">&lt;/a>&lt;/p>
&lt;h2 id="what-to-measure">What to measure?
&lt;/h2>&lt;blockquote>
&lt;p>These metrics are collected by the Prometheus &lt;code>node-exporter&lt;/code> and follows it&amp;rsquo;s naming. I&amp;rsquo;ve also created a dashboard that is available on &lt;a class="link" href="https://grafana.com/dashboards/9852" target="_blank" rel="noopener"
>Grafana.com&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>With the USE methodology as a guideline and the two separate but related &amp;ldquo;resources&amp;rdquo;, bandwidth and IOPS we can look for some useful metrics.&lt;/p>
&lt;p>Utilization:&lt;/p>
&lt;ul>
&lt;li>&lt;code>rate(node_disk_written_bytes_total)&lt;/code> - Write bandwidth. The maximum is given by Azure and is 25MB/s for our disk size.&lt;/li>
&lt;li>&lt;code>rate(node_disk_writes_completed_total)&lt;/code> - Write operations. The maximum is given by Azure and is 120 IOPS for our disk size.&lt;/li>
&lt;li>&lt;code>rate(node_disk_io_time_seconds_total)&lt;/code> - Disk active time in percent. The time the disk was busy servicing requests. 100% means fully utilized.&lt;/li>
&lt;/ul>
&lt;p>Saturation:&lt;/p>
&lt;ul>
&lt;li>&lt;code>rate(node_cpu_seconds_total{mode=&amp;quot;iowait&amp;quot;}&lt;/code> - CPU iowait. The percentage of time a CPU core is blocked from doing useful work because it&amp;rsquo;s waiting for an IO operation to complete (typically disk, but can also be network).&lt;/li>
&lt;/ul>
&lt;p>Useful calculated metrics:&lt;/p>
&lt;ul>
&lt;li>&lt;code>rate(node_disk_write_time_seconds_total) / rate(node_disk_writes_completed_total)&lt;/code> - Write latency. How long from a write is requested until it&amp;rsquo;s completed.&lt;/li>
&lt;li>&lt;code>rate(node_disk_written_bytes_total) / rate(node_disk_writes_completed_total)&lt;/code> - Write size. How big the &lt;strong>average&lt;/strong> write operation is. 4KB is minimum and indicates 100% random access while 512KB is maximum and indicates sequential access.&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="HowToMeasureDisk">&lt;/a>&lt;/p>
&lt;h2 id="how-to-measure-disk">How to measure disk
&lt;/h2>&lt;p>The best tool for measuring disk performance is &lt;code>fio&lt;/code>, even though it might seem a bit intimidating at first due to it&amp;rsquo;s insane number of options.&lt;/p>
&lt;p>Installing &lt;code>fio&lt;/code> on Ubuntu:&lt;/p>
&lt;pre>&lt;code>apt-get install fio
&lt;/code>&lt;/pre>
&lt;p>&lt;code>fio&lt;/code> executes &lt;code>jobs&lt;/code> described in a file. Here is the top of our jobs file:&lt;/p>
&lt;pre>&lt;code>[global]
ioengine=libaio # sync|libaio|mmap
group_reporting
thread
size=10g # Size of test file
cpus_allowed=1 # Only use this CPU core
runtime=300s # Run test for 5 minutes
[test1]
filename=/tmp/fio-test-file
direct=1 # If value is true, use non-buffered I/O. Non-buffered I/O usually means O_DIRECT
readwrite=write # read|write|randread|randwrite|readwrite|randrw
iodepth=1 # How many operations to queue to the disk
blocksize=4k
&lt;/code>&lt;/pre>
&lt;p>The fields we will be changing for the various tests are &lt;code>direct&lt;/code>, &lt;code>readwrite&lt;/code>, &lt;code>iodepth&lt;/code> and &lt;code>blocksize&lt;/code>. Save the contents in a file named &lt;code>jobs.fio&lt;/code> and we run a test with &lt;code>fio --sector test1 jobs.fio&lt;/code> and wait until the test completes.&lt;/p>
&lt;blockquote>
&lt;p>PS: To run these tests on higher performance hardware and better caching you might want to set &lt;code>runtime&lt;/code> to &lt;code>0&lt;/code> to have the test run continously and monitor the metrics until performance reaches a steady-state.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="HowToMeasureDiskOnAKS">&lt;/a>&lt;/p>
&lt;h2 id="how-to-measure-disk-on-azure-kubernetes-service">How to measure disk on Azure Kubernetes Service
&lt;/h2>&lt;p>For this testing we use a standard Prometheus installation collecting data from &lt;code>node-exporter&lt;/code> and visualizing data in Grafana. The dashboard I created for the testing can be found here: &lt;a class="link" href="https://grafana.com/dashboards/9852" target="_blank" rel="noopener"
>https://grafana.com/dashboards/9852&lt;/a>.&lt;/p>
&lt;p>By default Kubernetes will schedule a Pod to any node that has enough memory and CPU for our workload. Since one of the tests we are going to run are on the OS disk we do not want the Pod to run on the same node as any other disk-intensive application, such as Prometheus.&lt;/p>
&lt;p>Look at which Pods are running with &lt;code>kubectl get pods -o wide&lt;/code> and look for a node that does not have any disk-intensive application.&lt;/p>
&lt;p>Then we tag that node with &lt;code>kubectl label nodes aks-nodepool1-37707184-2 tag=disktest&lt;/code>. This allows us later to specify that we want to run our testing Pod on that specific node.&lt;/p>
&lt;hr>
&lt;p>A StorageClass in Kubernetes is a specification of a underlying disk that Pods can request usage of through &lt;code>volumeClaimTemplates&lt;/code>. AKS comes with a default StorageClass &lt;code>managed-premium&lt;/code> that has caching enabled. Most of these tests require the Azure cache disabled so create a new StorageClass &lt;code>managed-premium-retain-nocache&lt;/code>:&lt;/p>
&lt;pre>&lt;code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: managed-premium-retain-nocache
provisioner: kubernetes.io/azure-disk
reclaimPolicy: Retain
parameters:
storageaccounttype: Premium_LRS
kind: Managed
cachingmode: None
&lt;/code>&lt;/pre>
&lt;p>You can add it to your cluster with:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2019-02-23-disk-performance-on-aks-part-1/storageclass.yaml
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>Next we create a StatefulSet that uses a &lt;code>volumeClaimTemplate&lt;/code> to request a 250GB Azure disk. This provisions a P15 Azure Premium SSD with 125MB/s bandwidth and 1100 IOPS:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2019-02-23-disk-performance-on-aks-part-1/ubuntu-statefulset.yaml
&lt;/code>&lt;/pre>
&lt;p>Follow the progress of the Pod creation with &lt;code>kubectl get pods -w&lt;/code> and wait until it is &lt;code>Running&lt;/code>.&lt;/p>
&lt;hr>
&lt;p>When the Pod is &lt;code>Running&lt;/code> we can start a shell on it with &lt;code>kubectl exec -it disk-test-0 bash&lt;/code>&lt;/p>
&lt;p>Once inside &lt;code>bash&lt;/code> on the Pod, we install &lt;code>fio&lt;/code>:&lt;/p>
&lt;pre>&lt;code>apt-get update &amp;amp;&amp;amp; apt-get install -y fio wget
&lt;/code>&lt;/pre>
&lt;p>And save the contents of in the Pod:&lt;/p>
&lt;pre>&lt;code>wget https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2019-02-23-disk-performance-on-aks-part-1/jobs.fio
&lt;/code>&lt;/pre>
&lt;p>Now we can run the different test sections one by one. &lt;strong>PS: If you don&amp;rsquo;t specify a section &lt;code>fio&lt;/code> will run all the tests &lt;em>simultaneously&lt;/em>, which is not what we want.&lt;/strong>&lt;/p>
&lt;pre>&lt;code>fio --section=test1 jobs.fio
fio --section=test2 jobs.fio
fio --section=test3 jobs.fio
fio --section=test4 jobs.fio
fio --section=test5 jobs.fio
fio --section=test6 jobs.fio
fio --section=test7 jobs.fio
fio --section=test8 jobs.fio
fio --section=test9 jobs.fio
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="Tests">&lt;/a>&lt;/p>
&lt;h2 id="test-results">Test results
&lt;/h2>&lt;p>&lt;a id="Test1">&lt;/a>&lt;/p>
&lt;h3 id="test-1---learning-to-dislike-azure-cache">Test 1 - Learning to dislike Azure Cache
&lt;/h3>&lt;p>&lt;em>Sequential write, 4K block size, Azure Cache enabled, OS cache disabled. See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test1.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>I run the first tests on the OS disk of a Kubernetes node. The OS disks have Azure caching enabled.&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1.png"
width="1697"
height="1232"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1_hu1499039068193334488.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1_hu13957841185659198382.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="330px"
>&lt;/p>
&lt;p>The first 1-2 minutes of the test I get very good performance of 45MB/s and ~11.500 IOPS but that drops to 0 very quickly as the cache is full and busy writing things to the underlying disk. When that happens everything freezes and I cannot even execute shell commands. After stopping the test the system still hangs for a bit while the cache empties.&lt;/p>
&lt;p>The maximum latency measured by &lt;code>fio&lt;/code> was 108751k usec. Or about 108 seconds!&lt;/p>
&lt;blockquote>
&lt;p>For the first try of these tests a 20-30 second period of very fast writes (250MB/s) caused a 7-8 minutes hang while the cache emptied. Trying again caused another pattern of lower peak performance with shorter hangs in between. Very unpredictable.
I&amp;rsquo;m not sure what to make of this. It&amp;rsquo;s not acceptable that a Kubernetes node becomes unresponsive for many minutes following a short burst of writing. There are scattered recommendations online of disabling caching for write-heavy applications. Since I have not found any way to measure the Azure cache itself, the results are unpredictable and potentially very impactful as well as making it very hard to use the metrics we do have to evaluate application and storage behaviour I&amp;rsquo;ve concluded that it&amp;rsquo;s best to use data disks with caching disabled for our workloads (you cannot disable caching on an AKS node OS disk).&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test2">&lt;/a>&lt;/p>
&lt;h3 id="test-2---disable-azure-cache---enable-os-cache">Test 2 - Disable Azure Cache - enable OS cache
&lt;/h3>&lt;p>&lt;em>Sequential write, 4K block size. &lt;strong>Change: Azure cache disabled, OS caching enabled.&lt;/strong> See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test2.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test2.png"
width="1703"
height="1226"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test2_hu9653487243883720288.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test2_hu12369574169741432704.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="333px"
>&lt;/p>
&lt;p>If we swap the Azure cache for the Linux OS cache we see that &lt;code>iowait&lt;/code> increases while the writing occurs. The application sees high write performance until the number of &lt;code>Dirty bytes&lt;/code> reaches a threshold of about 3.7GB of memory. The performance of the underlying disk is 125MB/s and 250 IOPS. Here we are throttled by the 125MB/s limit of the Azure P15 Premium SSD.&lt;/p>
&lt;p>Also notice that on sequential writes of 4K with OS caching the actual blocks written to disk is 512K which saves us a lot of IOPS. This will become important later.&lt;/p>
&lt;p>&lt;a id="Test3">&lt;/a>&lt;/p>
&lt;h3 id="test-3---disable-os-cache">Test 3 - Disable OS cache
&lt;/h3>&lt;p>&lt;em>Sequential write, 4K block size. &lt;strong>Change: OS caching disabled.&lt;/strong> See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test3.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test3.png"
width="1698"
height="1233"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test3_hu9187350742741251895.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test3_hu14645523390977373096.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="330px"
>&lt;/p>
&lt;blockquote>
&lt;p>By disabling the OS cache (&lt;code>direct=1&lt;/code>) the results are consistent and predictable. There is no &lt;code>iowait&lt;/code> since the application does not have multiple writes pending at the same time. Because of the 2-3ms latency of the disks we are not able to get more than about 400 IOPS. This gives us a meager 1.5MB/s even though the disk is limited to 1100 IOPS and 125MB/s. To reach that we need multiple simultaneous writes or a bigger IO depth (queue). &lt;code>Disk active time&lt;/code> is also 0% which indicates that the disk is not saturated.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test4">&lt;/a>&lt;/p>
&lt;h3 id="test-4---increase-io-depth">Test 4 - Increase IO depth
&lt;/h3>&lt;p>&lt;em>Sequential write, 4K block size, OS caching disabled. &lt;strong>Change: IO depth 16.&lt;/strong> See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test4.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test4.png"
width="1698"
height="1237"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test4_hu5722226652359263237.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test4_hu2996385930643672719.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="329px"
>&lt;/p>
&lt;blockquote>
&lt;p>For this test we only increase the IO depth from 1 to 16. IO depth is the number of write operations &lt;code>fio&lt;/code> will execute simultaneously. Since we are using &lt;code>direct&lt;/code> these will be queued by the OS for writing. We are now able to hit the performance limit of 1100 IOPS. &lt;code>Disk active time&lt;/code> is now steady at 100% indicating that we have saturated the disk.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test5">&lt;/a>&lt;/p>
&lt;h3 id="test-5---larger-block-size-smaller-io-depth">Test 5 - Larger block size, smaller IO depth
&lt;/h3>&lt;p>&lt;em>Sequential write, OS caching disabled. &lt;strong>Change: 128K block size, IO depth 1.&lt;/strong> See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test5.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test5.png"
width="1699"
height="1229"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test5_hu14954921568768718351.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test5_hu3268361891850381379.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;blockquote>
&lt;p>We increase the block size to 128KB and reduce the IO depth to 1 again. The write latency for larger blocks increase to ~5ms which gives us 200 IOPS and 28MB/s. The disk is not saturated.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test6">&lt;/a>&lt;/p>
&lt;h3 id="test-6---enable-os-cache">Test 6 - Enable OS cache
&lt;/h3>&lt;p>&lt;em>Sequential write, 256K block size, IO depth 1. &lt;strong>Change: OS caching enabled.&lt;/strong> See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test6.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test6.png"
width="1697"
height="1230"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test6_hu5250198874418650337.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test6_hu6421477374997710977.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="331px"
>&lt;/p>
&lt;blockquote>
&lt;p>We have now enabled the OS cache/buffer (&lt;code>direct=0&lt;/code>). We can see that the writes hitting the disk are now merged to 512KB blocks. We are hitting the 125MB/s limit with about 250 IOPS. Enabling the cache also has other effects: CPU suddenly shows significant IO wait. The write latency shoots through the roof. Also note that the writing continued for 30-40 seconds after the test was done. &lt;strong>This also means that the bandwidth and IOPS that &lt;code>fio&lt;/code> sees and reports is higher than what is actually hitting the disk.&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test7">&lt;/a>&lt;/p>
&lt;h3 id="test-7---random-writes-small-block-size">Test 7 - Random writes, small block size
&lt;/h3>&lt;p>&lt;em>IO depth 1, OS caching enabled. &lt;strong>Change: Random write, 4K block size.&lt;/strong> See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test7.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test7.png"
width="1702"
height="1239"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test7_hu9566364161109018395.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test7_hu821024261791687868.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="329px"
>&lt;/p>
&lt;blockquote>
&lt;p>Here we go from sequential writes to random writes. We are limited by IOPS. The average size of the blocks actually written to disks, and the IOPS required to hit the bandwidth limit is actually varying a bit throughout the test. The time taken to empty the cache is about as long as I ran the test (4-5 minutes).&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test8">&lt;/a>&lt;/p>
&lt;h3 id="test-8---large-block-size">Test 8 - Large block size
&lt;/h3>&lt;p>&lt;em>Random write, OS caching enabled. &lt;strong>Change: 256K block size, IO depth 16.&lt;/strong> See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test8.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test8.png"
width="1699"
height="1235"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test8_hu1300341601469829731.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test8_hu923727430283491984.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="330px"
>&lt;/p>
&lt;blockquote>
&lt;p>Increasing the block size to 256K makes us bandwidth limited to 125MB/s.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Conclusion">&lt;/a>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>Access patterns and block sizes have a tremendous impact on the amount of data we are able to write to disk.&lt;/p></description></item><item><title>Managed Kubernetes on Microsoft Azure (English)</title><link>https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/</link><pubDate>Fri, 29 Dec 2017 00:00:00 +0000</pubDate><guid>https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/</guid><description>&lt;img src="https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/2017-12-23-managed-kubernetes-on-azure-eng.png" alt="Featured image of post Managed Kubernetes on Microsoft Azure (English)" />&lt;p>&lt;em>A few days ago I wrote a walkthrough of &lt;a class="link" href="https://blog.stian.omg.lol/2017/12/25/managed-kubernetes-on-azure.html" >setting up Azure Container Service (AKS) in Norwegian&lt;/a>. Someone asked me for an English version of that, and here it is.&lt;/em>&lt;/p>
&lt;p>Kubernetes(K8s) is becoming the de-facto standard for deploying container-based applications and workloads. Microsoft is currently in preview of their managed Kubernetes offering (Azure Kubernetes Service, AKS) which makes it easy to create a Kubernetes cluster and deploy workloads without the skill and time required to manage day-to-day operations of a Kubernetes-cluster, which today can be complex and time consuming.&lt;/p>
&lt;p>In this post we will set up a Kubernetes cluster from scratch using Azure CLI.&lt;/p>
&lt;p>Table of contents&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="#Background" >Background&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Dockercontainers" >Docker containers&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Containerorchestration" >Container orchestration&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#QuickstartAKS" >Getting started with Azure Kubernetes - AKS&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Caveats" >Caveats&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Preparations" >Preparations&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#AzureLogin" >Azure login&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#ActivateContainerService" >Activate ContainerService&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#CreateResourceGroup" >Create a resource group&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#CreateK8sCluster" >Create a Kubernetes cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#InstallKubectl" >Install kubectl&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#InspectCluster" >Inspect cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#StartNginx" >Start some nginx containere&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#NginxService" >Making nginx available with a service&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#ScaleCluster" >Scale cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#DeleteCluster" >Delete cluster&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Bonusmaterial" >Bonus material&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#HelmIntro" >Deploying services with Helm&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#HelmMinecraft" >Deploy MineCraft with Helm&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#KubernetesDashboard" >Kubernetes Dashboard&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Conclusion" >Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="microsoft-azure">Microsoft Azure
&lt;/h4>&lt;blockquote>
&lt;p>&lt;a class="link" href="https://azure.microsoft.com/en-us/free/" target="_blank" rel="noopener"
>If you don&amp;rsquo;t have a Azure subscription already you can try services for $200 for 30 days.&lt;/a> The VM size &lt;strong>Standard_B2s&lt;/strong> is Burstable, has 2vCPU, 4GB RAM, 8GB temp storage and costs roughly $38 / month. For $200 you can have a cluster of 3-4 B2s nodes plus traffic, loadbalancers and other additional costs.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;em>We have no affiliation with Microsoft Azure except their sponsorship of our startup &lt;a class="link" href="http://www.datadynamics.no/" target="_blank" rel="noopener"
>DataDynamics&lt;/a> with cloud services for 24 months in their &lt;a class="link" href="https://bizspark.microsoft.com/" target="_blank" rel="noopener"
>BizSpark program&lt;/a>.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Background">&lt;/a>&lt;/p>
&lt;h2 id="background">Background
&lt;/h2>&lt;p>&lt;a id="Dockercontainers">&lt;/a>&lt;/p>
&lt;h3 id="docker-containers">Docker containers
&lt;/h3>&lt;p>&lt;em>We will not do a deep dive on Docker containers in this post, but here is a summary for those who are not familiar with it.&lt;/em>&lt;/p>
&lt;p>Docker is a way to package software so that it can run on the most popular platforms without worrying about installation, dependencies and to a certain degree, configuration.&lt;/p>
&lt;p>In addition, a Docker container uses the operating system of the host machine when it runs. Because of this it&amp;rsquo;s possible to run many more containers on the same host machine compared to running virtual machines.&lt;/p>
&lt;p>Here is a incomplete and rough comparison between a Docker container and a virtual machine:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Virtual machine&lt;/th>
&lt;th>Docker container&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Image size&lt;/td>
&lt;td>from 200MB to many GB&lt;/td>
&lt;td>from 10MB to 3-400MB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Startup time&lt;/td>
&lt;td>60 seconds +&lt;/td>
&lt;td>1-10 seconds&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Memory usage&lt;/td>
&lt;td>256MB-512MB-1GB +&lt;/td>
&lt;td>2MB +&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Security&lt;/td>
&lt;td>Good isolation between VMs&lt;/td>
&lt;td>Not as good isolation between containers&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Building image&lt;/td>
&lt;td>Minutes&lt;/td>
&lt;td>Seconds&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> The numbers for virtual machines is taken from memory. I tried starting a MySQL virtual appliance on my laptop but VMware Player refuses to run because of Windows Hyper-V incompatibility. VMware Workstation refuses to run because of license issues and Oracle VirtualBox repeatedly gives me a nasty bluescreen. Hooray!&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong> The smallest and fastest Docker images are built on Alpine Linux. For the webserver Nginx the Alpine-based image is 15MB compared to 108MB for the normal Debian-based image. PostgreSQL:Alpine is 38MB compared to 287MB with &amp;ldquo;full&amp;rdquo; OS. Last version of MySQL is 343MB but will in version 8 support Alpine Linux as well.&lt;/p>
&lt;/blockquote>
&lt;p>To recap, some of the advantages of Docker containers are:&lt;/p>
&lt;ul>
&lt;li>Compatibility across platforms, Linux, Windows, MacOS.&lt;/li>
&lt;li>10-100x smaller size. Faster to download, build and upload.&lt;/li>
&lt;li>Memory usage only for application and not base OS.
&lt;ul>
&lt;li>Advantage when developing. Ability to run 10-20-30 containers on a development laptop.&lt;/li>
&lt;li>Advantage in production. Can reduce hardware/cloud costs considerably.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Near instant startup. Makes dynamic scaling of applications easier.&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://store.docker.com/editions/community/docker-ce-desktop-windows" target="_blank" rel="noopener"
>Download Docker for Windows here.&lt;/a>&lt;/p>
&lt;p>To start a MySQL database container from Windows CMD or Powershell:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">docker run --name mysql -p 3306:3306 -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Stop the container with:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">docker kill mysql
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>You can search for already built Docker images on &lt;a class="link" href="https://hub.docker.com/" target="_blank" rel="noopener"
>Docker Hub&lt;/a>. It&amp;rsquo;s also possible to create private Docker repositories for your own software that you don&amp;rsquo;t want to be publicly available.&lt;/p>
&lt;p>&lt;a id="Containerorchestration">&lt;/a>&lt;/p>
&lt;h4 id="container-orchestration">Container orchestration
&lt;/h4>&lt;p>Now that Docker container images has become the preferred way to package and distribute software on the Linux platform, there has emerged a need for systems to coordinate running and deploying these containers. Similar to the ecosystem of products VMware has built up around development and operation of virtual machines.&lt;/p>
&lt;p>Container orchestration systems have the responsibility for:&lt;/p>
&lt;ul>
&lt;li>Load balancing.&lt;/li>
&lt;li>Service discovery.&lt;/li>
&lt;li>Health checks.&lt;/li>
&lt;li>Automatic scaling and restarting of host nodes and containers.&lt;/li>
&lt;li>Zero downtime upgrades (rolling deploys).&lt;/li>
&lt;/ul>
&lt;p>Until recently the ecosystem around container orchestration has been fragmented, and the most popular alternatives have been:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://kubernetes.io/" target="_blank" rel="noopener"
>Kubernetes&lt;/a> (Originaly from Google, now managed by CNCF, the Cloud Native Computing Foundation)&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.docker.com/engine/swarm/" target="_blank" rel="noopener"
>Swarm&lt;/a> (From the maker of Docker)&lt;/li>
&lt;li>&lt;a class="link" href="http://mesos.apache.org/" target="_blank" rel="noopener"
>Mesos&lt;/a> (From Apache Software Foundation)&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/coreos/fleet" target="_blank" rel="noopener"
>Fleet&lt;/a> (From CoreOS)&lt;/li>
&lt;/ul>
&lt;p>But the last year there has been a convergence towards Kubernetes as the preferred solution.&lt;/p>
&lt;ul>
&lt;li>7 February
&lt;ul>
&lt;li>&lt;a class="link" href="https://coreos.com/blog/migrating-from-fleet-to-kubernetes.html" target="_blank" rel="noopener"
>CoreOS announces that they are removing Fleet from Container Linux and recommends Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>27 July
&lt;ul>
&lt;li>&lt;a class="link" href="https://azure.microsoft.com/en-us/blog/announcing-cncf/" target="_blank" rel="noopener"
>Microsoft joins the CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>9 August
&lt;ul>
&lt;li>&lt;a class="link" href="https://techcrunch.com/2017/08/09/aws-joins-the-cloud-native-computing-foundation/" target="_blank" rel="noopener"
>Amazon Web Services join the CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>29 August
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.geekwire.com/2017/now-vmware-pivotal-cncf-becoming-hub-enterprise-tech/" target="_blank" rel="noopener"
>VMware and Pivotal joins the CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>17 September
&lt;ul>
&lt;li>&lt;a class="link" href="https://techcrunch.com/2017/09/13/oracle-joins-the-cloud-native-computing-foundation-as-a-platinum-member/" target="_blank" rel="noopener"
>Oracle joins the CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>17 October
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.theregister.co.uk/2017/10/17/docker_ee_kubernetes_support/" target="_blank" rel="noopener"
>Docker announces native support for Kubernetes in addition to it&amp;rsquo;s own Swarm product&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>24 October
&lt;ul>
&lt;li>&lt;a class="link" href="https://azure.microsoft.com/en-us/blog/introducing-azure-container-service-aks-managed-kubernetes-and-azure-container-registry-geo-replication/" target="_blank" rel="noopener"
>Microsoft Azure announces the managed Kubernetes service AKS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>29 November
&lt;ul>
&lt;li>&lt;a class="link" href="https://aws.amazon.com/blogs/aws/amazon-elastic-container-service-for-kubernetes/" target="_blank" rel="noopener"
>Amazon Web Services announces the managed Kubernetes service EKS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Especially the last two news items are important. Deploying and running your own Kubernetes-installation requires time and skills (&lt;a class="link" href="https://stripe.com/blog/operating-kubernetes" target="_blank" rel="noopener"
>Read how Stripe used 5 months to trust running Kubernetes in production, just for batch jobs.&lt;/a>)&lt;/p>
&lt;p>Until now the choice has been running your own Kubernetes cluster or using Google Container Engine which has been &lt;a class="link" href="https://cloudplatform.googleblog.com/2014/11/unleashing-containers-and-kubernetes-with-google-compute-engine.html" target="_blank" rel="noopener"
>using Kubernetes since 2014&lt;/a>. Many of us feel a certain discomfort by locking ourselves to one provider. But this is now changing when you can develop infrastructure on Kubernetes and choose between the 3 large cloud providers in addition to running your own cluster if wanted. &lt;strong>*&lt;/strong>&lt;/p>
&lt;p>&lt;strong>*&lt;/strong> Kubernetes is a fast moving project, and features might be available on the different platforms on different timelines.&lt;/p>
&lt;p>&lt;a id="QuickstartAKS">&lt;/a>&lt;/p>
&lt;h2 id="getting-started-with-azure-kubernetes---aks">Getting started with Azure Kubernetes - AKS
&lt;/h2>&lt;p>&lt;a id="Caveats">&lt;/a>&lt;/p>
&lt;h3 id="caveats">Caveats
&lt;/h3>&lt;blockquote>
&lt;p>This guide is based on the documentation on &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough" target="_blank" rel="noopener"
>Microsoft.com&lt;/a>. Setting up a Azure Kubernetes cluster did not work in the beginning of December, but today, 23. December, it seems to work fairly well. But, upgrading the cluster from Kubernetes 1.7 to 1.8 for example does NOT work.&lt;/p>
&lt;p>AKS is in Preview and Azure are working continuously to make AKS stable and to support as many Kubernetes-features as possible. Amazon Web Services has a similar closed invite-only Preview currently while working on stability and features.&lt;/p>
&lt;p>Both Azure and AWS expresses expectations about their Kubernetes offerings will be ready for production in 2018.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Preparations">&lt;/a>&lt;/p>
&lt;h3 id="preparations">Preparations
&lt;/h3>&lt;p>You need Azure-CLI (version 2.0.21 or newer) to execute the &lt;code>az&lt;/code> commands:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://aka.ms/InstallAzureCliWindows" target="_blank" rel="noopener"
>Download Azure-CLI here&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest" target="_blank" rel="noopener"
>Information about Azure-CLI on MacOS and Linux here&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>All commands executed in Windows PowerShell.&lt;/p>
&lt;p>&lt;a id="AzureLogin">&lt;/a>&lt;/p>
&lt;h3 id="azure-login">Azure login
&lt;/h3>&lt;p>Log on to Azure:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az login
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>You will get a link to open in your browser together with an authentication code. Enter the code on the webpage and &lt;code>az login&lt;/code> will save the login information so that you will not have to authenticate again on the same machine.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> The login information gets saved in &lt;code>C:\Users\Username\.azure\&lt;/code>. You have to make sure nobody can access these files. They will then have full access to your Azure account.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="ActivateContainerService">&lt;/a>&lt;/p>
&lt;h3 id="activate-containerservice">Activate ContainerService
&lt;/h3>&lt;p>Since AKS is in Preview/Beta, you explicitly have to activate it in your subscription to get access to the &lt;code>aks&lt;/code> subcommands.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az provider register -n Microsoft.ContainerService
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">az provider show -n Microsoft.ContainerService
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="CreateResourceGroup">&lt;/a>&lt;/p>
&lt;h3 id="create-a-resource-group">Create a resource group
&lt;/h3>&lt;p>Here we create a resource group named &amp;ldquo;my_aks_rg&amp;rdquo; in Azure region West Europe.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az group create --name my_aks_rg --location westeurope
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong>
To see a list of all available Azure regions, use the command &lt;code>az account list-locations --output table&lt;/code>. &lt;strong>PS&lt;/strong> AKS might not be available in all regions yet!&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="CreateK8sCluster">&lt;/a>&lt;/p>
&lt;h3 id="create-kubernetes-cluster">Create Kubernetes cluster
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks create --resource-group my_aks_rg --name my_cluster --node-count 3 --generate-ssh-keys --node-vm-size Standard_B2s --node-osdisk-size 128 --kubernetes-version 1.8.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;code>--node-count&lt;/code>
&lt;ul>
&lt;li>Number of agent(host) nodes available to run containers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--generate-ssh-keys&lt;/code>
&lt;ul>
&lt;li>Creates and prints a SSH key which can be used for SSHing directly to the agent nodes.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--node-vm-size&lt;/code>
&lt;ul>
&lt;li>Which size Azure VMs the agent nodes should be created as. To see available sizes use &lt;code>az vm list-sizes -l westeurope --output table&lt;/code> and &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes" target="_blank" rel="noopener"
>Microsofts webpages&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--node-osdisk-size&lt;/code>
&lt;ul>
&lt;li>Disk size of the agent nodes in GB. &lt;strong>PS&lt;/strong> Containers can be stopped and moved to another host if Kubernetes finds it necessary or if a agent node disappears. All data saved locally in the container will be gone. If saving data permanently use Kubernetes PersistentVolumes and not the local agent node or container disks.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--kubernetes-version&lt;/code>
&lt;ul>
&lt;li>Which Kubernetes version to install. Azure does NOT necessarily install the last version by default, and currently upgrading with &lt;code>az aks upgrade&lt;/code> does not work. Latest version available right now is 1.8.2. It&amp;rsquo;s recommended to use the latest available version since there is a lot of changes from version to version. The documentation is also much better for newer versions.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Save the output of the command in a file in a secure location. It contains keys that can be used to connect to the cluster with SSH. Even though that should not in theory be necessary.&lt;/p>
&lt;p>&lt;a id="InstallKubectl">&lt;/a>&lt;/p>
&lt;h3 id="install-kubectl">Install kubectl
&lt;/h3>&lt;p>&lt;code>kubectl&lt;/code> is the client which performs all operations against your Kubernetes cluster. Azure CLI can install &lt;code>kubectl&lt;/code> for you:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks install-cli
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>After &lt;code>kubectl&lt;/code> is installed we need to get login information so that &lt;code>kubectl&lt;/code> can communicate with the Kubernetes cluster.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks get-credentials --resource-group my_aks_rg --name my_cluster
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>The login information is saved in &lt;code>C:\Users\Username\.kube\config&lt;/code>. Keep these files secure as well.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong> When you have several Kubernetes clusters you can change which one &lt;code>kubectl&lt;/code> talks to with &lt;code>kubectl config get-contexts&lt;/code> and &lt;code>kubectl config set-context my_cluster&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="InspectCluster">&lt;/a>&lt;/p>
&lt;h3 id="inspect-cluster">Inspect cluster
&lt;/h3>&lt;p>To check that the cluster and &lt;code>kubectl&lt;/code> works we start with a couple of commands.&lt;/p>
&lt;p>See all agent nodes and status:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get nodes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME STATUS AGE VERSION
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-0 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-1 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-2 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>See all services, pods and deployments:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get all --all-namespaces
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system po/kubernetes-dashboard-6fc8cf9586-frpkn 1/1 Running 0 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system svc/kubernetes-dashboard 10.0.161.132 &amp;lt;none&amp;gt; 80/TCP 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system deploy/kubernetes-dashboard 1 1 1 1 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME DESIRED CURRENT READY AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system rs/kubernetes-dashboard-6fc8cf9586 1 1 1 3d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>This is just some of the output from this command. You do not have to know what the resources in the &lt;code>kube-system&lt;/code> namespace does. That is part of the intention when Microsoft is managing our cluster for us.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Namespaces&lt;/strong>
In Kubernetes there is something called Namespaces. Resources in one namespace does not have automatic access to resources in another namespace. The services that runs Kubernetes itself use the namespace &lt;code>kube-system&lt;/code>. The &lt;code>kubectl&lt;/code> command by default only shows you resources in the &lt;code>default&lt;/code> namespace, unless you specify &lt;code>--all-namespaces&lt;/code> or &lt;code>--namespace=xx&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="StartNginx">&lt;/a>&lt;/p>
&lt;h3 id="start-some-nginx-containers">Start some nginx containers
&lt;/h3>&lt;blockquote>
&lt;p>An instance of a running container in Kubernetes is called a &lt;strong>Pod&lt;/strong>.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;code>nginx&lt;/code> is a fast and flexible web server.&lt;/p>
&lt;/blockquote>
&lt;p>Now that the clsuter is up we can start rolling out services and deployments on it.&lt;/p>
&lt;p>Lets start with creating a Deployment consiting of 3 containers all running the &lt;code>nginx:mainline-alpine&lt;/code> image from &lt;a class="link" href="https://hub.docker.com/r/_/nginx/" target="_blank" rel="noopener"
>Docker hub&lt;/a>.&lt;/p>
&lt;p>&lt;strong>nginx-dep.yaml&lt;/strong> looks like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">apiVersion: apps/v1beta2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kind: Deployment
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: nginx-deployment
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> replicas: 3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> selector:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> matchLabels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> template:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> containers:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - name: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> image: nginx:mainline-alpine
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ports:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - containerPort: 80
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Load this into the cluster with &lt;code>kubectl create&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-dep.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>This command creates the resources described in the file. &lt;code>kubectl&lt;/code> can read files either from your local disk or from a web URL.&lt;/p>
&lt;blockquote>
&lt;p>After making changes to a resource definition (&lt;code>.yaml&lt;/code> file), you can update the resources in the cluster with &lt;code>kubetl replace -f resource.yaml&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>We can verify that the Deployment is ready:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get deploy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment 3 3 3 3 10m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>We can also get the actual Pods that are running:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get pods
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-dqwx5 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-xwzpw 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-z5tfk 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Logger&lt;/strong> We can view logs from one pod with &lt;code>kubectl logs nginx-deployment-569477d6d8-xwzpw&lt;/code>. But since we in this case don&amp;rsquo;t know which Pod ends up getting an incomming request we can view logs from all the Pods which have &lt;code>app=nginx&lt;/code> label: &lt;code>kubectl logs -lapp=nginx&lt;/code>. The use of &lt;code>app=nginx&lt;/code> is our choice in &lt;code>nginx-dep.yaml&lt;/code> when we configured &lt;code>spec.template.metadata.labels: app: nginx&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="NginxService">&lt;/a>&lt;/p>
&lt;h3 id="making-nginx-available-with-a-service">Making nginx available with a service
&lt;/h3>&lt;p>To send traffic to our new Pods we need to create a &lt;strong>Service&lt;/strong>. A service consists of one or more Pods which are chosen based on different criteria, for example which labels they have and whether the Pods are Running and Ready.&lt;/p>
&lt;p>Lets create a service which forwards traffic to all Pods with label &lt;code>app: nginx&lt;/code> and are listening to port 80. In addition we make the service available via a LoadBalancer:&lt;/p>
&lt;p>&lt;strong>nginx-svc.yaml&lt;/strong> looks like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">apiVersion: v1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kind: Service
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> type: LoadBalancer
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ports:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - port: 80
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: http
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> targetPort: 80
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> selector:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>We tell Kubernetes to create our service with &lt;code>kubectl create&lt;/code> as usual:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-svc.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>We can then wait and see which IP-address Azure assigns our service:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get svc -w
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx 10.0.24.11 13.95.173.255 80:31522/TCP 15m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> It can take a few minutes for Azure to allocate and assign a Public IP for us. In the mean time &lt;code>&amp;lt;pending&amp;gt;&lt;/code> will appear under EXTERNAL-IP.&lt;/p>
&lt;/blockquote>
&lt;p>A simple &lt;strong>Welcome to nginx&lt;/strong> webpage should now be available on http://13.95.173.255 (&lt;em>remember to replace with your own External-IP&lt;/em>).&lt;/p>
&lt;p>We can also delete the service and deployment afterwards:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl delete svc nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete deploy nginx-deployment
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="ScaleCluster">&lt;/a>&lt;/p>
&lt;h3 id="scaling-the-cluster">Scaling the cluster
&lt;/h3>&lt;p>If we want to change the number of agent nodes running Pods we can do that via Azure-CLI:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks scale --name my_cluster --resource-group my_aks_rg --node-count 5
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>Currently all nodes will be created with the same size as when we created the cluster. AKS will probably get support for &lt;a class="link" href="https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools" target="_blank" rel="noopener"
>&lt;strong>node-pools&lt;/strong>&lt;/a> next year. That will allow for creating different groups of nodes with different size and operating systems, both Linux and Windows.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="DeleteCluster">&lt;/a>&lt;/p>
&lt;h3 id="delete-cluster">Delete cluster
&lt;/h3>&lt;p>You can delete the whole cluster like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks delete --name my_cluster --resource-group my_aks_rg --yes
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="Bonusmaterial">&lt;/a>&lt;/p>
&lt;h2 id="bonus-material">Bonus material
&lt;/h2>&lt;p>Here is some bonus material if you want to go a bit further with Kubernetes.&lt;/p>
&lt;p>&lt;a id="HelmIntro">&lt;/a>&lt;/p>
&lt;h3 id="deploying-services-with-helm">Deploying services with Helm
&lt;/h3>&lt;p>&lt;a class="link" href="https://helm.sh/" target="_blank" rel="noopener"
>Helm&lt;/a> is a package manager and library of software that is ready to be deployed on a Kubernetes cluster.&lt;/p>
&lt;p>Start by downloading the &lt;a class="link" href="https://github.com/kubernetes/helm/releases" target="_blank" rel="noopener"
>Helm-client&lt;/a>. It will read login information etc. from the same location as &lt;code>kubectl&lt;/code> automatically.&lt;/p>
&lt;p>Install the Helm-server (&lt;strong>Tiller&lt;/strong>) on the Kubernetes cluster and update the package library:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">helm init
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm repo update
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>See available packages (&lt;strong>Charts&lt;/strong>) with &lt;code>helm search&lt;/code>.&lt;/p>
&lt;p>&lt;a id="HelmMinecraft">&lt;/a>&lt;/p>
&lt;h4 id="deploy-minecraft-with-helm">Deploy MineCraft with Helm
&lt;/h4>&lt;p>Lets deploy a MineCraft server installation on our cluster, just because we can :-)&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">helm install --name stians --set minecraftServer.eula=true stable/minecraft
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;code>--set&lt;/code> overrides one or more of the standard values configured in the package. The MineCraft package is made in a way where it does not start without accepting the user license agreement by setting the variable &lt;code>minecraftServer.eula&lt;/code>. All the variables that can be set in the MineCraft package are &lt;a class="link" href="https://github.com/kubernetes/charts/blob/master/stable/minecraft/values.yaml" target="_blank" rel="noopener"
>documented here&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>Then we wait for Azure to assign us a Public IP:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get svc -w
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">stians-minecraft 10.0.237.0 13.95.172.192 25565:30356/TCP 3m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Now we can connect to our MineCraft server on &lt;code>13.95.172.192:25565&lt;/code>!&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/minecraft-k8s.png"
width="856"
height="507"
srcset="https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/minecraft-k8s_hu13197832490752396686.png 480w, https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/minecraft-k8s_hu16352153299487520650.png 1024w"
loading="lazy"
alt="Kubernetes in MineCraft on Kubernetes"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="405px"
>&lt;/p>
&lt;p>&lt;a id="KubernetesDashboard">&lt;/a>&lt;/p>
&lt;h3 id="kubernetes-dashboard">Kubernetes Dashboard
&lt;/h3>&lt;p>Kubernetes also has a graphic web user-interface which makes it a bit easier to see which resources are in the cluster, view logs and even open a remote shell inside a running Pod, among other things.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl proxy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Starting to serve on 127.0.0.1:8001
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>kubectl&lt;/code> encrypts and tunnels the traffic to the Kubernetes API servers. The dashboard is available on &lt;a class="link" href="http://127.0.0.1:8001/ui/" target="_blank" rel="noopener"
>http://127.0.0.1:8001/ui/&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/k8s-dash.png"
width="899"
height="588"
srcset="https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/k8s-dash_hu15091082541702636396.png 480w, https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/k8s-dash_hu17318776918264020386.png 1024w"
loading="lazy"
alt="Kubernetes Dashboard"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>&lt;/p>
&lt;p>&lt;a id="Conclusion">&lt;/a>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>I hope you enjoy Kubernetes as much as I have. The learning curve can be a bit steep in the beginning, but it does not take long before you are productive.&lt;/p>
&lt;p>Look at the &lt;a class="link" href="https://v1-8.docs.kubernetes.io/docs/tutorials/" target="_blank" rel="noopener"
>official guides on Kubernetes.io&lt;/a> to learn more about defining different types of resources and services to run on Kubernetes. &lt;strong>PS: There are big changes from version to version so make sure you use the documentation for the correct version!&lt;/strong>&lt;/p>
&lt;p>Kubernetes also have a very active Slack-community on &lt;a class="link" href="http://slack.k8s.io/" target="_blank" rel="noopener"
>kubernetes.slack.com&lt;/a> that is worthwhile to check out.&lt;/p></description></item><item><title>Managed Kubernetes på Microsoft Azure (Norwegian)</title><link>https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/</link><pubDate>Mon, 25 Dec 2017 00:00:00 +0000</pubDate><guid>https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/</guid><description>&lt;img src="https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/2017-12-23-managed-kubernetes-on-azure.png" alt="Featured image of post Managed Kubernetes på Microsoft Azure (Norwegian)" />&lt;p>&lt;em>Update 29. Dec: There is an &lt;a class="link" href="https://blog.stian.omg.lol/2017/12/29/managed-kubernetes-on-azure-eng.html" >English version of this post here.&lt;/a>&lt;/em>&lt;/p>
&lt;p>Kubernetes (K8s) er i ferd med å bli de-facto standard for deployments av kontainer-baserte applikasjoner. Microsoft har nå preview av deres managed Kubernetes tjeneste (Azure Kubernetes Service, AKS) som gjør det enkelt å opprette et Kubernetes cluster og rulle ut tjenester uten å måtte ha kompetanse og tid til den daglige driften av selve Kubernetes-clusteret, som per i dag kan være relativt komplisert og tidkrevende.&lt;/p>
&lt;p>I denne posten setter vi opp et Kubernetes cluster fra scratch ved bruk av Azure CLI.&lt;/p>
&lt;p>Table of contents&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="#Bakgrunn" >Bakgrunn&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Dockercontainers" >Docker containers&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Containerorchestration" >Container orchestration&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#OppretteAKS" >Kom i gang med Azure Kubernetes - AKS&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Forbehold" >Forbehold&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Forberedelser" >Forberedelser&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Azureinnlogging" >Azure innlogging&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#AktiverContainerService" >Aktiver ContainerService&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#OpprettResourceGroup" >Opprett en resource group&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#OpprettK8sCluster" >Opprette Kubernetes cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#InstallerKubectl" >Installer kubectl&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#InspiserCluster" >Inspiser cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#StarteNginx" >Starte noen nginx containere&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#NginxService" >Gjøre nginx tilgjengelig med en tjeneste&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#ScaleCluster" >Skalere cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#DeleteCluster" >Slette cluster&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Bonusmateriale" >Bonusmateriale&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#HelmIntro" >Rulle ut tjenester med Helm pakker&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#HelmMinecraft" >MineCraft server med Helm&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#KubernetesDashboard" >Kubernetes Dashboard&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Konklusjon" >Konklusjon&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="microsoft-azure">Microsoft Azure
&lt;/h4>&lt;blockquote>
&lt;p>&lt;a class="link" href="https://azure.microsoft.com/en-us/free/" target="_blank" rel="noopener"
>Hvis du ikke har Azure fra før kan du prøve tjenester for $200 i 30 dager.&lt;/a> VM typen &lt;strong>Standard_B2s&lt;/strong> er Burstable, har 2vCPU, 4GB RAM, 8GB temp storage og koster ~$38 / mnd. For $200 kan du ha et cluster på 3-4 B2s noder plus trafikkostnad, lastbalanserere og andre nødvendige tjenester.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;em>Vi har ingen tilknytning til Microsoft bortsett fra at de sponser vår startup &lt;a class="link" href="http://www.datadynamics.no/" target="_blank" rel="noopener"
>DataDynamics&lt;/a> med cloud-tjenester i 24 mnd i deres &lt;a class="link" href="https://bizspark.microsoft.com/" target="_blank" rel="noopener"
>BizSpark program&lt;/a>.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Bakgrunn">&lt;/a>&lt;/p>
&lt;h2 id="bakgrunn">Bakgrunn
&lt;/h2>&lt;p>&lt;a id="Dockercontainers">&lt;/a>&lt;/p>
&lt;h3 id="docker-containers">Docker containers
&lt;/h3>&lt;p>&lt;em>Vi tar ikke for oss Docker containers i dybden i denne posten, men her er en kort oppsummering for de som ikke er kjent med teknologien.&lt;/em>&lt;/p>
&lt;p>Docker er en måte å pakketere programvare slik at det kan kjøres på samtlige populære platformer uten å måtte bruke mye tid på dependencies, oppsett og konfigurasjon.&lt;/p>
&lt;p>I tillegg bruker en Docker container operativsystemet på vertsmaskinen når den kjører. Dette gjør at en kan kjøre mange flere containere på samme vertsmaskin sammenlignet med virtuelle maskiner.&lt;/p>
&lt;p>Her er en ufullstendig og grov sammenligning mellom en Docker container og en virtuell maskin:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Virtuel maskin&lt;/th>
&lt;th>Docker container&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Image størrelse&lt;/td>
&lt;td>fra 200MB til mange GB&lt;/td>
&lt;td>fra 10MB til 3-400MB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Oppstartstid&lt;/td>
&lt;td>60 sekunder +&lt;/td>
&lt;td>1-10 sekunder&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Minnebruk&lt;/td>
&lt;td>256MB-512MB-1GB +&lt;/td>
&lt;td>2MB +&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sikkerhet&lt;/td>
&lt;td>God isolasjon mellom VM&lt;/td>
&lt;td>Dårligere isolasjon mellom containere&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Bygge image&lt;/td>
&lt;td>Minutter&lt;/td>
&lt;td>Sekunder&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> Tallene for virtuelle maskiner er tatt fra hukommelsen. Jeg forsøkte å starte en MySQL virtuell appliance på min laptop men VMware Player nekter å kjøre pga inkompatibilitet med Windows Hyper-V. VMware Workstation nekter å kjøre pga utgått lisens og Oracle VirtualBox gir en nasty bluescreen gang på gang. Hooray!&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong> De minste og raskeste Docker imagene er bygget på Alpine Linux. For webserveren Nginx er det Alpine-baserte imaget 15MB mot det Debian-baserte imaget på 108MB. PostgreSQL:Alpine er 38MB mot 287MB. Siste versjon av MySQL er 343MB men vil i versjon 8 støtte Alpine Linux også.&lt;/p>
&lt;/blockquote>
&lt;p>Noen av fordelene med Docker containers er altså:&lt;/p>
&lt;ul>
&lt;li>Kompatibilitet på tvers av platformer, Linux, Windows og MacOS.&lt;/li>
&lt;li>10-100x mindre størrelse. Raskere å laste ned, raskere å bygge, raskere å laste opp.&lt;/li>
&lt;li>Minnebruk kun for applikasjon og ikke eget OS.
&lt;ul>
&lt;li>Fordel under utvikling, kan kjøre 10-20-30 Docker containere samtidig på en laptop.&lt;/li>
&lt;li>Fordel i produksjon, kan redusere hardware utgifter betraktelig.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Oppstart på få sekunder. Gjør dynamisk skalering av applikasjoner mye enklere.&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://store.docker.com/editions/community/docker-ce-desktop-windows" target="_blank" rel="noopener"
>Last ned Docker for Windows her.&lt;/a>&lt;/p>
&lt;p>Og start en MySQL database fra Windows CMD eller Powershell:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">docker run --name mysql -p 3306:3306 -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Stop containeren med:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">docker kill mysql
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>En kan søke etter ferdige Docker images på &lt;a class="link" href="https://hub.docker.com/" target="_blank" rel="noopener"
>Docker Hub&lt;/a>. Det er også mulig å lage private Docker repositories for egen programvare som ikke skal være tilgjengelig for omverden.&lt;/p>
&lt;p>&lt;a id="Containerorchestration">&lt;/a>&lt;/p>
&lt;h4 id="container-orchestration">Container orchestration
&lt;/h4>&lt;p>Etter som Docker containers har blitt den foretrukne måten å pakke og distribuere programvare på Linux platformen de siste par årene har det vokst frem et behov for systemer som kan samkjøre drift og utrulling av disse containerene. Ikke ulikt det økosystemet av produkter VMware har bygget opp rundt utvikling og drift av virtuelle maskiner.&lt;/p>
&lt;p>Container orchestration systemene har som oppgave å sørge for:&lt;/p>
&lt;ul>
&lt;li>Lastbalansering.&lt;/li>
&lt;li>Service discovery.&lt;/li>
&lt;li>Health checks.&lt;/li>
&lt;li>Automatisk skalering og restarting av vertsmaskiner og containere.&lt;/li>
&lt;li>Oppgraderinger uten nedetid (rolling deploy).&lt;/li>
&lt;/ul>
&lt;p>Frem til nylig har økosystemet rundt container orchestration vært fragmentert og de mest populære alternativene har vært:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://kubernetes.io/" target="_blank" rel="noopener"
>Kubernetes&lt;/a> (Opprinnelig fra Google, nå styrt av CNCF, Cloud Native Computing Foundation)&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.docker.com/engine/swarm/" target="_blank" rel="noopener"
>Swarm&lt;/a> (Fra produsenten bak Docker)&lt;/li>
&lt;li>&lt;a class="link" href="http://mesos.apache.org/" target="_blank" rel="noopener"
>Mesos&lt;/a> (Fra Apache Software Foundation)&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/coreos/fleet" target="_blank" rel="noopener"
>Fleet&lt;/a> (Fra CoreOS)&lt;/li>
&lt;/ul>
&lt;p>Men det siste året har det vært en konvergens mot Kubernetes som foretrukket løsning.&lt;/p>
&lt;ul>
&lt;li>7 februar
&lt;ul>
&lt;li>&lt;a class="link" href="https://coreos.com/blog/migrating-from-fleet-to-kubernetes.html" target="_blank" rel="noopener"
>CoreOS annonserer at de fjerner Fleet fra Container Linux og anbefaler Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>27 juli
&lt;ul>
&lt;li>&lt;a class="link" href="https://azure.microsoft.com/en-us/blog/announcing-cncf/" target="_blank" rel="noopener"
>Microsoft slutter seg til CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>9 august
&lt;ul>
&lt;li>&lt;a class="link" href="https://techcrunch.com/2017/08/09/aws-joins-the-cloud-native-computing-foundation/" target="_blank" rel="noopener"
>Amazon Web Services slutter seg til CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>29 august
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.geekwire.com/2017/now-vmware-pivotal-cncf-becoming-hub-enterprise-tech/" target="_blank" rel="noopener"
>VMware og Pivotal slutter seg til CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>17 september
&lt;ul>
&lt;li>&lt;a class="link" href="https://techcrunch.com/2017/09/13/oracle-joins-the-cloud-native-computing-foundation-as-a-platinum-member/" target="_blank" rel="noopener"
>Oracle slutter seg til CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>17 oktober
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.theregister.co.uk/2017/10/17/docker_ee_kubernetes_support/" target="_blank" rel="noopener"
>Docker annonserer native støtte for Kubernetes i tillegg til sitt eget Swarm produkt&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>24 oktober
&lt;ul>
&lt;li>&lt;a class="link" href="https://azure.microsoft.com/en-us/blog/introducing-azure-container-service-aks-managed-kubernetes-and-azure-container-registry-geo-replication/" target="_blank" rel="noopener"
>Microsoft Azure annonserer managed Kubernetes med tjenesten AKS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>29 november
&lt;ul>
&lt;li>&lt;a class="link" href="https://aws.amazon.com/blogs/aws/amazon-elastic-container-service-for-kubernetes/" target="_blank" rel="noopener"
>Amazon Web Services annonserer managed Kubernetes med tjenesten EKS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>De to siste nyhetene er spesielt viktige. Å drifte sin egen Kubernetes-installasjon krever tid og kompetanse. (&lt;a class="link" href="https://stripe.com/blog/operating-kubernetes" target="_blank" rel="noopener"
>Les hvordan Stripe brukte 5 måneder på å bli fortrolig med å drifte sitt eget Kubernetes cluster, bare for batch jobs.&lt;/a>)&lt;/p>
&lt;p>Frem til nå har valget vært mellom å drifte sitt eget Kubernetes cluster eller bruke Google Container Engine som har &lt;a class="link" href="https://cloudplatform.googleblog.com/2014/11/unleashing-containers-and-kubernetes-with-google-compute-engine.html" target="_blank" rel="noopener"
>brukt Kubernetes siden 2014&lt;/a>. Mange av oss føler et visst ubehag ved å låse oss til én tilbyder. Men dette er nå anderledes når en kan utvikle infrastruktur på Kubernetes, og velge tilnærmet fritt &lt;strong>*&lt;/strong> mellom de 3 store cloud-tilbyderene i tillegg til å drifte selv om ønskelig.&lt;/p>
&lt;p>&lt;strong>*&lt;/strong> Kubernetes utvikles raskt, og funksjonalitet blir ofte ikke tilgjengelig på de ulike platformene samtidig.&lt;/p>
&lt;p>&lt;a id="OppretteAKS">&lt;/a>&lt;/p>
&lt;h2 id="opprette-azure-kubernetes-cluster">Opprette Azure Kubernetes Cluster
&lt;/h2>&lt;p>&lt;a id="Forbehold">&lt;/a>&lt;/p>
&lt;h3 id="forbehold">Forbehold
&lt;/h3>&lt;blockquote>
&lt;p>Denne gjennomgangen tar utgangspunkt i dokumentasjonen på &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough" target="_blank" rel="noopener"
>Microsoft.com&lt;/a>. Å sette opp et Azure Kubernetes cluster fungerte ikke i starten av desember, men per dags dato, 23. desember, ser det ut til å fungere relativt bra. Men, oppgradering av cluster fra Kubernetes 1.7 til 1.8 fungerer for eksempel IKKE.&lt;/p>
&lt;p>AKS er i Preview og Azure jobber kontinuerlig med å gjøre AKS stabilt og støtte så mange Kubernetes-funksjoner som mulig. Amazon Web Services har tilsvarende en lukket invite-only Preview per dags dato mens de også jobber med stabilitet og funksjonalitet.&lt;/p>
&lt;p>Både Azure og AWS uttrykker forventning om at deres Kubernetes tjenester skal være klare for produksjonsmiljø ila 2018.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Forberedelser">&lt;/a>&lt;/p>
&lt;h3 id="forberedelser">Forberedelser
&lt;/h3>&lt;p>Du behøver Azure-CLI (versjon 2.0.21 eller nyere) for å utføre kommandoene:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://aka.ms/InstallAzureCliWindows" target="_blank" rel="noopener"
>Last ned Azure-CLI her&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest" target="_blank" rel="noopener"
>Informasjon om Azure-CLI på MacOS og Linux finner du her&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Alle kommandoer gjøres i Windows PowerShell.&lt;/p>
&lt;p>&lt;a id="Azureinnlogging">&lt;/a>&lt;/p>
&lt;h3 id="azure-innlogging">Azure innlogging
&lt;/h3>&lt;p>Logg på Azure:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az login
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Du får en link som du åpner i din browser samt en autentiseringskode. Skriv koden på nettsiden og &lt;code>az login&lt;/code> lagrer påloggingsinformasjonen slik at du ikke behøver å autentisere igjen på samme maskin.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> Pålogingsinformasjonen lagres i &lt;code>C:\Users\Brukernavn\.azure\&lt;/code>. Du må selv passe på at ingen kopierer disse filene. Da får de full tilgang til din Azure konto.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="AktiverContainerService">&lt;/a>&lt;/p>
&lt;h3 id="aktiver-containerservice">Aktiver ContainerService
&lt;/h3>&lt;p>Siden AKS er i Preview/Beta må du eksplisitt aktivere det for å få tilgang til &lt;code>aks&lt;/code> kommandoene.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az provider register -n Microsoft.ContainerService
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">az provider show -n Microsoft.ContainerService
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="OpprettResourceGroup">&lt;/a>&lt;/p>
&lt;h3 id="opprett-en-resource-group">Opprett en resource group
&lt;/h3>&lt;p>Her oppretter vi en resource group med navn &amp;ldquo;min_aks_rg&amp;rdquo; i Azure region West Europe.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az group create --name min_aks_rg --location westeurope
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong>
For å se en liste over tilgjengelige Azure regioner, bruk kommandoen &lt;code>az account list-locations --output table&lt;/code>. &lt;strong>PS&lt;/strong> Det kan hende AKS ikke er tilgjengelig i alle regioner enda.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="OpprettK8sCluster">&lt;/a>&lt;/p>
&lt;h3 id="opprette-kubernetes-cluster">Opprette Kubernetes cluster
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks create --resource-group min_aks_rg --name mitt_cluster --node-count 3 --generate-ssh-keys --node-vm-size Standard_B2s --node-osdisk-size 256 --kubernetes-version 1.8.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;code>--node-count&lt;/code>
&lt;ul>
&lt;li>Antall vertsmaskiner tilgjengelig for å kjøre containers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--generate-ssh-keys&lt;/code>
&lt;ul>
&lt;li>Oppretter og outputter en SSH key som kan brukes for å SSHe direkte til vertsmaskinene.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--node-vm-size&lt;/code>
&lt;ul>
&lt;li>Hvilken type Azure VM clusteret skal bestå av. For å se tilgjengelige størrelser bruk &lt;code>az vm list-sizes -l westeurope --output table&lt;/code> og &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes" target="_blank" rel="noopener"
>Microsofts nettsider.&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--node-osdisk-size&lt;/code>
&lt;ul>
&lt;li>Disk størrelse på vertsmaskiner i GB. &lt;strong>PS&lt;/strong> Conteinere kan bli stoppet og flyttet til en annen host ved behov eller hvis en vertsmaskin forsvinner. Alle data lagret lokalt i conteineren blir da borte. Hvis en skal lagre ting permanent må en bruke PersistentVolumes og ikke lokal disk på vertsmaskin.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--kubernetes-version&lt;/code>
&lt;ul>
&lt;li>Hvilken Kubernetes versjon som skal installeres. Azure installerer IKKE den siste versjonen som standard, og per dags dato fungerer ikke &lt;code>az aks upgrade&lt;/code> tilstrekkelig. Siste tilgjengelige versjon per dags dato er 1.8.2. Det er en fordel å bruke siste versjon da det skjer store forbedringer i Kubernetes fra versjon til versjon. Dokumentasjon er også mye bedre for nyere versjoner.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Lagre teksten som kommandoen spytter ut i en fil på en trygg plass. Den inneholder nøkler som kan brukes for å kople til clusteret med SSH. Selv om det i teorien ikke skal være nødvendig.&lt;/p>
&lt;p>&lt;a id="InstallerKubectl">&lt;/a>&lt;/p>
&lt;h3 id="installer-kubectl">Installer kubectl
&lt;/h3>&lt;p>&lt;code>kubectl&lt;/code> er klienten som gjør alle operasjoner mot ditt Kubernetes cluster. Azure CLI kan installere &lt;code>kubectl&lt;/code> for deg:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks install-cli
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Etter &lt;code>kubectl&lt;/code> er installert behøver vi å få påloggingsinformasjon slik at &lt;code>kubectl&lt;/code> kan kommunisere med Kubernetes clusteret.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks get-credentials --resource-group min_aks_rg --name mitt_cluster
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Påloggingsinformasjonen lagres i &lt;code>C:\Users\Brukernavn\.kube\config&lt;/code>. Hold disse filene hemmelig også.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong> Når en har flere ulike Kubernetes clusters kan en bytte hvilken &lt;code>kubectl&lt;/code> skal snakke til med &lt;code>kubectl config get-contexts&lt;/code> og &lt;code>kubectl config set-context mitt_cluster&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="InspiserCluster">&lt;/a>&lt;/p>
&lt;h3 id="inspiser-cluster">Inspiser cluster
&lt;/h3>&lt;p>For å se at clusteret og &lt;code>kubectl&lt;/code> virker begynner vi med noen kommandoer.&lt;/p>
&lt;p>Se alle vertsmaskiner og status:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get nodes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME STATUS AGE VERSION
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-0 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-1 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-2 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Se alle tjenester, pods, deployments:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get all --all-namespaces
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system po/kubernetes-dashboard-6fc8cf9586-frpkn 1/1 Running 0 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system svc/kubernetes-dashboard 10.0.161.132 &amp;lt;none&amp;gt; 80/TCP 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system deploy/kubernetes-dashboard 1 1 1 1 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME DESIRED CURRENT READY AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system rs/kubernetes-dashboard-6fc8cf9586 1 1 1 3d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Jeg har bare tatt et lite utdrag fra denne kommandoen. Du behøver ikke å forstå hva alle ressursene i &lt;code>kube-system&lt;/code> namespacet gjør. Det er hensikten at du skal slippe det når Microsoft står for management av selve clusteret.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Namespaces&lt;/strong>
I Kubernetes er det noe som heter Namespaces. Ressurser i ett namespace har ikke automatisk tilgang til ressurser i et annet namespace. Tjenestene som Kubernetes selv benytter installeres i namespacet &lt;code>kube-system&lt;/code>. Kommandoen &lt;code>kubectl&lt;/code> viser deg vanligvis bare ressurser i &lt;code>default&lt;/code> namespace med mindre du spesifiserer &lt;code>--all-namespaces&lt;/code> eller &lt;code>--namespace=xx&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="StarteNginx">&lt;/a>&lt;/p>
&lt;h3 id="starte-noen-nginx-containere">Starte noen nginx containere
&lt;/h3>&lt;blockquote>
&lt;p>En instans av en kjørende container kalles i Kubernetes for en &lt;strong>Pod&lt;/strong>.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;code>nginx&lt;/code> er en rask og fleksibel webserver.&lt;/p>
&lt;/blockquote>
&lt;p>Nå som clusteret er oppe å kjøre kan vi begynne å rulle ut tjenster og deployments på det.&lt;/p>
&lt;p>Vi begynner med å lage en Deployment bestående av 3 containere som alle kjører &lt;code>nginx:mainline-alpine&lt;/code> imaget fra &lt;a class="link" href="https://hub.docker.com/r/_/nginx/" target="_blank" rel="noopener"
>Docker hub&lt;/a>.&lt;/p>
&lt;p>&lt;strong>nginx-dep.yaml&lt;/strong> ser slik ut:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">apiVersion: apps/v1beta2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kind: Deployment
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: nginx-deployment
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> replicas: 3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> selector:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> matchLabels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> template:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> containers:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - name: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> image: nginx:mainline-alpine
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ports:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - containerPort: 80
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Last denne inn på clusteret med &lt;code>kubectl create&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-dep.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Denne kommandoen oppretter ressursene beskrevet i filen. &lt;code>kubectl&lt;/code> kan lese filer enten lokalt fra din maskin eller fra en URL.&lt;/p>
&lt;blockquote>
&lt;p>Etter du har gjort endringer i en ressurs-definisjon (&lt;code>.yaml&lt;/code> fil) kan du oppdatere ressursene i clusteret med &lt;code>kubectl replace -f ressurs.yaml&lt;/code>&lt;/p>
&lt;/blockquote>
&lt;p>Vi kan verifisere at Deployment er klar:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get deploy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment 3 3 3 3 10m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Vi kan også hente de faktiske Pods som er startet:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get pods
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-dqwx5 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-xwzpw 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-z5tfk 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Logger&lt;/strong> Vi kan se logger fra én pod med &lt;code>kubectl logs nginx-deployment-569477d6d8-xwzpw&lt;/code>. Men siden vi i dette tilfellet ikke vet hvilken Pod som ender opp med å få innkommende forespørsler kan vi se logger fra alle Pods som har &lt;code>app=nginx&lt;/code> label: &lt;code>kubectl logs -lapp=nginx&lt;/code>. At vi her bruker &lt;code>app=nginx&lt;/code> har vi selv bestemt i &lt;code>nginx-dep.yaml&lt;/code> når vi satt &lt;code>spec.template.metadata.labels: app: nginx&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="NginxService">&lt;/a>&lt;/p>
&lt;h3 id="gjøre-nginx-tilgjengelig-med-en-tjeneste">Gjøre nginx tilgjengelig med en tjeneste
&lt;/h3>&lt;p>For å kommunisere med våre nye Pods behøver vi å opprette en tjeneste (&lt;strong>Service&lt;/strong>). En tjeneste består av en eller flere Pods som velges basert på ulike kriterier, blant annet hvilke labels de har og om Podene det gjelder er Running og Ready.&lt;/p>
&lt;p>Nå lager vi en tjeneste som ruter trafikk til alle Pods som har label &lt;code>app: nginx&lt;/code> og som lytter på port 80. I tillegg gjør vi tjenesten tilgjengelig via en LoadBalancer:&lt;/p>
&lt;p>&lt;strong>nginx-svc.yaml&lt;/strong> ser slik ut:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">apiVersion: v1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kind: Service
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> type: LoadBalancer
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ports:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - port: 80
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: http
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> targetPort: 80
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> selector:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Vi ber Kubernetes om å opprette tjeneten vår med &lt;code>kubectl create&lt;/code> som vanlig:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-svc.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Deretter kan vi se hvilken IP-adresse tjenesten vår har fått av Azure:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get svc -w
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx 10.0.24.11 13.95.173.255 80:31522/TCP 15m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> Det kan ta et par minutter for Azure å tildele tjenesten vår en Public IP, i mellomtiden vil det stå &lt;code>&amp;lt;pending&amp;gt;&lt;/code> under EXTERNAL-IP.&lt;/p>
&lt;/blockquote>
&lt;p>En enkel &lt;strong>Welcome to nginx&lt;/strong> webside skal nå være tilgjengelig på http://13.95.173.255 (&lt;em>husk å bytt ut med din egen External-IP&lt;/em>).&lt;/p>
&lt;p>&lt;strong>Vi har nå en lastbalansert &lt;code>nginx&lt;/code> tjeneste med 3 servere klar til å ta imot trafikk.&lt;/strong>&lt;/p>
&lt;p>For ordens skyld kan vi slette tjeneste og deployment etterpå:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl delete svc nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete deploy nginx-deployment
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="ScaleCluster">&lt;/a>&lt;/p>
&lt;h3 id="skalere-cluster">Skalere cluster
&lt;/h3>&lt;p>Hvis en ønsker å endre antall vertsmaskiner/noder som kjører Pods kan en gjøre det via Azure-CLI:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks scale --name mitt_cluster --resource-group min_aks_rg --node-count 5
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>For øyeblikket blir alle noder opprettet med samme størrelse som når clusteret ble opprettet. AKS vil antageligvis få støtte for &lt;a class="link" href="https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools" target="_blank" rel="noopener"
>&lt;strong>node-pools&lt;/strong>&lt;/a> i løpet av neste år. Da kan en opprette grupper av noder med forskjellig størrelse og operativsystem, både Linux og Windows.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="DeleteCluster">&lt;/a>&lt;/p>
&lt;h3 id="slette-cluster">Slette cluster
&lt;/h3>&lt;p>En kan slette hele clusteret slik:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks delete --name mitt_cluster --resource-group min_aks_rg --yes
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="Bonusmateriale">&lt;/a>&lt;/p>
&lt;h2 id="bonusmateriale">Bonusmateriale
&lt;/h2>&lt;p>Her er litt bonusmateriale dersom du ønsker å gå enda litt videre med Kubernetes.&lt;/p>
&lt;p>&lt;a id="HelmIntro">&lt;/a>&lt;/p>
&lt;h3 id="rulle-ut-tjenester-med-helm">Rulle ut tjenester med Helm
&lt;/h3>&lt;p>&lt;a class="link" href="https://helm.sh/" target="_blank" rel="noopener"
>Helm&lt;/a> er en pakke-behandler og et bibliotek av programvare som er klart for å rulles ut i et Kubernetes-cluster.&lt;/p>
&lt;p>Start med å laste ned &lt;a class="link" href="https://github.com/kubernetes/helm/releases" target="_blank" rel="noopener"
>Helm-klienten&lt;/a>. Den henter påloggingsinformasjon osv fra samme sted som &lt;code>kubectl&lt;/code> automatisk.&lt;/p>
&lt;p>Installer Helm-serveren (Tiller) på Kubernetes clusteret og oppdater pakke-biblioteket:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">helm init
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm repo update
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Se tilgjengelige pakker (&lt;strong>Charts&lt;/strong>) med: &lt;code>helm search&lt;/code>.&lt;/p>
&lt;p>&lt;a id="HelmMinecraft">&lt;/a>&lt;/p>
&lt;h4 id="rulle-ut-minecraft-med-helm">Rulle ut MineCraft med Helm
&lt;/h4>&lt;p>La oss rulle ut en MineCraft installasjon på clusteret vårt, fordi vi kan :-)&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">helm install --name stian-sin --set minecraftServer.eula=true stable/minecraft
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;code>--set&lt;/code> overstyrer en eller flere av standardverdiene som er satt i pakken. MineCraft pakken er laget slik at den ikke starter uten å ha sagt seg enig i brukervilkårene i variabelen &lt;code>minecraftServer.eula&lt;/code>. Alle variablene som kan overstyres i MineCraft pakken er &lt;a class="link" href="https://github.com/kubernetes/charts/blob/master/stable/minecraft/values.yaml" target="_blank" rel="noopener"
>dokumentert her&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>Så venter vi litt på at Azure skal tildele en Public IP:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get svc -w
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">stian-sin-minecraft 10.0.237.0 13.95.172.192 25565:30356/TCP 3m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Og vipps kan vi kople til Minecraft på &lt;code>13.95.172.192:25565&lt;/code>.&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/minecraft-k8s.png"
width="856"
height="507"
srcset="https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/minecraft-k8s_hu13197832490752396686.png 480w, https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/minecraft-k8s_hu16352153299487520650.png 1024w"
loading="lazy"
alt="Kubernetes in MineCraft on Kubernetes"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="405px"
>&lt;/p>
&lt;p>&lt;a id="KubernetesDashboard">&lt;/a>&lt;/p>
&lt;h3 id="kubernetes-dashboard">Kubernetes Dashboard
&lt;/h3>&lt;p>Kubernetes har også et grafisk web-grensesnitt som gjør det litt lettere å se hvilke ressurser som er i clusteret, se logger og åpne remote-shell inne i en kjørende Pod, blant annet.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl proxy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Starting to serve on 127.0.0.1:8001
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>kubectl&lt;/code> krypterer og tunnelerer trafikken inn til Kubernetes&amp;rsquo; API servere. Dashboardet er tilgjengelig på &lt;a class="link" href="http://127.0.0.1:8001/ui/" target="_blank" rel="noopener"
>http://127.0.0.1:8001/ui/&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/k8s-dash.png"
width="899"
height="588"
srcset="https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/k8s-dash_hu15091082541702636396.png 480w, https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/k8s-dash_hu17318776918264020386.png 1024w"
loading="lazy"
alt="Kubernetes Dashboard"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>&lt;/p>
&lt;p>&lt;a id="Konklusjon">&lt;/a>&lt;/p>
&lt;h2 id="konklusjon">Konklusjon
&lt;/h2>&lt;p>Jeg håper du har fått mersmak for Kubernetes. Lærekurven kan være litt bratt i begynnelsen men det tar ikke så veldig lang tid før du er produktiv.&lt;/p>
&lt;p>Se på de &lt;a class="link" href="https://v1-8.docs.kubernetes.io/docs/tutorials/" target="_blank" rel="noopener"
>offisielle guidene på Kubernetes.io&lt;/a> for å lære mer om hvordan du definerer forskjellige typer ressurser og tjenester for å kjøre på Kubernetes. &lt;strong>PS: Det gjøres store endringer fra versjon til versjon så sørg for å bruke dokumentasjonen for riktig versjon!&lt;/strong>&lt;/p>
&lt;p>Kubernetes har også et veldig aktivt Slack-miljø på &lt;a class="link" href="http://slack.k8s.io/" target="_blank" rel="noopener"
>kubernetes.slack.com&lt;/a>. Der er det også en kanal for norske Kubernetes brukere; &lt;strong>#norw-users&lt;/strong>.&lt;/p></description></item></channel></rss>