<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes on blog.stian.omg.lol</title><link>https://blog.stian.omg.lol/tags/kubernetes/</link><description>Recent content in Kubernetes on blog.stian.omg.lol</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 25 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.stian.omg.lol/tags/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>Configuring Envoy as an edge proxy - through istio</title><link>https://blog.stian.omg.lol/p/configuring-envoy-as-an-edge-proxy-through-istio/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://blog.stian.omg.lol/p/configuring-envoy-as-an-edge-proxy-through-istio/</guid><description>&lt;img src="https://blog.stian.omg.lol/p/configuring-envoy-as-an-edge-proxy-through-istio/envoy-logo.svg" alt="Featured image of post Configuring Envoy as an edge proxy - through istio" />&lt;ul>
&lt;li>&lt;a class="link" href="#introduction" >Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#custom-bootstrap" >Configuring overload manager and global connection limits using a custom Envoy bootstrap&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#envoyfilter" >Configuring buffer sizes and connection timeouts via EnvoyFilter&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#appendices" >Appendices&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#appendix-get-envoy-config" >Appendix A - Displaying currently active Envoy edge configuration settings&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#appendix-overload-manager-metrics" >Appendix B - Overload manager metrics&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#appendix-istio-at-signicat" >Appendix C - istio installation and configuration at Signicat&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#outro" >Outro&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>This is a cross-post of a blog post also published on the &lt;a class="link" href="" >Signicat Blog&lt;/a>&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>We deployed this with Istio 1.23 and the Envoy edge proxy recommendations as of November 2024.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="introduction">&lt;/a>&lt;/p>
&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>If you are using istio as a Service Mesh for your Kubernetes clusters, chances are you are also using &lt;code>istio-ingressgateway&lt;/code> to handle incoming traffic from the Internet.&lt;/p>
&lt;p>Envoy however, which istio relies on, is not tuned for running at the edge by default:&lt;/p>
&lt;blockquote>
&lt;p>Envoy is a production-ready edge proxy, however, the default settings are tailored for the service mesh use case, and some values need to be adjusted when using Envoy as an edge proxy.
&lt;br>— &lt;cite>&lt;a class="link" href="https://www.envoyproxy.io/docs/envoy/latest/configuration/best_practices/edge" target="_blank" rel="noopener"
>envoyproxy.io/docs&lt;/a>&lt;/cite>&lt;/p>
&lt;/blockquote>
&lt;p>The &lt;a class="link" href="https://www.envoyproxy.io/docs/envoy/latest/configuration/best_practices/edge" target="_blank" rel="noopener"
>Envoy edge proxy best practices&lt;/a> document outlines specific recommended configuration parameters for running envoy (and thus &lt;code>istio-ingressgateway&lt;/code>) on the edge.&lt;/p>
&lt;p>It&amp;rsquo;s not immediately obvious how you would propagate these configurations through the regular istio installation and configuration procedures. There is an &lt;a class="link" href="https://github.com/istio/istio/issues/24715" target="_blank" rel="noopener"
>open feature request on GitHub&lt;/a> asking for the ability to configure &lt;code>istio-ingressgateway&lt;/code> according to best practices.&lt;/p>
&lt;p>Here I&amp;rsquo;ll show you at least one way of getting these settings deployed.&lt;/p>
&lt;hr>
&lt;p>We need two different approaches to achieve our goals. A &lt;a class="link" href="https://github.com/istio/istio/tree/master/samples/custom-bootstrap" target="_blank" rel="noopener"
>custom bootstrap configuration&lt;/a> and an &lt;a class="link" href="https://istio.io/latest/docs/reference/config/networking/envoy-filter/" target="_blank" rel="noopener"
>EnvoyFilter&lt;/a>.&lt;/p>
&lt;p>&lt;a id="custom-bootstrap">&lt;/a>&lt;/p>
&lt;h1 id="configuring-overload-manager-and-global-connection-limits-using-a-custom-envoy-bootstrap">Configuring overload manager and global connection limits using a custom Envoy bootstrap
&lt;/h1>&lt;p>To enable/configure &lt;a class="link" href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/overload_manager" target="_blank" rel="noopener"
>Envoy overload manager&lt;/a> and global connection limits we first create our config file:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;span class="lnt">60
&lt;/span>&lt;span class="lnt">61
&lt;/span>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;span class="lnt">64
&lt;/span>&lt;span class="lnt">65
&lt;/span>&lt;span class="lnt">66
&lt;/span>&lt;span class="lnt">67
&lt;/span>&lt;span class="lnt">68
&lt;/span>&lt;span class="lnt">69
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ConfigMap&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">istio-envoy-custom-bootstrap-config&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">namespace&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">istio-system&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">data&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">custom_bootstrap.yaml&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">|&lt;/span>&lt;span class="sd">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> # Untrusted downstreams:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> overload_manager:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> refresh_interval: 0.25s
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> resource_monitors:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - name: &amp;#34;envoy.resource_monitors.fixed_heap&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> typed_config:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> &amp;#34;@type&amp;#34;: type.googleapis.com/envoy.extensions.resource_monitors.fixed_heap.v3.FixedHeapConfig
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> max_heap_size_bytes: 350000000 # 350000000=350MB
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - name: &amp;#34;envoy.resource_monitors.global_downstream_max_connections&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> typed_config:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> &amp;#34;@type&amp;#34;: type.googleapis.com/envoy.extensions.resource_monitors.downstream_connections.v3.DownstreamConnectionsConfig
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> max_active_downstream_connections: 25000
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> actions:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> # Possible actions: https://www.envoyproxy.io/docs/envoy/latest/configuration/operations/overload_manager/overload_manager#overload-actions
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - name: &amp;#34;envoy.overload_actions.shrink_heap&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> triggers:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - name: &amp;#34;envoy.resource_monitors.fixed_heap&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> threshold:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> value: 0.9
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - name: &amp;#34;envoy.overload_actions.stop_accepting_requests&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> triggers:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - name: &amp;#34;envoy.resource_monitors.fixed_heap&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> threshold:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> value: 0.95
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> # Additional settings from https://www.envoyproxy.io/docs/envoy/latest/configuration/operations/overload_manager/overload_manager
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - name: &amp;#34;envoy.overload_actions.disable_http_keepalive&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> triggers:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - name: &amp;#34;envoy.resource_monitors.fixed_heap&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> threshold:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> value: 0.95
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> # From https://www.envoyproxy.io/docs/envoy/latest/configuration/operations/overload_manager/overload_manager#reducing-timeouts
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - name: &amp;#34;envoy.overload_actions.reduce_timeouts&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> triggers:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - name: &amp;#34;envoy.resource_monitors.fixed_heap&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> scaled:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> scaling_threshold: 0.85
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> saturation_threshold: 0.95
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> typed_config:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> &amp;#34;@type&amp;#34;: type.googleapis.com/envoy.config.overload.v3.ScaleTimersOverloadActionConfig
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> timer_scale_factors:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - timer: HTTP_DOWNSTREAM_CONNECTION_IDLE
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> min_timeout: 2s
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> # https://www.envoyproxy.io/docs/envoy/latest/configuration/operations/overload_manager/overload_manager#load-shed-points
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> loadshed_points:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - name: &amp;#34;envoy.load_shed_points.tcp_listener_accept&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> triggers:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - name: &amp;#34;envoy.resource_monitors.fixed_heap&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> threshold:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> value: 0.95
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> # From https://www.envoyproxy.io/docs/envoy/latest/configuration/best_practices/edge#best-practices-edge / https://www.envoyproxy.io/docs/envoy/latest/configuration/listeners/runtime#config-listeners-runtime
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> # Also mentioned in https://istio.io/latest/news/security/istio-security-2020-007/#mitigation with much higher limits
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> # Here we configure one limit for the &amp;#34;regular&amp;#34; public listener on port 8443 and a separate global limit that is higher to
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> # avoid starving connections for admin and metrics and probes
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> layered_runtime:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> layers:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - name: static_layer_0
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> static_layer:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> envoy:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> resource_limits:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> listener:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> 0.0.0.0_8443:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> connection_limit: 10000&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Let&amp;rsquo;s save it to &lt;code>istio-envoy-custom-bootstrap-config.yaml&lt;/code>.&lt;/p>
&lt;p>&lt;strong>There is a field here you MUST adjust to your environment.&lt;/strong> That is the &lt;code>max_heap_size_bytes&lt;/code> which we set to about 90% of the configured K8s memory limit.&lt;/p>
&lt;p>What this does is inform the overload manager of how much memory it has available, and is used for evaluating percentage of current usage compared to what it thinks it has available, that again triggers overload actions at certain thresholds.&lt;/p>
&lt;p>You may also have to adjust the second to last line (&lt;code>0.0.0.0_8443&lt;/code>) in case your public listener is named something else.&lt;/p>
&lt;p>Now we install it in the cluster:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl apply -n istio-system -f istio-envoy-custom-bootstrap-config.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Then we can use an &lt;a class="link" href="https://istio.io/latest/docs/setup/additional-setup/customize-installation/" target="_blank" rel="noopener"
>overlay&lt;/a> to modify the Deployment that &lt;code>istioctl&lt;/code> produces, before &lt;code>istioctl&lt;/code> actually installs it in the cluster. This is the path and contents of what you need to add to your existing IstioOperator that you feed to &lt;code>istioctl&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">install.istio.io/v1alpha1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">IstioOperator&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">components&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">ingressGateways&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">istio-ingressgateway&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">k8s&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">overlays&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Deployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">istio-ingressgateway&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">patches&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">spec.template.spec.containers.[name:istio-proxy].env[-1]&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ISTIO_BOOTSTRAP_OVERRIDE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/etc/istio/custom-bootstrap/custom_bootstrap.yaml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">spec.template.spec.containers.[name:istio-proxy].volumeMounts[-1]&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">mountPath&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/etc/istio/custom-bootstrap&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">custom-bootstrap-volume&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">readOnly&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">spec.template.spec.volumes[-1]&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">configMap&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">istio-envoy-custom-bootstrap-config&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">defaultMode&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">420&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">optional&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">custom-bootstrap-volume&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>Huge shoutout to our eminent &lt;a class="link" href="https://www.linkedin.com/in/csaba-k%C3%A1rp%C3%A1ti-4a229086/" target="_blank" rel="noopener"
>Csaba Kárpáti&lt;/a> which helped me out actually getting the overlay above work.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="envoyfilter">&lt;/a>&lt;/p>
&lt;h1 id="configuring-buffer-sizes-and-connection-timeouts-via-envoyfilter">Configuring buffer sizes and connection timeouts via EnvoyFilter
&lt;/h1>&lt;p>We set the rest of the recommended configurations via an EnvoyFilter.&lt;/p>
&lt;p>Create a file for it, named &lt;code>listener-filters-edge.yaml&lt;/code> for example:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># Based on recommendations for edge deployments with untrusted downstreams:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c"># - https://www.envoyproxy.io/docs/envoy/latest/configuration/best_practices/edge#best-practices-edge&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c"># - https://www.envoyproxy.io/docs/envoy/latest/faq/configuration/timeouts#faq-configuration-timeouts&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">networking.istio.io/v1alpha3&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">EnvoyFilter&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">listener-filters-edge&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">workloadSelector&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">istio&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ingressgateway&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">configPatches&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">applyTo&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">LISTENER&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">match&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">context&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">GATEWAY&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">patch&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">operation&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">MERGE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">per_connection_buffer_limit_bytes&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">32768&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c"># Doc examples 32 KiB # Default 1MB&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">applyTo&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">NETWORK_FILTER&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">match&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">context&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">GATEWAY&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">listener&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">filterChain&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">filter&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;envoy.filters.network.http_connection_manager&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">patch&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">operation&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">MERGE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;envoy.filters.network.http_connection_manager&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">typed_config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">&amp;#34;@type&amp;#34;: &lt;/span>&lt;span class="s2">&amp;#34;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/network/http_connection_manager/v3/http_connection_manager.proto#envoy-v3-api-field-extensions-filters-network-http-connection-manager-v3-httpconnectionmanager-request-headers-timeout&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">request_headers_timeout&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">10s &lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c"># Default no timeout&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/core/v3/protocol.proto#envoy-v3-api-msg-config-core-v3-http1protocoloptions&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">common_http_protocol_options&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">max_connection_duration&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">60s &lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c"># Default no timeout&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">idle_timeout&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">900s &lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c"># Default 1 hour. Doc example 900s&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">headers_with_underscores_action&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">REJECT_REQUEST&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/core/v3/protocol.proto#config-core-v3-http2protocoloptions&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">http2_protocol_options&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">max_concurrent_streams&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">100&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c"># Default 2147483647&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">initial_stream_window_size&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">65536&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c"># Doc examples 64 KiB - Default 268435456 (256 * 1024 * 1024)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">initial_connection_window_size&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1048576&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c"># Doc examples 1 MiB - Same default as initial_stream_window_size&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/network/http_connection_manager/v3/http_connection_manager.proto.html#extensions-filters-network-http-connection-manager-v3-httpconnectionmanager&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">stream_idle_timeout&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">300s &lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c"># Default 5 mins. Must be disabled for long-lived and streaming requests&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">request_timeout&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">300s &lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c"># Default no timeout. Must be disabled for long-lived and streaming requests&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">use_remote_address&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">normalize_path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">merge_slashes&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">path_with_escaped_slashes_action&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">UNESCAPE_AND_REDIRECT&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>And install it like usual:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl apply -n istio-system -f listener-filters-edge.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="appendices">&lt;/a>&lt;/p>
&lt;h1 id="appendices">Appendices
&lt;/h1>&lt;p>&lt;a id="appendix-get-envoy-config">&lt;/a>&lt;/p>
&lt;h2 id="appendix-a---displaying-currently-active-envoy-edge-configuration-settings">Appendix A - Displaying currently active Envoy edge configuration settings
&lt;/h2>&lt;p>Here is a handy script for printing the currently active settings to console. Useful for verifying the changes have actually made it all the way to Envoy.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;span class="lnt">60
&lt;/span>&lt;span class="lnt">61
&lt;/span>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;span class="lnt">64
&lt;/span>&lt;span class="lnt">65
&lt;/span>&lt;span class="lnt">66
&lt;/span>&lt;span class="lnt">67
&lt;/span>&lt;span class="lnt">68
&lt;/span>&lt;span class="lnt">69
&lt;/span>&lt;span class="lnt">70
&lt;/span>&lt;span class="lnt">71
&lt;/span>&lt;span class="lnt">72
&lt;/span>&lt;span class="lnt">73
&lt;/span>&lt;span class="lnt">74
&lt;/span>&lt;span class="lnt">75
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="nv">CONFIG_FILE&lt;/span>&lt;span class="o">=&lt;/span>igw_config.json
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> &lt;span class="s2">&amp;#34;Looking for pods labeled istio=ingressgateway&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">ISTIO_INGRESSGATEWAY_POD&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">$(&lt;/span>kubectl get pods -n istio-system -l &lt;span class="nv">istio&lt;/span>&lt;span class="o">=&lt;/span>ingressgateway -o &lt;span class="nv">jsonpath&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;{.items[0].metadata.name}&amp;#39;&lt;/span>&lt;span class="k">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> &lt;span class="s2">&amp;#34;Using &lt;/span>&lt;span class="nv">$ISTIO_INGRESSGATEWAY_POD&lt;/span>&lt;span class="s2"> and dumping configuration to &lt;/span>&lt;span class="nv">$CONFIG_FILE&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl &lt;span class="nb">exec&lt;/span> -n istio-system &lt;span class="nv">$ISTIO_INGRESSGATEWAY_POD&lt;/span> curl http://localhost:15000/config_dump &amp;gt; &lt;span class="nv">$CONFIG_FILE&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> &lt;span class="s2">&amp;#34;Custom bootstrap configuration: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;bootstrap.overload_manager.refresh_interval: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq -r &lt;span class="s1">&amp;#39;.configs[0].bootstrap.overload_manager.refresh_interval&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;max_active_downstream_connections: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[0].bootstrap.overload_manager.resource_monitors[] | select(.name == &amp;#34;envoy.resource_monitors.global_downstream_max_connections&amp;#34;) | .typed_config.max_active_downstream_connections&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;max_heap_size_bytes: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[0].bootstrap.overload_manager.resource_monitors[] | select(.name == &amp;#34;envoy.resource_monitors.fixed_heap&amp;#34;) | .typed_config.max_heap_size_bytes&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;overload_actions.shrink_heap: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[0].bootstrap.overload_manager.actions[] | select(.name == &amp;#34;envoy.overload_actions.shrink_heap&amp;#34;) | .triggers[0].threshold.value&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;overload_actions.stop_accepting_requests: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[0].bootstrap.overload_manager.actions[] | select(.name == &amp;#34;envoy.overload_actions.stop_accepting_requests&amp;#34;) | .triggers[0].threshold.value&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;overload_actions.disable_http_keepalive: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[0].bootstrap.overload_manager.actions[] | select(.name == &amp;#34;envoy.overload_actions.disable_http_keepalive&amp;#34;) | .triggers[0].threshold.value&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;overload_actions.reduce_timeouts scaling_threshold: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[0].bootstrap.overload_manager.actions[] | select(.name == &amp;#34;envoy.overload_actions.reduce_timeouts&amp;#34;) | .triggers[0].scaled.scaling_threshold&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;overload_actions.reduce_timeouts saturation_threshold: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[0].bootstrap.overload_manager.actions[] | select(.name == &amp;#34;envoy.overload_actions.reduce_timeouts&amp;#34;) | .triggers[0].scaled.saturation_threshold&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;overload_actions.reduce_timeouts timer_scale_factors timer: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[0].bootstrap.overload_manager.actions[] | select(.name == &amp;#34;envoy.overload_actions.reduce_timeouts&amp;#34;) | .typed_config.timer_scale_factors[0].timer&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;overload_actions.reduce_timeouts timer_scale_factors min_timeout: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[0].bootstrap.overload_manager.actions[] | select(.name == &amp;#34;envoy.overload_actions.reduce_timeouts&amp;#34;) | .typed_config.timer_scale_factors[0].min_timeout&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;load_shed_points.tcp_listener_accept: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[0].bootstrap.overload_manager.loadshed_points[] | select(.name == &amp;#34;envoy.load_shed_points.tcp_listener_accept&amp;#34;) | .triggers[0].threshold.value&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;resource_limits.0.0.0.0_8443.connection_limit: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[0].bootstrap.layered_runtime.layers[] | select(.name == &amp;#34;static_layer_0&amp;#34;) | .static_layer.envoy.resource_limits.listener.&amp;#34;0.0.0.0_8443&amp;#34;.connection_limit&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">echo&lt;/span> &lt;span class="s2">&amp;#34;EnvoyFilter configuration: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;per_connection_buffer_limit_bytes: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[2].dynamic_listeners[] | select(.name == &amp;#34;0.0.0.0_8443&amp;#34;) | .active_state.listener.per_connection_buffer_limit_bytes&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;http2_protocol_options.max_concurrent_streams: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[2].dynamic_listeners[] | select(.name == &amp;#34;0.0.0.0_8443&amp;#34;) | .active_state.listener.filter_chains[0].filters[0].typed_config.http2_protocol_options.max_concurrent_streams&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;http2_protocol_options.initial_stream_window_size: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[2].dynamic_listeners[] | select(.name == &amp;#34;0.0.0.0_8443&amp;#34;) | .active_state.listener.filter_chains[0].filters[0].typed_config.http2_protocol_options.initial_stream_window_size&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;http2_protocol_options.initial_connection_window_size: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[2].dynamic_listeners[] | select(.name == &amp;#34;0.0.0.0_8443&amp;#34;) | .active_state.listener.filter_chains[0].filters[0].typed_config.http2_protocol_options.initial_connection_window_size&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;stream_idle_timeout: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[2].dynamic_listeners[] | select(.name == &amp;#34;0.0.0.0_8443&amp;#34;) | .active_state.listener.filter_chains[0].filters[0].typed_config.stream_idle_timeout&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;request_timeout: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[2].dynamic_listeners[] | select(.name == &amp;#34;0.0.0.0_8443&amp;#34;) | .active_state.listener.filter_chains[0].filters[0].typed_config.request_timeout&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;common_http_protocol_options.idle_timeout: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[2].dynamic_listeners[] | select(.name == &amp;#34;0.0.0.0_8443&amp;#34;) | .active_state.listener.filter_chains[0].filters[0].typed_config.common_http_protocol_options.idle_timeout&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;common_http_protocol_options.max_connection_duration: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[2].dynamic_listeners[] | select(.name == &amp;#34;0.0.0.0_8443&amp;#34;) | .active_state.listener.filter_chains[0].filters[0].typed_config.common_http_protocol_options.max_connection_duration&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">printf&lt;/span> &lt;span class="s2">&amp;#34;request_headers_timeout: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cat &lt;span class="nv">$CONFIG_FILE&lt;/span> &lt;span class="p">|&lt;/span> jq &lt;span class="s1">&amp;#39;.configs[2].dynamic_listeners[] | select(.name == &amp;#34;0.0.0.0_8443&amp;#34;) | .active_state.listener.filter_chains[0].filters[0].typed_config.request_headers_timeout&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Save it to &lt;code>get-edge-config-values.sh&lt;/code> and run it with for example &lt;code>bash get-edge-config-values.sh&lt;/code>.&lt;/p>
&lt;p>&lt;a id="appendix-overload-manager-metrics">&lt;/a>&lt;/p>
&lt;h2 id="appendix-b---overload-manager-metrics">Appendix B - Overload manager metrics
&lt;/h2>&lt;p>Envoy can also export metrics related to overload manager, they can be enabled by adding &lt;code>overload&lt;/code> to approximately this location in the &lt;code>IstioOperator&lt;/code> spec:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">install.istio.io/v1alpha1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">IstioOperator&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">components&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">ingressGateways&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">istio-ingressgateway&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">k8s&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">podAnnotations&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">proxy.istio.io/config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">|-&lt;/span>&lt;span class="sd">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> proxyStatsMatcher:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> inclusionPrefixes:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - &amp;#34;overload&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="appendix-istio-at-signicat">&lt;/a>&lt;/p>
&lt;h2 id="appendix-c---istio-installation-and-configuration-at-signicat">Appendix C - istio installation and configuration at Signicat
&lt;/h2>&lt;p>At Signicat we use &lt;a class="link" href="https://helm.sh/" target="_blank" rel="noopener"
>helm&lt;/a> to template all resources going in to our istio installation. That includes resource types like:&lt;/p>
&lt;ul>
&lt;li>IstioOperator&lt;/li>
&lt;li>EnvoyFilters&lt;/li>
&lt;li>ConfigMaps&lt;/li>
&lt;li>Gateways&lt;/li>
&lt;li>Sidecars
and so on.&lt;/li>
&lt;/ul>
&lt;p>We don&amp;rsquo;t use &lt;code>helm&lt;/code> to install anything. Only to generate a set of manifests that is then used as inputs to the appropriate tools, like feeding generated &lt;code>IstioOperator&lt;/code> manifests to &lt;code>istioctl&lt;/code> and &lt;code>EnvoyFilter&lt;/code> manifests to &lt;code>kubectl&lt;/code>.&lt;/p>
&lt;p>This works fairly well and allows us to have the same set of base manifests with adjustable values and feature sets per environment and using standard &lt;code>helm&lt;/code> that most platform engineers are already familiar with.&lt;/p>
&lt;p>We also have a couple of other tricks to enable us to have zero-downtime blue-green upgrades to &lt;code>istio-ingressgateway&lt;/code> that we may cover in a future post.&lt;/p>
&lt;p>&lt;a id="outro">&lt;/a>&lt;/p>
&lt;h1 id="outro">Outro
&lt;/h1>&lt;p>I hope this was helpful on your journey towards scale, resilience and reliability.&lt;/p>
&lt;p>The next chapter in this saga would be once we complete extensive load testing with the new configurations compared to the defaults as well as trying to find optimal values. And associated Grafana dashboards are always nice!&lt;/p></description></item><item><title>Kubernetes: Don't use NodeLocal DNSCache on GKE (without Cloud DNS)</title><link>https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/</link><pubDate>Wed, 13 Dec 2023 00:00:00 +0000</pubDate><guid>https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/</guid><description>&lt;img src="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/cover.webp" alt="Featured image of post Kubernetes: Don't use NodeLocal DNSCache on GKE (without Cloud DNS)" />&lt;ul>
&lt;li>&lt;a class="link" href="#introduction" >Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#tl-dr" >TL;DR:&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#nodelocal-dnscache-coredns" >NodeLocal DNSCache / CoreDNS&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#kubedns-dnsmasq" >kube-dns / dnsmasq&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#am-i-affected" >Am I affected?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#investigation" >Investigation&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#the-initial-problem" >The initial Problem&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#istio-envoy-metrics" >Istio / Envoy metrics&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#reproducing" >Reproducing and troubleshooting HTTP connection problems&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#troubleshooting-dns" >Troubleshooting potentially slow or broken DNS lookups&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#appendices" >Appendices&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#appendix-enabling-additional-envoy-metrics" >Appendix - Enabling additional envoy metrics&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#appendix-overview-of-dns-on-gke" >Appendix - Overview of DNS on GKE&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#appendix-reducing-dns-lookups" >Appendix - Reducing DNS lookups in Kubernetes and GKE&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#appendix-overview-of-gke-dns-with-nodelocal-dnscache" >Appendix - Overview of GKE DNS with NodeLocal DNSCache&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#appendix-enabling-node-local-dns-metrics" >Appendix - Enabling node-local-dns metrics&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#appendix-using-dnsperf" >Appendix - Using dnsperf to test DNS performance&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#appendix-network-packet-capture" >Appendix - Network packet capture without dependencies&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#appendix-verbose-logging-on-kube-dns" >Appendix - Verbose logging on kube-dns&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#appendix-analyzing-dnsmasq-logs" >Appendix - Analyzing dnsmasq logs&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#appendix-analyzing-concurrent-tcp-connections" >Appendix - Analyzing concurrent TCP connections&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#analyzing-dns-problems-based-on-packet-capture" >Appendix - Analyzing DNS problems based on packet captures&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#footnotes" >Footnotes&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>This is a cross-post of a blog post also published on the &lt;a class="link" href="https://www.signicat.com/blog/dont-use-nodelocal-dnscache-on-gke" target="_blank" rel="noopener"
>Signicat Blog&lt;/a>&lt;/em>&lt;/p>
&lt;p>&lt;a id="introduction">&lt;/a>&lt;/p>
&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>Last year I was freelancing as a consultant in Signicat and recently I returned, now as an actual employee!&lt;/p>
&lt;p>The first week after returning, a tech lead for another team reaches out to me about a technical issue he&amp;rsquo;s been struggling with.&lt;/p>
&lt;p>In this blog post I&amp;rsquo;ll try to guide you through the troubleshooting with actionable take-aways. It appears it&amp;rsquo;s going to be a long one with a lot of detours, so I&amp;rsquo;ve summarized our findings and recommendations here on the top starting right below this introduction. There are a few appendixes at the end that I&amp;rsquo;ve tried to make self-contained as to make them useful in other contexts and without necessarily having to follow the main story.&lt;/p>
&lt;p>If you don&amp;rsquo;t want any spoilers but follow the bumpy journey from start to end, fill your coffee mug and skip ahead to &lt;a class="link" href="#investigation" >Investigation&lt;/a>.&lt;/p>
&lt;p>&lt;a id="tl-dr">&lt;/a>&lt;/p>
&lt;h1 id="tldr">TL;DR:
&lt;/h1>&lt;p>Due to an unfortunate combination of behaviour of &lt;a class="link" href="https://coredns.io/manual/toc/" target="_blank" rel="noopener"
>CoreDNS&lt;/a> (which NodeLocal DNSCache uses) and &lt;a class="link" href="https://github.com/kubernetes/dns" target="_blank" rel="noopener"
>kube-dns&lt;/a> (which is the default on GKE) &lt;strong>I recommend NOT using them in combination.&lt;/strong>&lt;/p>
&lt;p>Since &lt;a class="link" href="https://cloud.google.com/knowledge/kb/how-to-run-coredns-on-kubernetes-engine-000004698" target="_blank" rel="noopener"
>GKE does not offer CoreDNS as a managed option&lt;/a> for &lt;code>kube-dns&lt;/code> (even though &lt;a class="link" href="https://kubernetes.io/blog/2018/12/03/kubernetes-1-13-release-announcement/" target="_blank" rel="noopener"
>Kubernetes made CoreDNS the default in 1.13 in 2018&lt;/a>) you are left with two options:&lt;/p>
&lt;ul>
&lt;li>Not enabling &lt;a class="link" href="https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache" target="_blank" rel="noopener"
>NodeLocal DNSCache on GKE&lt;/a>&lt;/li>
&lt;li>Switching from &lt;code>kube-dns&lt;/code> &lt;a class="link" href="https://cloud.google.com/kubernetes-engine/docs/how-to/cloud-dns" target="_blank" rel="noopener"
>to Google Cloud DNS&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>What is this unfortunate combination you ask?&lt;/em>&lt;/p>
&lt;p>&lt;a id="nodelocal-dnscache-coredns">&lt;/a>&lt;/p>
&lt;h2 id="nodelocal-dnscache--coredns">NodeLocal DNSCache / CoreDNS
&lt;/h2>&lt;p>NodeLocal DNSCache (in GKE at least) is configured with:&lt;/p>
&lt;pre>&lt;code># kubectl get configmap --namespace kube-system node-local-dns -o yaml | yq .data.Corefile
cluster.local:53 {
forward . __PILLAR__CLUSTER__DNS__ {
force_tcp
expire 1s
&lt;/code>&lt;/pre>
&lt;p>This means CoreDNS will upgrade any and all incoming DNS requests to TCP connections before connecting to &lt;code>dnsmasq&lt;/code> (the first of two containers in a &lt;code>kube-dns&lt;/code> Pod). CoreDNS reuses TCP connections if available. Unused connections in the connection pool should be cleaned up every 1 second (in theory). If no connections are available a new will be created, apparently with no upper bound.&lt;/p>
&lt;p>This means new connections may be created en-masse when needed, but old connections can take a while (I&amp;rsquo;ve observed 5-10 seconds) before being cleaned up, and most importantly, closed.&lt;/p>
&lt;p>&lt;a id="kubedns-dnsmasq">&lt;/a>&lt;/p>
&lt;h2 id="kube-dns--dnsmasq">kube-dns / dnsmasq
&lt;/h2>&lt;p>&lt;code>dnsmasq&lt;/code> has a &lt;a class="link" href="https://github.com/imp/dnsmasq/blob/master/src/config.h#L18" target="_blank" rel="noopener"
>hardcoded maximum number of 20 workers&lt;/a>. For us that means each &lt;code>kube-dns&lt;/code> Pod is limited to 20 open connections.&lt;/p>
&lt;p>&lt;code>kube-dns&lt;/code> scaling is managed by a bespoke kube-dns autoscaler that by default in GKE is &lt;a class="link" href="https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache#scaling_up_kube-dns" target="_blank" rel="noopener"
>configured&lt;/a> like:&lt;/p>
&lt;pre>&lt;code># kubectl get configmap --namespace kube-system kube-dns-autoscaler -o yaml | yq .data.linear
{&amp;quot;coresPerReplica&amp;quot;:256, &amp;quot;nodesPerReplica&amp;quot;:16,&amp;quot;preventSinglePointFailure&amp;quot;:true}
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;code>preventSinglePointFailure&lt;/code> - Run at least two Pods.&lt;/li>
&lt;li>&lt;code>nodesPerReplica&lt;/code> - Run one &lt;code>kube-dns&lt;/code> Pod for each 16 nodes.&lt;/li>
&lt;/ul>
&lt;p>This default setup will have two &lt;code>kube-dns&lt;/code> Pods until your cluster grows beyond 32 nodes.&lt;/p>
&lt;p>Two &lt;code>kube-dns&lt;/code> Pods have a limit of 40 open TCP connections in total from the &lt;code>node-local-dns&lt;/code> Pods running on each node. The &lt;code>node-local-dns&lt;/code> Pods though are happy to try to open many more TCP connections.&lt;/p>
&lt;p>GKE kube-dns docs mention &amp;ldquo;&lt;a class="link" href="https://cloud.google.com/kubernetes-engine/docs/how-to/kube-dns#performance_limitations_with_kube-dns" target="_blank" rel="noopener"
>Performance limitations with kube-dns&lt;/a>&amp;rdquo; as a known issue suggesting enabling NodeLocal DNSCache as a potential fix.&lt;/p>
&lt;p>And GKE NodeLocal DNSCache docs mention &amp;ldquo;&lt;a class="link" href="https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache#timeout_issues" target="_blank" rel="noopener"
>NodeLocal DNSCache timeout errors&lt;/a>&amp;rdquo; as a known issue with the possible reasons being&lt;/p>
&lt;ul>
&lt;li>An underlying network connectivity problem. (&lt;em>Spoiler: it probably isn&amp;rsquo;t&lt;/em>)&lt;/li>
&lt;li>Significantly increased DNS queries from the workload or due to node pool upscaling. (&lt;em>Spoiler: it probably isn&amp;rsquo;t&lt;/em>)&lt;/li>
&lt;/ul>
&lt;p>With the fix being&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>&amp;ldquo;The workaround is to increase the number of kube-dns replicas by tuning the autoscaling parameters.&amp;rdquo;&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>Increasing the number of &lt;code>kube-dns&lt;/code> Pods will reduce the frequency and impact but not eliminate it. Until we migrate to Cloud DNS we run &lt;code>kube-dns&lt;/code> at a ratio of 1:1.5 of nodes by configuring autoscaling with &lt;code>&amp;quot;coresPerReplica&amp;quot;:24&lt;/code> and using 16 core nodes. Resulting in 12 &lt;code>kube-dns&lt;/code> Pods in a 17 node cluster.&lt;/p>
&lt;blockquote>
&lt;p>By looking at the &lt;code>coredns_forward_conn_cache_misses_total&lt;/code> metric I observe it at least increasing by more than 300 in a 15 second metric sampling window for &lt;em>one&lt;/em> &lt;code>node-local-dns&lt;/code> Pod. This means &lt;em>on average&lt;/em> during those 15 seconds 20 new TCP connections were attempted since no existing connection could be re-used. (&lt;a class="link" href="#footnote-a" >Footnote A&lt;/a>)&lt;/p>
&lt;p>This means even setting &lt;code>&amp;quot;nodesPerReplica&amp;quot;:1&lt;/code> thus running one &lt;code>kube-dns&lt;/code> Pod for each node may not be enough to guarantee not hitting the 20 process limit in &lt;code>dnsmasq&lt;/code> occasionally.&lt;/p>
&lt;p>I guess you could start lowering &lt;code>coresPerReplica&lt;/code> to have more than one &lt;code>kube-dns&lt;/code> Pod for each node, but now it&amp;rsquo;s getting silly.&lt;/p>
&lt;/blockquote>
&lt;p>To re-iterate; If you&amp;rsquo;re using NodeLocal DNSCache and &lt;code>kube-dns&lt;/code> you should plan to migrate to Cloud DNS. You can alleviate the problem in the short term by scaling up &lt;code>kube-dns&lt;/code> aggressively but it will not eliminate occasional latency spikes.&lt;/p>
&lt;p>&lt;a id="am-i-affected">&lt;/a>&lt;/p>
&lt;h2 id="am-i-affected">Am I affected?
&lt;/h2>&lt;p>How do I know if I&amp;rsquo;m affected by this?&lt;/p>
&lt;ul>
&lt;li>You are using NodeLocal DNSCache (CoreDNS) and &lt;code>kube-dns&lt;/code> (default on GKE)&lt;/li>
&lt;li>You see log lines with &lt;code>i/o timeout&lt;/code> in node-local-dns pods (&lt;code>kubectl logs -n kube-system node-local-dns-xxxxx&lt;/code>)&lt;/li>
&lt;li>You are hitting dnsmasq max procs. Create a graph &lt;code>container_processes{namespace=&amp;quot;kube-system&amp;quot;, container=&amp;quot;dnsmasq&amp;quot;}&lt;/code> or use my &lt;a class="link" href="https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/grafana-dashboard-kube-dns.json" target="_blank" rel="noopener"
>kube-dns Grafana dashboard&lt;/a>. (&lt;a class="link" href="#footnote-b" >Footnote B&lt;/a>).&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="investigation">&lt;/a>&lt;/p>
&lt;h1 id="investigation">Investigation
&lt;/h1>&lt;p>&lt;a id="the-initial-problem">&lt;/a>&lt;/p>
&lt;h2 id="the-initial-problem">The initial Problem
&lt;/h2>&lt;p>We have a microservice architecture and use &lt;a class="link" href="https://www.openpolicyagent.org/" target="_blank" rel="noopener"
>Open Policy Agent (OPA)&lt;/a> deployed as &lt;a class="link" href="https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns/#example-1-sidecar-containers" target="_blank" rel="noopener"
>sidecars&lt;/a> to process and validate request tokens.&lt;/p>
&lt;p>However, some services were getting timeouts and he suspected it was an issue with &lt;a class="link" href="https://istio.io/" target="_blank" rel="noopener"
>istio&lt;/a>, a service mesh that provides security and observability for traffic inside Kubernetes clusters.&lt;/p>
&lt;p>The actual error message showing up in the logs was a generic &lt;code>context deadline exceeded (Client.Timeout exceeded while awaiting headers)&lt;/code> which I recognize as a Golang error probably stemming from a &lt;a class="link" href="https://pkg.go.dev/net/http" target="_blank" rel="noopener"
>http.Get()&lt;/a> call or similar.&lt;/p>
&lt;p>The first thing that comes to mind is that last year we made a change in our istio practices. Inside our clusters we call services in other namespaces (crossing team boundaries) using the same FQDN domain and path that our customers would use, not internal Kubernetes names. So internally we&amp;rsquo;re calling &lt;code>api.signicat.com/service&lt;/code> and not &lt;code>service.some-team-namespace&lt;/code>.&lt;/p>
&lt;p>He had noticed that the problem seemed to have increased when doing the switch from internal DNS names to FQDN.&lt;/p>
&lt;p>&lt;a id="istio-envoy-metrics">&lt;/a>&lt;/p>
&lt;h2 id="istio--envoy-metrics">Istio / Envoy metrics
&lt;/h2>&lt;p>We started troubleshooting by enabling additional metrics in envoy to hopefully be able to confirm that the problem was visible as envoy timeouts or some other kind of error.&lt;/p>
&lt;blockquote>
&lt;p>&lt;a class="link" href="https://www.envoyproxy.io/" target="_blank" rel="noopener"
>Envoy&lt;/a> is the actual software that traffic goes through in an istio service mesh.&lt;/p>
&lt;/blockquote>
&lt;p>This turned out to be a dead end and we couldn’t find any signs of timeouts or errors in the envoy metrics.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>Enabling these additional metrics was a bit of a chore and there were a few surprises. Have a look at the &lt;a class="link" href="#appendix-enabling-additional-envoy-metrics" >Enabling additional envoy metrics&lt;/a> appendix for steps and caveats.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="reproducing">&lt;/a>&lt;/p>
&lt;h2 id="reproducing-and-troubleshooting-http-connection-problems">Reproducing and troubleshooting HTTP connection problems
&lt;/h2>&lt;p>At this point I&amp;rsquo;m thinking the problem may be something else. Failure to acquire a HTTP connection from the pool or a TCP socket maybe?&lt;/p>
&lt;p>Using the &lt;a class="link" href="https://pkg.go.dev/net/http/httptrace" target="_blank" rel="noopener"
>httptrace&lt;/a> golang library I created a small program that would continuously query both the internal &lt;code>some-service.some-namespace&lt;/code> hostname and FQDN &lt;code>api.signicat.com/some-service&lt;/code>, while logging and counting each phase of the HTTP request as well as it&amp;rsquo;s timings.&lt;/p>
&lt;p>Here is a quick and dirty way of starting the program in an ephemeral Pod:&lt;/p>
&lt;pre>&lt;code># Start a Pod named `conn-test` using the `golang:latest` image. Attach to it (`-i --tty`) and start `bash`. Delete it after disconnecting (`--rm`):
kubectl run conn-test --rm -i --tty --image=golang:latest -- bash
# Create and enter a directory for the program:
mkdir conn-test &amp;amp;&amp;amp; cd conn-test
# Initialize a go environment for it:
go mod init conn-test
# Download the source:
wget https://raw.githubusercontent.com/signicat/blog-attachements/main/2023-gke-node-local-dns-cache/files/go-http-connection-test/main.go
# Download dependencies:
go mod tidy
# Configure it (refer to the source for more configuration options):
export URL_1=http://some-service.some-namespace.svc.cluster.local
# Start it:
go run main.go
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>If you need to run it for longer it probably makes sense to build it as a Docker image, upload it to an image registry and create a Deployment for it. It also exposes metrics in Prometheus format so you can also add scraping of the metrics either through annotations or a ServiceMonitor.&lt;/p>
&lt;/blockquote>
&lt;p>After a few hours the program crashed, and the last log message indicated that &lt;code>DNSStart&lt;/code> was the last thing to happened. (The program crashed since I just &lt;code>log.Fatal&lt;/code>ed if the request failed, which it did timing out. I since improved that, even though we already learned what we needed).&lt;/p>
&lt;p>We did some manual DNS lookups using &lt;code>nslookup -debug api.signicat.com&lt;/code> which (obviously, in retrospect) shows 6 failing DNS requests before finally getting it right:&lt;/p>
&lt;ul>
&lt;li>&lt;code>api.signicat.com.$namespace.svc.cluster.local - NXDOMAIN&lt;/code>&lt;/li>
&lt;li>&lt;code>api.signicat.com.svc.cluster.local - NXDOMAIN&lt;/code>&lt;/li>
&lt;li>&lt;code>api.signicat.com.cluster.local - NXDOMAIN&lt;/code>&lt;/li>
&lt;li>&lt;code>api.signicat.com.$gcp-region.c.$gcp-project.internal - NXDOMAIN&lt;/code>&lt;/li>
&lt;li>&lt;code>api.signicat.com.c.$gcp-project.internal - NXDOMAIN&lt;/code>&lt;/li>
&lt;li>&lt;code>api.signicat.com.google.internal - NXDOMAIN&lt;/code>&lt;/li>
&lt;li>&lt;code>api.signicat.com - OK&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>See &lt;a class="link" href="#appendix-overview-of-dns-on-gke" >Overview of DNS on GKE&lt;/a> for a lengthier explanation of why this happens.&lt;/em>&lt;/p>
&lt;p>The default timeout for HTTP requests in Golang is 5 seconds. DNS resolution &lt;em>should&lt;/em> be fast enough and it shouldn&amp;rsquo;t be a problem to do 7 DNS lookups in that time. But if there is some sluggishness and variability in DNS resolution times the probability of having one slow lookup out of the 7 being slow increases the risk of causing a timeout. In addition, always doing 7 lookups puts additional strain on the DNS infrastructure potentially further exacerbating the probability of slow lookups.&lt;/p>
&lt;p>From here on we have two courses of action. Figure out if we can reduce the number of DNS lookups as well as figure out if, and why, DNS resolution isn&amp;rsquo;t consistently fast.&lt;/p>
&lt;p>&lt;em>See &lt;a class="link" href="#appendix-reducing-dns-lookups" >Reducing DNS lookups in Kubernetes and GKE&lt;/a> for some thoughts on reducing these extraneous DNS lookups.&lt;/em>&lt;/p>
&lt;p>&lt;a id="troubleshooting-dns">&lt;/a>&lt;/p>
&lt;h2 id="troubleshooting-potentially-slow-or-broken-dns-lookups">Troubleshooting potentially slow or broken DNS lookups
&lt;/h2>&lt;p>We use &lt;a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/" target="_blank" rel="noopener"
>NodeLocal DNSCache&lt;/a> which on GKE is &lt;a class="link" href="https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache" target="_blank" rel="noopener"
>enabled by clicking the right button&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>NodeLocal DNSCache summary&lt;/strong>&lt;/p>
&lt;p>It is deployed by a ReplicaSet causing one CoreDNS Pod (named &lt;code>node-local-dns-x&lt;/code> in &lt;code>kube-system&lt;/code> namespace) to be running on each node.&lt;/p>
&lt;p>It adds a listener on the same IP as the &lt;code>kube-dns&lt;/code> Service. Thereby intercepting traffic that would otherwise go to that IP.&lt;/p>
&lt;p>The &lt;code>node-local-dns&lt;/code> Pod caches both positive and negative results. Domains ending in &lt;code>.cluster.local&lt;/code> are forwarded to &lt;code>kube-dns&lt;/code> in the cluster (but through a new Service called &lt;code>kube-dns-upstream&lt;/code> with a different IP). Requests for other domains are forwarded to 8.8.8.8 and 8.8.4.4.&lt;/p>
&lt;p>NodeLocal DNSCache instances use a 2 second timeout when querying upstream DNS servers.&lt;/p>
&lt;p>&lt;em>See &lt;a class="link" href="#appendix-overview-of-gke-dns-with-nodelocal-dnscache" >Overview of GKE DNS with NodeLocal DNSCache&lt;/a> for additional details.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>We wanted to look at some metrics from the &lt;code>node-local-dns&lt;/code> Pods but found that we didn&amp;rsquo;t have any! We fixed that but didn&amp;rsquo;t at the time learn anything new. &lt;em>See &lt;a class="link" href="#appendix-enabling-node-local-dns-metrics" >Enabling node-local-dns metrics&lt;/a> for how to fix/enable node-local-dns metrics.&lt;/em>&lt;/p>
&lt;p>Stumbling over the logs of one of the &lt;code>node-local-dns-x&lt;/code> Pods I notice:&lt;/p>
&lt;pre>&lt;code>[ERROR] plugin/errors: 2 archive.ubuntu.com.some-namespace.svc.cluster.local. A: dial tcp 172.18.165.130:53: i/o timeout
&lt;/code>&lt;/pre>
&lt;p>This tells us that at least lookups going to &lt;code>kube-dns&lt;/code> in the cluster (&lt;code>172.18.165.130:53&lt;/code>) are having problems.&lt;/p>
&lt;p>So timeouts are happening on individual lookups, it&amp;rsquo;s not just the total duration of 7 lookups timing out. And lookups are not only happening inside the client application but in &lt;code>node-local-dns&lt;/code> as well. We still don&amp;rsquo;t know if these lookups are lost or merely slow. But seen from the application anything longer than 2 seconds, that is timing out on &lt;code>node-local-dns&lt;/code>, might as well be lost.&lt;/p>
&lt;p>Just to be sure I checked that packets were not being dropped or lost on the nodes on both sides. And indeed the network seems fine. It&amp;rsquo;s been a long time since I&amp;rsquo;ve actually experienced problems due to packets being lost, but it&amp;rsquo;s always good to rule it out.&lt;/p>
&lt;p>Next step is to capture the packets as they (hopefully) leave the &lt;code>node-local-dns&lt;/code> Pod and (maybe) arriving at one of the &lt;code>kube-dns&lt;/code> Pods.&lt;/p>
&lt;p>&lt;em>See &lt;a class="link" href="#appendix-network-packet-capture" >Network packet capture without dependencies&lt;/a> for how to capture packets in Kubernetes.&lt;/em>&lt;/p>
&lt;p>The packets are indeed arriving at the &lt;code>dnsmasq&lt;/code> container in the &lt;code>kube-dns&lt;/code> Pods:&lt;/p>
&lt;pre>&lt;code>Query 0x31e8:
Leaves node-local-dns eth0:
10:53:11.134 10.0.0.107 172.20.13.4 DNS 0x31e8 Standard query 0x31e8 A archive.ubuntu.com.some-ns.svc.cluster.local
Arrives at dnsmasq eth0:
10:53:11.134 10.0.0.107 172.20.13.4 DNS 0x31e8 Standard query 0x31e8 A archive.ubuntu.com.some-ns.svc.cluster.local
&lt;/code>&lt;/pre>
&lt;p>Looking at DNS resolution time from the &lt;code>kube-dns&lt;/code> container which &lt;code>dnsmasq&lt;/code> forwards to shows that &lt;code>kube-dns&lt;/code> is consistently answering queries extremely fast (not shown here).&lt;/p>
&lt;p>But after some head scratching I look at the time between queries arriving at &lt;code>dnsmasq&lt;/code> on &lt;code>eth0&lt;/code> before leaving again on &lt;code>lo&lt;/code> for &lt;code>kube-dns&lt;/code> and indeed there is a (relatively) long delay of 952ms between 10:53:11.134 and 10:53:12.086:&lt;/p>
&lt;pre>&lt;code>Leaves dnsmasq lo:
10:53:12.086 127.0.0.1 127.0.0.1 DNS 0x31e8 Standard query 0x31e8 A archive.ubuntu.com.some-ns.svc.cluster.local
&lt;/code>&lt;/pre>
&lt;p>In this case the query just sits around in &lt;code>dnsmasq&lt;/code> for almost one second before being forwarded to &lt;code>kube-dns&lt;/code>!&lt;/p>
&lt;p>Why? As with most issues we start by checking if there are any issues with memory or CPU usage or CPU throttling (&lt;a class="link" href="#footnote-c" >Footnote C&lt;/a>).&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/kube-dns-resource-usage.png"
width="1377"
height="693"
srcset="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/kube-dns-resource-usage_hu15148347036814534097.png 480w, https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/kube-dns-resource-usage_hu11608676099510034976.png 1024w"
loading="lazy"
alt="Graphs showing DNS response time on node-local-dns Pods and resource usage on kube-dns Pods."
class="gallery-image"
data-flex-grow="198"
data-flex-basis="476px"
>&lt;/p>
&lt;p>Nope. CPU and memory usage is both low and stable but the 99 percentile DNS request duration is all over the place.&lt;/p>
&lt;p>We also check and see that there is plenty of unused CPU available on the underlying nodes these pods are running on.&lt;/p>
&lt;blockquote>
&lt;p>We also used &lt;code>dnsperf&lt;/code> to benchmark and stress-test the various components and while fun, didn&amp;rsquo;t teach us anything new. &lt;em>See &lt;a class="link" href="#appendix-using-dnsperf" >Using dnsperf to test DNS performance&lt;/a> for more information on that particular side-quest.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>Next up I want to increase the logging verbosity of &lt;code>dnsmasq&lt;/code> to see if there are any clues to why it was (apparently) delaying processing DNS requests.&lt;/p>
&lt;p>&lt;em>Have a look at &lt;a class="link" href="#appendix-verbose-logging-on-kube-dns" >Verbose logging on kube-dns&lt;/a> for how to increase logging and &lt;a class="link" href="#appendix-analyzing-dnsmasq-logs" >Analyzing dnsmasq logs&lt;/a> for how I analyzed the logs.&lt;/em>&lt;/p>
&lt;p>Analyzing the logs we learn that at least it appears that individual requests are fast and not clogging up the machinery.&lt;/p>
&lt;p>In the meantime Google Cloud Support came back to us asking if we could run a command &lt;code>for i in $(seq 1 1800) ; do echo &amp;quot;$(date) Try: ${i} DnsmasqProcess: $(pidof dnsmasq | wc -w)&amp;quot;; sleep 1; done&lt;/code> on the VM of the Pod. It also works to run this in a debug container attached to the &lt;code>dnsmasq&lt;/code> container. The output looks like:&lt;/p>
&lt;pre>&lt;code>Wed Oct 11 14:50:57 UTC 2023 Try: 1 DnsmasqProcess: 9
Wed Oct 11 14:50:58 UTC 2023 Try: 2 DnsmasqProcess: 14
Wed Oct 11 14:50:59 UTC 2023 Try: 3 DnsmasqProcess: 15
Wed Oct 11 14:51:00 UTC 2023 Try: 4 DnsmasqProcess: 21
Wed Oct 11 14:51:01 UTC 2023 Try: 5 DnsmasqProcess: 21
&lt;/code>&lt;/pre>
&lt;p>It turns out that &lt;code>dnsmasq&lt;/code> has a &lt;a class="link" href="https://github.com/imp/dnsmasq/blob/master/src/config.h#L18" target="_blank" rel="noopener"
>hard coded limit of 20 child processes&lt;/a>. So every time we see 21 it means it will not spawn a new child to process incoming connections.&lt;/p>
&lt;p>I plotted this on the same graph as the DNS request times observed from raw packet captures (See &lt;a class="link" href="#analyzing-dns-problems-based-on-packet-capture" >Analyzing DNS problems based on packet captures&lt;/a>):&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-and-dns-packet-latency.png"
width="1428"
height="489"
srcset="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-and-dns-packet-latency_hu3974894387970546800.png 480w, https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-and-dns-packet-latency_hu12671590469830325247.png 1024w"
loading="lazy"
alt="Graph showing DNS resolution time from packet captures in blue. Number of dnsmasq processes in orange."
class="gallery-image"
data-flex-grow="292"
data-flex-basis="700px"
>&lt;/p>
&lt;p>Graphing the number of &lt;code>dnsmasq&lt;/code> processes and observed DNS request latency finally shows a stable correlation between our symptoms and a potential cause.&lt;/p>
&lt;p>You can also get a crude estimation by graphing the &lt;code>container_processes{namespace=&amp;quot;kube-system&amp;quot;, container=&amp;quot;dnsmasq&amp;quot;}&lt;/code> metric, or use my &lt;a class="link" href="https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/grafana-dashboard-kube-dns.json" target="_blank" rel="noopener"
>kube-dns Grafana dashboard&lt;/a> if you have cAdvisor/container metrics enabled:&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-graph-and-node-local-dns-duration.png"
width="1631"
height="646"
srcset="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-graph-and-node-local-dns-duration_hu5189994088160500401.png 480w, https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-graph-and-node-local-dns-duration_hu2637781448338722321.png 1024w"
loading="lazy"
alt="Graph showing DNS resolution time from node-local-dns and dnsmasq processes."
class="gallery-image"
data-flex-grow="252"
data-flex-basis="605px"
>&lt;/p>
&lt;p>Going back to our packet captures I see connections that are unused for many seconds before finally being closed by &lt;code>node-local-dns&lt;/code>.&lt;/p>
&lt;pre>&lt;code>08:06:07.849 172.20.9.11 10.0.0.29 53 → 46123 [ACK] Seq=1 Ack=80 Win=43648 Len=0
08:06:09.849 10.0.0.29 172.20.9.11 46123 → 53 [FIN, ACK] Seq=80 Ack=1 Win=42624 Len=0
08:06:09.890 172.20.9.11 10.0.0.29 53 → 46123 [ACK] Seq=1 Ack=81 Win=43648 Len=0
&lt;/code>&lt;/pre>
&lt;p>However &lt;code>dnsmasq&lt;/code> only acknowledges the request to close the connection, it does not actually close it yet. In order for the connection to be properly closed both sides have to &lt;code>FIN, ACK&lt;/code>.&lt;/p>
&lt;p>Many seconds later &lt;code>dnsmasq&lt;/code> faithfully tries to return a response and finish the closing of the connection, but &lt;code>node-local-dns&lt;/code> (CoreDNS) has promptly forgotten about the whole thing and replies with the TCP equivalent of a shrug (&lt;code>RST&lt;/code>):&lt;/p>
&lt;pre>&lt;code>08:06:15.836 172.20.9.11 10.0.0.29 53 46123 Standard query response 0x7f12 No such name A storage.googleapis.com.some-ns.svc.cluster.local SOA ns.dns.cluster.local
08:06:15.836 172.20.9.11 10.0.0.29 53 46123 53 → 46123 [FIN, ACK] Seq=162 Ack=81 Win=43648 Len=0
08:06:15.836 10.0.0.29 172.20.9.11 46123 53 46123 → 53 [RST] Seq=81 Win=0 Len=0
&lt;/code>&lt;/pre>
&lt;p>Then I analyzed the whole packet capture to find the number of open connections at any time as well as the frequency and duration of these lingering TCP connections. &lt;em>See &lt;a class="link" href="#appendix-analyzing-concurrent-tcp-connections" >Analyzing concurrent TCP connections&lt;/a> for details on how.&lt;/em>&lt;/p>
&lt;p>What we find is that the number of open connections at times surge way past 20 and that coincides with increased latency. In addition unused connections stay open for a problematic long time (5-10 seconds).&lt;/p>
&lt;p>Since the closing of connections is initiated by CoreDNS. Is there any way we can make CoreDNS close connections faster or limit the number of connections it uses?&lt;/p>
&lt;p>NodeLocal DNSCache (in GKE at least) is configured (&lt;code>kubectl get configmap --namespace kube-system node-local-dns -o yaml | yq .data.Corefile&lt;/code>) with:&lt;/p>
&lt;pre>&lt;code>cluster.local:53 {
forward . __PILLAR__CLUSTER__DNS__ {
force_tcp
expire 1s
&lt;/code>&lt;/pre>
&lt;p>So it is actually configured to &amp;ldquo;expire&amp;rdquo; connections after 1 second.&lt;/p>
&lt;p>I&amp;rsquo;m not well versed in the CoreDNS codebase but it seems expiring connections is &lt;a class="link" href="https://github.com/coredns/coredns/blob/master/plugin/pkg/proxy/persistent.go#L48" target="_blank" rel="noopener"
>handled by a ticker&lt;/a>. A ticker in go sends a signal every &amp;ldquo;tick&amp;rdquo; that can be used to trigger events for example. This means connections aren&amp;rsquo;t closed once they pass the 1 second mark, but the cleanup process runs every 1 second and then purges expired connections. So a connection can be idle for almost 2x the time (2 seconds in our case) before being cleaned up.&lt;/p>
&lt;p>I still can&amp;rsquo;t explain why we see connections lingering on for 5-10 seconds though.&lt;/p>
&lt;p>There are no configuration options indicating that it&amp;rsquo;s possible to limit the number of connections. Nor anything in the code that would suggest it&amp;rsquo;s a possibility. Adding it is probably not done in a day either as it would require making decisions on trade-offs about how to handle excess traffic volume. How much to queue, for how long. How to avoid the queue eating too much memory. What to do when the queue is full and so on and so on.&lt;/p>
&lt;p>At this point I feel we have a pretty good grasp of how all of this conspires to cause the problems we observe. But unfortunately I can&amp;rsquo;t see any permanent solution that does not require modifying CoreDNS and ideally &lt;code>dnsmasq&lt;/code> code itself. At the moment we don&amp;rsquo;t have the capacity to create and push through such changes upstream. If I could snap my fingers and magically get some new features they would be:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>dnsmasq&lt;/p>
&lt;ul>
&lt;li>Make &lt;code>MAX_PROCS&lt;/code> configurable.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>CoreDNS&lt;/p>
&lt;ul>
&lt;li>Configurable max TCP connection pool size. Metrics on pool size and usage.&lt;/li>
&lt;li>Configurable queue size for requests waiting for a new connection from the pool. Metrics on queue capacity (max) and current size for tuning.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>But until that becomes a reality I think the best option is to avoid using NodeLocal DNSCache in combination with &lt;code>kube-dns&lt;/code>, and instead replace &lt;code>kube-dns&lt;/code> with Cloud DNS.&lt;/p>
&lt;p>&lt;a id="appendices">&lt;/a>&lt;/p>
&lt;h1 id="appendices">Appendices
&lt;/h1>&lt;p>&lt;a id="appendix-enabling-additional-envoy-metrics">&lt;/a>&lt;/p>
&lt;h2 id="appendix---enabling-additional-envoy-metrics">Appendix - Enabling additional envoy metrics
&lt;/h2>&lt;p>&lt;em>See &lt;a class="link" href="https://istio.io/latest/docs/concepts/observability/#proxy-level-metrics" target="_blank" rel="noopener"
>istio.io&lt;/a> for some high level info on proxy-level metrics and &lt;a class="link" href="https://www.envoyproxy.io/docs/envoy/latest/configuration/upstream/cluster_manager/cluster_stats" target="_blank" rel="noopener"
>envoyproxy.io&lt;/a> for complete list of available metrics.&lt;/em>&lt;/p>
&lt;p>Most likely we are interested in the &lt;code>upstream_cx_*&lt;/code> and &lt;code>upstream_rq_*&lt;/code> metrics.&lt;/p>
&lt;p>By default metrics are only gathered and exposed for the &lt;code>xds-grpc&lt;/code> cluster, and the full stats name looks like &lt;code>cluster.xds-grpc.upstream_cx_total&lt;/code>. The &lt;code>xds-grpc&lt;/code> cluster I assume is metrics for traffic between the &lt;code>istio-proxy&lt;/code> containers in each Pod and the central istio services (&lt;code>istiod&lt;/code>) used for configuration management.&lt;/p>
&lt;blockquote>
&lt;p>A cluster in envoy is a grouping of backends and endpoints. Typically each Kubernetes &lt;code>Service&lt;/code> will be it’s own cluster. There are also separate clusters named &lt;code>BlackHoleCluster&lt;/code>, &lt;code>InboundPassthroughClusterIpv4&lt;/code> and &lt;code>PassthroughCluster&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>When enabling metrics for other services (or clusters as they&amp;rsquo;re also called) they look like&lt;/p>
&lt;ul>
&lt;li>&lt;code>cluster.outbound|80||my-service.my-namespace.svc.cluster.local.upstream_cx_total&lt;/code> for outgoing traffic and&lt;/li>
&lt;li>&lt;code>cluster.inbound|8080||.upstream_cx_total&lt;/code> for incoming traffic.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Beware that traffic to for example api.signicat.com/some-service will be mapped to the internal Kubernetes Service DNS name in the metric, like some-service.some-team.svc.cluster.local.&lt;/p>
&lt;p>Ports are also mapped. For outgoing traffic it will be the port in the Service. While for incoming traffic it will be the port the Pod is actually listening on, and not the one mapped in the Service.&lt;/p>
&lt;/blockquote>
&lt;h3 id="enabling-additional-envoy-metrics">Enabling additional envoy metrics
&lt;/h3>&lt;p>istio.io documents &lt;a class="link" href="https://istio.io/latest/docs/ops/configuration/telemetry/envoy-stats/" target="_blank" rel="noopener"
>how to enable additional envoy metrics&lt;/a> both globally in the mesh by configuring the &lt;code>IstioOperator&lt;/code> object as well as on a per Deployment/Pod using the &lt;code>proxy.istio.io/config&lt;/code> annotation.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>WARNING: Be very careful when enabling additional metrics as they have a tendency to expose orders of magnitude more time-series than you might expect.&lt;/strong>&lt;/p>
&lt;p>I apparently managed to get Prometheus OOMKilled even with my fairly limited testing on two deployments.&lt;/p>
&lt;p>If you happen to have &lt;a class="link" href="https://victoriametrics.com/" target="_blank" rel="noopener"
>VictoriaMetrics&lt;/a> in your monitoring stack you can monitor cardinality (the number of unique time-series, which is the thing that usually breaks a time-series database) in the VM UI:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl -n metrics port-forward services/victoria-metrics-cluster-vmselect 8481:8481
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>And going to http://localhost:8481/select/0/vmui/?#/cardinality&lt;/p>
&lt;/blockquote>
&lt;h3 id="enable-additional-metrics-on-a-deployment-or-pod">Enable additional metrics on a Deployment or Pod
&lt;/h3>&lt;p>To enable additional metrics on a Deployment or Pod, add the following annotations:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">annotations&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">proxy.istio.io/config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">|-&lt;/span>&lt;span class="sd">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> proxyStatsMatcher:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> inclusionSuffixes:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - &amp;#34;upstream_rq_timeout&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - &amp;#34;upstream_rq_retry&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - &amp;#34;upstream_rq_retry_limit_exceeded&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - &amp;#34;upstream_rq_retry_success&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="sd"> - &amp;#34;upstream_rq_retry_overflow&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Remember to add this to the &lt;strong>Pod&lt;/strong> metadata (&lt;code>spec.template.metadata&lt;/code>) if adding to a Deployment.&lt;/p>
&lt;blockquote>
&lt;p>This results in a new time-series being exposed for each Service and Port (Cluster) that envoy has configured.&lt;/p>
&lt;p>In our case we have ~500 services in our dev cluster and enabling these specific metrics adds ~3.800 new time-series for &lt;em>each Pod&lt;/em> in the Deployment we added it to. The test Deployment I&amp;rsquo;m playing with has 6 Pods so ~23.000 new time-series from adding 5 additional metrics to 1 Deployment!&lt;/p>
&lt;/blockquote>
&lt;p>Another option is to use regex to enable additional metrics:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">inclusionRegexps&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;.*upstream_rq_.*&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;.*upstream_cx_.*&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>But again, enabling these ~50 metrics on this specific Deployment will result in ~250.000 new time-series.&lt;/p>
&lt;blockquote>
&lt;p>Be especially wary of metrics envoy exposes as histograms, such as &lt;code>upstream_cx_connect_ms&lt;/code> and &lt;code>upstream_cx_length_ms&lt;/code> as they result in many _bucket time-series. During my testing this resulted in 6-7 million new time-series in total.&lt;/p>
&lt;/blockquote>
&lt;p>It&amp;rsquo;s possible to specify only the connections we are interested in, which makes it manageable, cardinality wise. For example to only gather metrics from Services A through F in their corresponding namespaces:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">inclusionRegexps&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;.*(svc-a.ns-a.svc|svc-b.ns-b.svc|svc-c.ns-c.svc|svc-d.ns-d.svc|svc-e.ns-e.svc|svc-f.ns-f.svc).*.upstream_rq.*&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="s2">&amp;#34;.*(svc-a.ns-a.svc|svc-b.ns-b.svc|svc-c.ns-c.svc|svc-d.ns-d.svc|svc-e.ns-e.svc|svc-f.ns-f.svc).*.upstream_cx.*&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="enable-additional-metrics-globally-please-dont">Enable additional metrics globally (please don’t!)
&lt;/h3>&lt;p>You can also enable additional metrics globally across the mesh. But that&amp;rsquo;s probably a very bad idea if you are running at any sort of scale. I estimated that enabling &lt;code>.*upstream_rq_.*&lt;/code> and &lt;code>.*upstream_cx_.*&lt;/code> in our dev cluster would result in 50M additional time-series at a minimum. Or 5-10x our current Prometheus usage.&lt;/p>
&lt;p>If you are using the old &lt;a class="link" href="https://istio.io/latest/docs/setup/install/operator/" target="_blank" rel="noopener"
>Istio Operator&lt;/a> way of installing and managing istio it should be enough to update the &lt;code>IstioOperator&lt;/code> object in the cluster. If you are using &lt;code>istioctl&lt;/code> (&lt;a class="link" href="https://istio.io/latest/about/faq/#install-method-selection" target="_blank" rel="noopener"
>recommended&lt;/a>) you must update the source &lt;code>IstioOperator&lt;/code> manifest that is being fed to &lt;code>istioctl&lt;/code> and run the appropriate &lt;code>istioctl&lt;/code> commands again to update. Note that this also creates an &lt;code>IstioOperator&lt;/code> object in the cluster with whatever config is used. But in this case it&amp;rsquo;s never used for anything other than reference. So updating the &lt;code>IstioOperator&lt;/code> object in a cluster if managing istio with &lt;code>istioctl&lt;/code> does nothing.&lt;/p>
&lt;h3 id="viewing-stats-and-metrics">Viewing stats and metrics
&lt;/h3>&lt;p>Metrics should start becoming available in Prometheus with names like &lt;code>envoy_cluster_upstream_cx_total&lt;/code>. Note that by default you&amp;rsquo;ll already see metrics from the &lt;code>xds-grpc&lt;/code> cluster.&lt;/p>
&lt;p>You can also get the &lt;a class="link" href="https://istio.io/latest/docs/ops/configuration/telemetry/envoy-stats/" target="_blank" rel="noopener"
>stats directly from a sidecar&lt;/a>. Either by querying the &lt;code>pilot-agent&lt;/code> directly:&lt;/p>
&lt;pre>&lt;code>kubectl exec -n my-namespace my-pod -c istio-proxy -- pilot-agent request GET stats
&lt;/code>&lt;/pre>
&lt;p>Or querying the metrics endpoint exposed to Prometheus:&lt;/p>
&lt;pre>&lt;code>kubectl exec -n my-namespace my-pod -c istio-proxy -- curl -sS 'localhost:15000/stats/prometheus'
&lt;/code>&lt;/pre>
&lt;p>Note that these will not give you any additional metrics compared to those exposed to Prometheus. A metric that is not enabled will not show up, and all metrics that are enabled are also automatically exposed to Prometheus.&lt;/p>
&lt;h3 id="notes-on-timeout-and-retry-metrics">Notes on timeout and retry metrics
&lt;/h3>&lt;p>If istio is configured to be involved with timeouts and retries that is configured on the &lt;a class="link" href="https://istio.io/latest/docs/reference/config/networking/virtual-service/#Destination" target="_blank" rel="noopener"
>VirtualService&lt;/a> level.&lt;/p>
&lt;p>That means it will only take effect if using &lt;code>api.signicat.com/some-service&lt;/code> (a route in a Istio &lt;code>VirtualService&lt;/code>) and not &lt;code>some-service.some-namespace.svc.cluster.local&lt;/code> (Kubernetes &lt;code>Service&lt;/code>).&lt;/p>
&lt;p>Istio will only count timeouts and retries for requests to a &lt;code>VirtualService&lt;/code> route that has timeouts (and optionally retries) configured.&lt;/p>
&lt;p>Additionally, the timeouts and retries seems to be enforced, and counted, in the Source istio-proxy (envoy), and not the Target.&lt;/p>
&lt;h3 id="further-work">Further work
&lt;/h3>&lt;p>It would be beneficial to be able to have envoy only show/export metrics that are non-zero. Since there is usually only a very small set of all possible Service-to-Service pairs that will actually regularly have traffic.&lt;/p>
&lt;p>It’s possible to customize metrics using the Telemetry API (&lt;a class="link" href="https://istio.io/latest/docs/tasks/observability/metrics/telemetry-api/" target="_blank" rel="noopener"
>Customizing Istio Metrics with Telemetry API&lt;/a>) but it seems limited to only working with metrics and their dimensions. Not the time-series values.&lt;/p>
&lt;p>&lt;a class="link" href="https://istio.io/latest/docs/reference/config/proxy_extensions/wasm-plugin/" target="_blank" rel="noopener"
>WASM plugins&lt;/a> are probably not a good fit either and are experimental and causes a severe CPU and memory penalty.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Edit to add in 2024&lt;/strong>&lt;/p>
&lt;p>We know today that the reason for so many time-series (reporting zero), as well as elevated memory usage in &lt;code>istio-proxy&lt;/code>, is because we did not filter exposed Services between namespaces causing every &lt;code>istio-proxy&lt;/code> to keep track of every pair of possible workloads in the whole cluster.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="appendix-overview-of-dns-on-gke">&lt;/a>&lt;/p>
&lt;h2 id="appendix---overview-of-dns-on-gke">Appendix - Overview of DNS on GKE
&lt;/h2>&lt;blockquote>
&lt;p>&lt;strong>kube-dns and CoreDNS confusion&lt;/strong>&lt;/p>
&lt;p>The main difference between DNS on upstream Kubernetes and GKE is that Kubernetes &lt;a class="link" href="https://kubernetes.io/blog/2018/07/10/coredns-ga-for-kubernetes-cluster-dns/" target="_blank" rel="noopener"
>switched to&lt;/a> CoreDNS 5 years ago while GKE &lt;a class="link" href="https://cloud.google.com/kubernetes-engine/docs/how-to/kube-dns" target="_blank" rel="noopener"
>still uses&lt;/a> the old &lt;code>kube-dns&lt;/code>. It’s &lt;a class="link" href="https://cloud.google.com/knowledge/kb/how-to-run-coredns-on-kubernetes-engine-000004698" target="_blank" rel="noopener"
>possible to add CoreDNS but it’s not possible to remove or disable&lt;/a> &lt;code>kube-dns&lt;/code>.&lt;/p>
&lt;p>In upstream K8s, &lt;a class="link" href="https://kubernetes.io/blog/2018/07/10/coredns-ga-for-kubernetes-cluster-dns/" target="_blank" rel="noopener"
>CoreDNS reached GA back in 2018 in K8s 1.11&lt;/a> and &lt;code>kube-dns&lt;/code> was &lt;a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/coredns/#migrating-to-coredns" target="_blank" rel="noopener"
>removed from kubeadm in K8s 1.21&lt;/a>.&lt;/p>
&lt;p>However for &lt;a class="link" href="https://github.com/coredns/deployment/issues/116" target="_blank" rel="noopener"
>backwards compatibility&lt;/a> CoreDNS &lt;a class="link" href="https://github.com/coredns/deployment/blob/master/kubernetes/coredns.yaml.sed" target="_blank" rel="noopener"
>still uses&lt;/a> the name &lt;code>kube-dns&lt;/code>, which makes things confusing for sure!&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/DNS-on-GKE.png"
width="1313"
height="586"
srcset="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/DNS-on-GKE_hu2575099721844054612.png 480w, https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/DNS-on-GKE_hu15301862917548489956.png 1024w"
loading="lazy"
alt="Diagram showing DNS on GKE"
class="gallery-image"
data-flex-grow="224"
data-flex-basis="537px"
>&lt;/p>
&lt;p>Part of the magic of Kubernetes is it&amp;rsquo;s DNS-based Service Discovery. Lets say we deploy an application that has a &lt;code>Service&lt;/code> named &lt;code>helloworld&lt;/code> to the namespace &lt;code>team-a&lt;/code>. Other applications in the same namespace are able to connect to that service by calling for example &lt;code>http://helloworld&lt;/code>. This makes it easy to build and configure a group of microservices that talk to each other without needing to know about namespaces or FQDNs. Applications in another namespace can also call that application using &lt;code>http://helloworld.team-a&lt;/code>. (&lt;a class="link" href="#footnote-d" >Footnote D&lt;/a>).&lt;/p>
&lt;p>This magic is achieved using the standard DNS &lt;code>search&lt;/code> domain list feature. On Linux the &lt;code>/etc/resolv.conf&lt;/code> &lt;a class="link" href="https://man7.org/linux/man-pages/man5/resolv.conf.5.html" target="_blank" rel="noopener"
>file&lt;/a> defines which DNS servers to use. It also includes a &lt;code>search&lt;/code> option that works together with the &lt;code>ndots&lt;/code> option. You can see this by executing &lt;code>cat /etc/resolv.conf&lt;/code> in any &lt;code>Pod&lt;/code>.&lt;/p>
&lt;p>If a domain name contains fewer &amp;ldquo;dots&amp;rdquo; (&amp;quot;.&amp;quot;) than &lt;code>ndots&lt;/code> is set to, the DNS resolver will first try the hostname with each of the appended domains in &lt;code>search&lt;/code>. In Kubernetes (and GKE) ndots is by default set to 5.&lt;/p>
&lt;p>In vanilla Kubernetes the search domains are&lt;code> &amp;lt;namespace&amp;gt;.svc.cluster.local svc.cluster.local cluster.local&lt;/code> while on GKE it&amp;rsquo;s &lt;code>&amp;lt;namespace&amp;gt;.svc.cluster.local svc.cluster.local cluster.local &amp;lt;gcp-zone&amp;gt;.c.technology-dev-platform.internal c.&amp;lt;gcp-project&amp;gt;.internal google.internal&lt;/code>.&lt;/p>
&lt;p>This means if we call &lt;code>https://api.signicat.com&lt;/code> (2 dots) it will first try to resolve &lt;code>api.signicat.com.some-namespace.svc.cluster.local&lt;/code> and so on through the whole list, sequentally, before finally trying &lt;code>api.signicat.com&lt;/code> and actually succeeding.&lt;/p>
&lt;p>&lt;em>See &lt;a class="link" href="#appendix-reducing-dns-lookups" >Reducing DNS lookups in Kubernetes and GKE&lt;/a> for thoughts on avoiding some of this.&lt;/em>&lt;/p>
&lt;p>Then the requests are sent to the &lt;code>dnsmasq&lt;/code> container of a &lt;code>kube-dns&lt;/code> Pod. &lt;code>dnsmasq&lt;/code> is configured by a combination of command-line arguments defined in the &lt;code>kube-dns&lt;/code> Deployment and data from the &lt;code>kube-dns&lt;/code> &lt;a class="link" href="https://cloud.google.com/kubernetes-engine/docs/how-to/kube-dns" target="_blank" rel="noopener"
>ConfigMap&lt;/a>. We have not changed the default &lt;code>kube-dns&lt;/code> ConfigMap and this results in &lt;code>dnsmasq&lt;/code> running with these arguments:&lt;/p>
&lt;pre>&lt;code>/usr/sbin/dnsmasq -k \
--cache-size=1000 \
--no-negcache \
--dns-forward-max=1500 \
--log-facility=- \
--server=/cluster.local/127.0.0.1#10053 \
--server=/in-addr.arpa/127.0.0.1#10053 \
--server=/ip6.arpa/127.0.0.1#10053 \
--max-ttl=30 \
--max-cache-ttl=30 \
--server /internal/169.254.169.254 \
--server 8.8.8.8 \
--server 8.8.4.4 \
--no-resolv
&lt;/code>&lt;/pre>
&lt;p>This means all lookups for &lt;code>cluster.local&lt;/code>, &lt;code>in-addr.arpa&lt;/code> and &lt;code>ip6.arpa&lt;/code> will be sent to &lt;code>127.0.0.1:10053&lt;/code>, which is the &lt;code>kube-dns&lt;/code> container. Lookups for &lt;code>internal&lt;/code> are sent to &lt;code>169.254.169.254&lt;/code>. All others are sent to &lt;code>8.8.8.8&lt;/code> and &lt;code>8.8.4.4&lt;/code>.&lt;/p>
&lt;p>In addition &lt;code>--no-negcache&lt;/code> disables caching for lookups that were not found (&lt;code>NXDOMAIN&lt;/code>). This is particularly interesting since that means when looking up for example &lt;code>api.signicat.com&lt;/code> and the domains in the &lt;code>search&lt;/code> list are first tried, they will all result in &lt;code>NXDOMAIN&lt;/code> but &lt;code>dnsmasq&lt;/code> will not cache those results but send them on to &lt;code>kube-dns&lt;/code>. &lt;em>Every&lt;/em>. &lt;em>Time&lt;/em>. This may severely reduce the usefulness of the cache and create a constant volume of traffic to &lt;code>kube-dns&lt;/code> itself.&lt;/p>
&lt;blockquote>
&lt;p>It&amp;rsquo;s also worth noting that Google Public DNS servers have a &lt;a class="link" href="https://developers.google.com/speed/public-dns/docs/isp" target="_blank" rel="noopener"
>default rate limit of 1500 QPS per IP&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="appendix-reducing-dns-lookups">&lt;/a>&lt;/p>
&lt;h2 id="appendix---reducing-dns-lookups-in-kubernetes-and-gke">Appendix - Reducing DNS lookups in Kubernetes and GKE
&lt;/h2>&lt;p>There are two reasons for the 7 DNS lookups instead of the ideal 1. First is the &lt;code>ndots&lt;/code> which tells the DNS resolver in the container if a domain name has fewer dots than this, it will first try looking up the name by appending each entry in the &lt;code>search&lt;/code> list sequentially. For Kubernetes &lt;code>ndots&lt;/code> is set to 5 (why is &lt;a class="link" href="https://github.com/kubernetes/kubernetes/issues/33554#issuecomment-266251056" target="_blank" rel="noopener"
>explained by Tim Hockin here&lt;/a>). So &lt;code>api.signicat.com&lt;/code> only has 2 dots, and hence will first go through the list of search domains.&lt;/p>
&lt;p>Secondly GKE adds 3 extra search domains in addition to the 3 standard ones in Kubernetes. Bringing the total to 6 before doing the &amp;ldquo;proper&amp;rdquo; DNS lookup.&lt;/p>
&lt;p>The specific assumption we are deviating from leading to problems in our case is &amp;ldquo;We could mitigate some of the perf penalties by always trying names as upstream FQDNs first, but that means that all intra-cluster lookups get slower. Which do we expect more frequently? I&amp;rsquo;ll argue intra-cluster names, if only because the TTL is so low.&amp;rdquo;&lt;/p>
&lt;p>An option that we’re currently exploring is using a trailing dot in the FQDN (&lt;code>api.signicat.com.&lt;/code>) when calling other services. This explicitly tells DNS that this is a FQDN and should not search through the search domain lists for a hit first. This seems to work on some of our services but not all. Indicating that there isn’t any inherent problems doing this with regards to istio or other infrastructure. But there may be additional changes needed on some services to support this. I’m suspecting certain web application frameworks in certain languages not handling this well out of the box.&lt;/p>
&lt;p>&lt;a id="appendix-overview-of-gke-dns-with-nodelocal-dnscache">&lt;/a>&lt;/p>
&lt;h2 id="appendix---overview-of-gke-dns-with-nodelocal-dnscache">Appendix - Overview of GKE DNS with NodeLocal DNSCache
&lt;/h2>&lt;p>&lt;img src="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/DNS-on-GKE-with-Node-Local-DNS-Cache.png"
width="1201"
height="855"
srcset="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/DNS-on-GKE-with-Node-Local-DNS-Cache_hu15358002095465690191.png 480w, https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/DNS-on-GKE-with-Node-Local-DNS-Cache_hu7906030369844017798.png 1024w"
loading="lazy"
alt="Diagram showing DNS on GKE with NodeLocal DNSCache enabled"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="337px"
>&lt;/p>
&lt;p>&lt;a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/" target="_blank" rel="noopener"
>NodeLocal DNSCache&lt;/a> is a feature of Kubernetes that primarily aims to improve performance, scale and latency by:&lt;/p>
&lt;ul>
&lt;li>Potentially not traversing the network to another node&lt;/li>
&lt;li>Skip iptables DNAT which sometimes caused problems&lt;/li>
&lt;li>Upgrading connections from UDP to TCP which should reduce latency in case of dropped packets&lt;/li>
&lt;li>Enabling negative caching&lt;/li>
&lt;/ul>
&lt;p>NodeLocal DNSCache is &lt;a class="link" href="https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache" target="_blank" rel="noopener"
>available as a GKE add-on&lt;/a>.&lt;/p>
&lt;p>NodeLocal DNSCache is deployed as a &lt;code>DaemonSet&lt;/code> named &lt;code>node-local-dns&lt;/code> in &lt;code>kube-system&lt;/code> namespace. One &lt;code>node-local-dns-x&lt;/code> Pod is created on each node in the cluster.&lt;/p>
&lt;p>The &lt;code>node-local-dns-x&lt;/code> Pods run CoreDNS and are configured by the &lt;code>node-local-dns&lt;/code> ConfigMap (&lt;code>kubectl get cm node-local-dns -o yaml|yq .data.Corefile&lt;/code>).&lt;/p>
&lt;p>The configuration is similar to &lt;code>dnsmasq&lt;/code> in that requests for &lt;code>cluster.local&lt;/code>, &lt;code>in-addr.arpa&lt;/code> and &lt;code>ip6.arpa&lt;/code> are sent to &lt;code>kube-dns&lt;/code> while the rest are sent to &lt;code>8.8.8.8&lt;/code> and &lt;code>8.8.4.4&lt;/code>.&lt;/p>
&lt;p>It binds to (re-uses) the same IP as the &lt;code>kube-dns&lt;/code> Service. That way all requests from Pods on the node towards &lt;code>kube-dns&lt;/code> will actually be handled by &lt;code>node-local-dns&lt;/code> instead. And to actually communicate with &lt;code>kube-dns&lt;/code> another Service named &lt;code>kube-dns-upstream&lt;/code> is created that is a clone of the &lt;code>kube-dns&lt;/code> Service but with a different IP.&lt;/p>
&lt;p>Even though &lt;code>node-local-dns&lt;/code> uses CoreDNS and &lt;a class="link" href="https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack" target="_blank" rel="noopener"
>kube-prometheus-stack&lt;/a> has support for scraping CoreDNS it won&amp;rsquo;t necessarily work for &lt;code>node-local-dns&lt;/code>. See the next appendix for how to scrape metrics from &lt;code>node-local-dns&lt;/code>.&lt;/p>
&lt;p>&lt;a id="appendix-enabling-node-local-dns-metrics">&lt;/a>&lt;/p>
&lt;h2 id="appendix---enabling-node-local-dns-metrics">Appendix - Enabling node-local-dns metrics
&lt;/h2>&lt;p>We wanted to look at some metrics from node-local-dns, but found that we didn&amp;rsquo;t have any!&lt;/p>
&lt;p>The &lt;code>node-local-dns&lt;/code> pods do have annotations for scraping metrics:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">annotations&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">prometheus.io/port&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;9253&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">prometheus.io/scrape&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;true&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>But in our Prometheus we don&amp;rsquo;t use the &lt;code>kubernetes-pods&lt;/code> &lt;a class="link" href="https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml#L998" target="_blank" rel="noopener"
>scrape job config from the prometheus example&lt;/a>. meaning that these targets will not be discovered or scraped by prometheus. (And we don&amp;rsquo;t want to add and allow for scraping this way).&lt;/p>
&lt;p>The &lt;a class="link" href="https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack" target="_blank" rel="noopener"
>kube-prometheus-stack helm chart&lt;/a> comes with both a &lt;a class="link" href="https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/templates/exporters/core-dns/service.yaml" target="_blank" rel="noopener"
>Service&lt;/a> and &lt;a class="link" href="https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/templates/exporters/core-dns/servicemonitor.yaml" target="_blank" rel="noopener"
>ServiceMonitor&lt;/a> to scrape metrics from CoreDNS (which NodeLocal DNSCache uses).&lt;/p>
&lt;p>However the Service and ServiceMonitor uses a hardcoded selector of &lt;code>k8s-app: kube-dns&lt;/code> while &lt;code>node-local-dns&lt;/code> Pods have &lt;code>k8s-app: node-local-dns&lt;/code>.&lt;/p>
&lt;p>We made some changes to these and deployed them to the cluster and now started getting metrics in the &lt;code>coredns_dns_*&lt;/code> timeserieses.&lt;/p>
&lt;blockquote>
&lt;p>You can add our updated Service and ServiceMonitor like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl apply -f -n kube-system https://raw.githubusercontent.com/signicat/blog-attachements/main/2023-gke-node-local-dns-cache/files/node-local-dns-metrics-service.yaml
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl apply -f -n metrics https://raw.githubusercontent.com/signicat/blog-attachements/main/2023-gke-node-local-dns-cache/files/node-local-dns-servicemonitor.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>And metrics should start flowing within a couple of minutes.&lt;/p>
&lt;/blockquote>
&lt;p>In new versions of the &lt;a class="link" href="https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/templates/grafana/dashboards-1.14/k8s-coredns.yaml" target="_blank" rel="noopener"
>CoreDNS dashboard&lt;/a> that comes with &lt;code>kube-prometheus-stack&lt;/code> you should be able to select the &lt;code>node-local-dns&lt;/code> job.&lt;/p>
&lt;blockquote>
&lt;p>I added the &lt;code>job&lt;/code> template variable to the dashboard in &lt;a class="link" href="https://github.com/prometheus-community/helm-charts/pull/3798" target="_blank" rel="noopener"
>this PR&lt;/a>, which may not have made it into a new version of the &lt;code>kube-prometheus-stack&lt;/code> chart yet. In the mean time you can use &lt;a class="link" href="https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/grafana-dashboard-coredns.json" target="_blank" rel="noopener"
>our updated CoreDNS dashboard&lt;/a> which adds the necessary template variable as well as a couple of other improvements.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="appendix-using-dnsperf">&lt;/a>&lt;/p>
&lt;h2 id="appendix---using-dnsperf-to-test-dns-performance">Appendix - Using dnsperf to test DNS performance
&lt;/h2>&lt;p>Trying to tease out more information on the problem we run some DNS load testing using &lt;a class="link" href="https://linux.die.net/man/1/dnsperf" target="_blank" rel="noopener"
>dnsperf&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>An alternative to &lt;code>dnsperf&lt;/code> is &lt;a class="link" href="https://linux.die.net/man/1/resperf" target="_blank" rel="noopener"
>resperf&lt;/a> which is &amp;ldquo;a companion tool to dnsperf&amp;rdquo; designed for testing resolution performance of a caching DNS server. Whereas &lt;code>dnsperf&lt;/code> is primarily meant to test authoritative DNS servers. Since &lt;code>resperf&lt;/code> is more complicated to work with, and we want to test at a fairly low volume, we assume that &lt;code>dnsperf&lt;/code> is good enough for now.&lt;/p>
&lt;/blockquote>
&lt;p>We spin up a new pod with Ubuntu to run dnsperf from&lt;/p>
&lt;pre>&lt;code>kubectl run dnsperf -n default --image=ubuntu:22.04 -i --tty --restart=Never
&lt;/code>&lt;/pre>
&lt;p>And install dnsperf&lt;/p>
&lt;pre>&lt;code>apt-get update &amp;amp;&amp;amp; apt-get install -y dnsperf
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;ll be testing three different destinations.&lt;/p>
&lt;ul>
&lt;li>&lt;code>kube-dns&lt;/code> Service: First we use the IP of the &lt;code>kube-dns&lt;/code> Service (&lt;code>172.18.0.10&lt;/code>) which will be intercepted by the &lt;code>node-local-dns&lt;/code> Pod on the same node. It will do it&amp;rsquo;s caching and the traffic is visible in our modified CoreDNS Grafana dashboard for that node.&lt;/li>
&lt;li>&lt;code>kube-dns-upstream&lt;/code> Service: Then we use the IP of the &lt;code>kube-dns-upstream&lt;/code> Service (&lt;code>172.18.165.139&lt;/code>) which is a copy of the &lt;code>kube-dns&lt;/code> Service that &lt;code>node-local-dns&lt;/code> uses when looking up &lt;code>.cluster.local&lt;/code> domains. It has a different IP than &lt;code>kube-dns&lt;/code> so that it won&amp;rsquo;t be intercepted by &lt;code>node-local-dns&lt;/code>.&lt;/li>
&lt;li>One &lt;code>kube-dns&lt;/code> Pod: Then we use the IP of one specific &lt;code>kube-dns&lt;/code> Pod (&lt;code>172.20.14.115&lt;/code>) so that we can observe the behaviour of one instance without any load balancing sprinkling the traffic all over the place.&lt;/li>
&lt;/ul>
&lt;p>&lt;code>dnsperf&lt;/code> takes a list of domains to look up in a file that looks like&lt;/p>
&lt;pre>&lt;code>archive.ubuntu.com A
test.salesforce.com A
storage.googleapis.com A
&lt;/code>&lt;/pre>
&lt;p>To create a file with the domains we actually look up I do a packet capture on one of the &lt;code>node-local-dns&lt;/code> Pods for half an hour. Open it in Wireshark. Filter by &lt;code>dns.flags.response == 1&lt;/code>. Add DNS -&amp;gt; Queries -&amp;gt; query -&amp;gt; Name as a Column. Export Packet Dissections as CSV. And use Excel and Notepad to get a file of 50.000 DNS names in that format. Upload that file to the newly created &lt;code>dnsperf&lt;/code> Pod. Create a separate file with only &lt;code>cluster.local&lt;/code> domains as well (&lt;code>cat domains-all.txt | grep &amp;quot;cluster.local&amp;quot; &amp;gt; domains-cluster-local.txt&lt;/code>).&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>dnsperf&lt;/code> does not expand/multiply domains by adding the domains from &lt;code>search-path&lt;/code> in &lt;code>/etc/resolve.conf&lt;/code>. The list we extracted from the packet capture already have these additional lookups though so it is representative nonetheless.&lt;/p>
&lt;/blockquote>
&lt;p>Run dnsperf:&lt;/p>
&lt;pre>&lt;code>dnsperf -d domains-all.txt -l 60 -Q 100 -S 5 -t 2 -s 172.18.0.10
&lt;/code>&lt;/pre>
&lt;p>Explanation of arguments:&lt;/p>
&lt;ul>
&lt;li>&lt;code>-d domains-all.txt&lt;/code> - Domain list file.&lt;/li>
&lt;li>&lt;code>-l 60&lt;/code> - Length (duration) of test in seconds.&lt;/li>
&lt;li>&lt;code>-Q 100&lt;/code> - Queries per second.&lt;/li>
&lt;li>&lt;code>-S 5&lt;/code> - Print statistics every 5 seconds.&lt;/li>
&lt;li>&lt;code>-t 2&lt;/code> - Timeout 2 seconds.&lt;/li>
&lt;li>&lt;code>-s 172.18.0.18&lt;/code> - DNS server to query&lt;/li>
&lt;/ul>
&lt;p>Each test has different values for the arguments.&lt;/p>
&lt;p>Results from running a series of tests ranging from 100 Queries per second (QPS) to 1500 QPS:&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/2023-12-14-table-load-tests.png"
width="889"
height="499"
srcset="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/2023-12-14-table-load-tests_hu14351295037840062642.png 480w, https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/2023-12-14-table-load-tests_hu1398839472891939138.png 1024w"
loading="lazy"
alt="Load test results table"
class="gallery-image"
data-flex-grow="178"
data-flex-basis="427px"
>&lt;/p>
&lt;blockquote>
&lt;p>These results are from a second round of testing a couple of weeks after the first. On the first round I observed a lot (6%) timing out on just 200 QPS directly towards a &lt;code>kube-dns&lt;/code> Pod. I&amp;rsquo;m not sure why the results are suddenly much better. Looking at the graphs from &lt;code>node-local-dns&lt;/code> there is a significant improvement overall some days before the second round of testing. We have not done any changes that could explain the sudden improvement. I guess it&amp;rsquo;s just one of those things&amp;hellip;&lt;/p>
&lt;p>CoreDNS did &lt;a class="link" href="https://coredns.io/2018/11/27/cluster-dns-coredns-vs-kube-dns/" target="_blank" rel="noopener"
>a benchmark&lt;/a> of &lt;code>kube-dns&lt;/code> and &lt;code>CoreDNS&lt;/code> and managed to get ~36.000 QPS on internal names and ~2.200 QPS on external names on &lt;code>kube-dns&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="appendix-network-packet-capture">&lt;/a>&lt;/p>
&lt;h2 id="appendix---network-packet-capture-without-dependencies">Appendix - Network packet capture without dependencies
&lt;/h2>&lt;p>I haven&amp;rsquo;t done packet capture in a Kubernetes cluster before. First I tried &lt;a class="link" href="https://github.com/eldadru/ksniff" target="_blank" rel="noopener"
>ksniff&lt;/a> that &lt;a class="link" href="https://anythingsimple.medium.com/how-to-do-network-sniff-for-kubernetes-pod-running-on-gke-fb23d0b63e95" target="_blank" rel="noopener"
>this blog post describes&lt;/a>. But no dice.&lt;/p>
&lt;blockquote>
&lt;p>Another method that is much more reliable is running tcpdump in a &lt;a class="link" href="https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container" target="_blank" rel="noopener"
>debug container&lt;/a> as mentioned &lt;a class="link" href="https://downey.io/blog/kubernetes-ephemeral-debug-container-tcpdump/" target="_blank" rel="noopener"
>here&lt;/a>. He pipes tcpdump from a container directly to wireshark on his own machine. But since I&amp;rsquo;m using &lt;code>kubectl&lt;/code> etc inside Ubuntu on WSL2 on Windows that is probably going to require much more setup.&lt;/p>
&lt;/blockquote>
&lt;p>Instead I&amp;rsquo;m opting for just saving packet captures in the container as files and copy them to a directory on my machine accessible by Windows.&lt;/p>
&lt;pre>&lt;code># Spin up a new container using the ubuntu:22.04 image in the existing node-local-dns-7vqpp Pod. Try to attach to the process-namespace of the node-cache container if possible.
kubectl debug -c debug -n kube-system -it node-local-dns-7vqpp --image=ubuntu:22.04 --target=node-cache
&lt;/code>&lt;/pre>
&lt;p>As soon as the image is downloaded and container started your console should give you a new shell inside the container where we can start preparing:&lt;/p>
&lt;pre>&lt;code># Update apt and install tshark. Enter &amp;quot;no&amp;quot; for &amp;quot;Should non-superusers be able to capture packets?&amp;quot;
apt-get update &amp;amp;&amp;amp; apt-get install -y tshark
# Capture 100 packets from eth0 to verify that things are working
tshark -i eth0 -n -c 100
&lt;/code>&lt;/pre>
&lt;p>At first I captured 1 million packets without any filters. That resulted in a 6GB file which may be a bit on the large side when I&amp;rsquo;m just interested in some DNS lookups. So lets add a capture filter &lt;code>port 53&lt;/code> going forward.&lt;/p>
&lt;p>Capturing DNS packets to and from a &lt;code>node-local-dns&lt;/code> Pod:&lt;/p>
&lt;pre>&lt;code># In the shell running in the debug container with tshark installed:
date; tshark -i eth0 -n -c 1000000 -f &amp;quot;port 53&amp;quot; -w node-local-dns-7vqpp-eth0.pcap
&lt;/code>&lt;/pre>
&lt;p>After either capturing 100.000 packets or I&amp;rsquo;m happy I stop the capture with &lt;code>Ctrl+C&lt;/code> and I can download the pcap file in WSL:&lt;/p>
&lt;pre>&lt;code># On your local machine:
kubectl cp -c debug kube-system/node-local-dns-7vqpp:node-local-dns-7vqpp-eth0.pcap /mnt/c/Data/node-local-dns-7vqpp-eth0.pcap
&lt;/code>&lt;/pre>
&lt;p>Then browse to &lt;code>C:\Data\&lt;/code> where you can open &lt;code>node-local-dns-7vqpp-eth0.pcap&lt;/code> in &lt;a class="link" href="https://www.wireshark.org/" target="_blank" rel="noopener"
>Wireshark&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>I print the current time on the Pod just before starting packet capture. Most of the time the correct UTC time will be recorded on the packets. But in case it isn&amp;rsquo;t and the time starts from 0, I can use that time to adjust the offset at least roughly. Making it easier to correlate packet captures from different Pods.&lt;/p>
&lt;/blockquote>
&lt;p>To capture on the receiving &lt;code>kube-dns&lt;/code> Pod, both incoming DNS traffic to the &lt;code>dnsmasq&lt;/code> container on port 53 on &lt;code>eth0&lt;/code>, and between &lt;code>dnsmasq&lt;/code> and &lt;code>kube-dns&lt;/code> containers on &lt;code>lo&lt;/code> port 10053:&lt;/p>
&lt;pre>&lt;code>kubectl debug -c debug -n kube-system -it kube-dns-d7bc86d4c-d2x8p --image=ubuntu:22.04 --target=dnsmasq
# Install tshark as shown above
# Start two packet captures running in the background:
date; tshark -i eth0 -n -c 1000000 -f &amp;quot;port 53&amp;quot; -w kube-dns-d7bc86d4c-d2x8p-eth0.pcap &amp;amp;
date; tshark -i lo -n -c 1000000 -f &amp;quot;port 10053&amp;quot; -w kube-dns-d7bc86d4c-d2x8p-lo.pcap &amp;amp;
# To stop these type `fg` in the shell to bring a background process to the foreground and stop it with `Ctrl+C`.
# Then `fg` and `Ctrl+C` again to stop the other.
&lt;/code>&lt;/pre>
&lt;p>Download the files to my local machine as before:&lt;/p>
&lt;pre>&lt;code>kubectl cp -c debug kube-system/kube-dns-d7bc86d4c-d2x8p:kube-dns-d7bc86d4c-d2x8p-eth0.pcap /mnt/c/Data/kube-dns-d7bc86d4c-d2x8p-eth0.pcap
kubectl cp -c debug kube-system/kube-dns-d7bc86d4c-d2x8p:kube-dns-d7bc86d4c-d2x8p-lo.pcap /mnt/c/Data/kube-dns-d7bc86d4c-d2x8p-lo.pcap
&lt;/code>&lt;/pre>
&lt;p>&lt;em>Check out &lt;a class="link" href="#analyzing-dns-problems-based-on-packet-capture" >Analyzing DNS problems based on packet captures&lt;/a> for some tips and tricks on analyzing the packet captures.&lt;/em>&lt;/p>
&lt;p>&lt;a id="appendix-verbose-logging-on-kube-dns">&lt;/a>&lt;/p>
&lt;h2 id="appendix---verbose-logging-on-kube-dns">Appendix - Verbose logging on &lt;code>kube-dns&lt;/code>
&lt;/h2>&lt;p>The &lt;a class="link" href="https://dnsmasq.org/docs/dnsmasq-man.html" target="_blank" rel="noopener"
>dnsmasq man page&lt;/a> lists several interesting options such as &lt;code>--log-queries=extra&lt;/code> and &lt;code>--log-debug&lt;/code>!&lt;/p>
&lt;p>But it&amp;rsquo;s not possible to make changes to the &lt;code>kube-dns&lt;/code> Deployment since it&amp;rsquo;s managed by GKE. Any changes you make will be reverted immediately.&lt;/p>
&lt;p>Instead we take the existing manifests for &lt;code>kube-dns&lt;/code>, modify them and create a parallel deployment that we control:&lt;/p>
&lt;p>&lt;a class="link" href="https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/kube-dns-deployment.yaml" target="_blank" rel="noopener"
>kube-dns-deployment.yaml&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Deployment named &lt;code>kube-dns-debug&lt;/code>.&lt;/li>
&lt;li>A couple of annotations commented out that would otherwise make the Deployment be instantly removed.&lt;/li>
&lt;li>Keep existing &lt;code>k8s-app: kube-dns&lt;/code> label on the Pods so they will receive traffic for the &lt;code>kube-dns-upstream&lt;/code> Service.&lt;/li>
&lt;li>An additional &lt;code>reason: debug&lt;/code> label on the Pods so we can target them specifically.&lt;/li>
&lt;li>Additional logging enabled with &lt;code>--log-queries=extra&lt;/code>, &lt;code>--log-debug&lt;/code> and &lt;code>--log-async=25&lt;/code>. I enabled these one at a time but the log volume with everything enabled isn&amp;rsquo;t overwhelming.&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/kube-dns-debug-service.yaml" target="_blank" rel="noopener"
>kube-dns-debug-service.yaml&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Service named &lt;code>kube-dns-debug&lt;/code> with port udp+tcp/53 targeting only the Pods with the added &lt;code>reason: debug&lt;/code> label. I used this service to run load tests only towards these Pods.&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/kube-dns-metrics-service.yaml" target="_blank" rel="noopener"
>kube-dns-metrics-service.yaml&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Service named &lt;code>kube-dns-debug-metrics&lt;/code> with port tcp/10054 and tcp/10055 targeting the same Pods as above but for exposing metrics.&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/kube-dns-servicemonitor.yaml" target="_blank" rel="noopener"
>kube-dns-servicemonitor.yaml&lt;/a>&lt;/p>
&lt;ul>
&lt;li>ServiceMonitor named &lt;code>kube-dns-debug&lt;/code> that targets the &lt;code>kube-dns-debug-metrics&lt;/code> Service above.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Strictly speaking the &lt;code>kube-dns-debug&lt;/code> Service, &lt;code>kube-dns-debug-metrics&lt;/code> Service and &lt;code>kube-dns-debug&lt;/code> ServiceMonitor isn&amp;rsquo;t necessary if using the &lt;code>k8s-app: kube-dns&lt;/code> label on the new Pods. But it makes it possible to separate the two deployments completely by using another label on the Pods such as &lt;code>k8s-app: kube-dns-debug&lt;/code> and thus avoid for example load tests affecting real cluster DNS traffic.&lt;/p>
&lt;/blockquote>
&lt;p>Now some DNS requests should arrive at the &lt;code>kube-dns-debug&lt;/code> Pods with additional logging enabled.&lt;/p>
&lt;p>It&amp;rsquo;s also possible to force all DNS traffic to this Pod by manually adding the &lt;code>reason: debug&lt;/code> label as a selector on the &lt;code>kube-dns-upstream&lt;/code> Service. It will stay that way for &amp;ldquo;a while&amp;rdquo; (hours, maybe days) before being reverted. Plenty of time to play around at least.&lt;/p>
&lt;p>&lt;a id="appendix-analyzing-dnsmasq-logs">&lt;/a>&lt;/p>
&lt;h2 id="appendix---analyzing-dnsmasq-logs">Appendix - Analyzing &lt;code>dnsmasq&lt;/code> logs
&lt;/h2>&lt;p>Once we have increased the log verbosity of &lt;code>dnsmasq&lt;/code> we can see if there&amp;rsquo;s anything to learn there.&lt;/p>
&lt;p>First I used &lt;a class="link" href="https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/grafana-dashboard-coredns.json" target="_blank" rel="noopener"
>our updated CoreDNS dashboard&lt;/a> to identify a time interval where we observed latency spikes. Then using loki, our centralized log storage, I download about 5000 lines of logs spanning about 50 seconds worth of logs. (Our loki is limited to loading 5000 lines, therefore it&amp;rsquo;s important to try to narrow down and find logs where we are actually experiencing issues).&lt;/p>
&lt;pre>&lt;code>2023-10-10T10:02:36+02:00 I1010 08:02:36.571446 1 nanny.go:146] dnsmasq[4811]: 315207 10.0.0.83/56382 forwarded metadata.google.internal.cluster.local to 127.0.0.1#10053
2023-10-10T10:02:36+02:00 I1010 08:02:36.571457 1 nanny.go:146] dnsmasq[4811]: 315207 10.0.0.83/56382 reply metadata.google.internal.cluster.local is NXDOMAIN
2023-10-10T10:02:36+02:00 I1010 08:02:36.571488 1 nanny.go:146] dnsmasq[4810]: 315107 10.0.0.83/24595 forwarded metadata.google.internal.cluster.local to 127.0.0.1#10053
2023-10-10T10:02:36+02:00 I1010 08:02:36.571495 1 nanny.go:146] dnsmasq[4810]: 315107 10.0.0.83/24595 reply metadata.google.internal.cluster.local is NXDOMAIN
2023-10-10T10:02:36+02:00 I1010 08:02:36.575810 1 nanny.go:146] dnsmasq[4810]: 315108 10.0.0.83/24595 query[A] metadata.google.internal.some-namespace.svc.cluster.local from 10.0.0.83
2023-10-10T10:02:36+02:00 I1010 08:02:36.576101 1 nanny.go:146] dnsmasq[4810]: 315108 10.0.0.83/24595 forwarded metadata.google.internal.some-namespace.svc.cluster.local to 127.0.0.1#10053
2023-10-10T10:02:36+02:00 I1010 08:02:36.576132 1 nanny.go:146] dnsmasq[4810]: 315108 10.0.0.83/24595 reply metadata.google.internal.some-namespace.svc.cluster.local is NXDOMAIN
&lt;/code>&lt;/pre>
&lt;p>Here we can see exact time (&lt;code>08:02:36.571446&lt;/code>). Which &lt;code>dnsmasq&lt;/code> process logged the line, identified by the process ID (&lt;code>dnsmasq[4811]&lt;/code>). A number identifying an individual request (&lt;code>315207&lt;/code>). And the client IP and port (&lt;code>10.0.0.83/56382&lt;/code>). One process always maps to one TCP connection so PID and client IP and port will always match.&lt;/p>
&lt;p>Just from this snippet we can see that multiple DNS requests are handled by each process and we have exact time for when individual requests were received from the client as well as exact time for replies.&lt;/p>
&lt;p>I wrote a small (and ugly) &lt;a class="link" href="https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/analyze-dnsmasq-logs/main.go" target="_blank" rel="noopener"
>program&lt;/a> to analyze the logs. It gathers the duration of each request identified by the request number and each &amp;ldquo;session&amp;rdquo; identified by the process ID.&lt;/p>
&lt;p>It filters out requests faster than 2 milliseconds and sessions shorter than 500 milliseconds. The output looks like:&lt;/p>
&lt;pre>&lt;code>Request: PID 5439 RequestNumber 356558 Domain . Duration 2.273ms
Request: PID 4914 RequestNumber 319022 Domain api.statuspage.io.some-namespace.svc.cluster.local Duration 2.619ms
Request: PID 4915 RequestNumber 319123 Domain api.statuspage.io.cluster.local Duration 3.088ms
Session: PID 4890 Duration 1.005229s
Session: PID 5022 Duration 852.861ms
Session: PID 5435 Duration 2.627174s
&lt;/code>&lt;/pre>
&lt;p>The vast majority of individual requests complete in microseconds, and none are slower than 10 milliseconds. This is an indication that the delays aren&amp;rsquo;t coming from the processing of individual requests.&lt;/p>
&lt;p>Sessions however last much longer, regularly in the 1-3 second range. This isn&amp;rsquo;t necessarily a problem since it&amp;rsquo;s resource efficient to keep sessions longer and re-using them for many requests.&lt;/p>
&lt;p>&lt;a id="appendix-analyzing-concurrent-tcp-connections">&lt;/a>&lt;/p>
&lt;h2 id="appendix---analyzing-concurrent-tcp-connections">Appendix - Analyzing concurrent TCP connections
&lt;/h2>&lt;p>I want to see how many connections are open at any point (and not impacted by metric scraping intervals etc) as well as how long they tend to stay idle before being closed.&lt;/p>
&lt;p>I made another small (and probably even uglier) &lt;a class="link" href="https://github.com/signicat/blog-attachements/blob/main/2023-gke-node-local-dns-cache/files/analyze-tcp-conns/main.go" target="_blank" rel="noopener"
>program&lt;/a> to analyze the packet captures (exported from Wireshark as CSV) from &lt;code>kube-dns&lt;/code> and try to answer those questions.&lt;/p>
&lt;p>While iterating through every packet it keeps a counter on how many distinct connections are observed as well as the time since the previous packet in the same connection was observed:&lt;/p>
&lt;pre>&lt;code>Time: 08:08:30.610010781 TimeShort: 08:08:30 Port: 61236 Action: open Result: none Flags: PSHACK IdleGroup: fast ConIdleTime: 564.26µs ActiveConnections: 11
Time: 08:08:30.610206151 TimeShort: 08:08:30 Port: 10806 Action: open Result: none Flags: ACK IdleGroup: fast ConIdleTime: 178.4µs ActiveConnections: 11
Time: 08:08:30.900267009 TimeShort: 08:08:30 Port: 62083 Action: open Result: none Flags: FINACK IdleGroup: slow ConIdleTime: 6.429595796s ActiveConnections: 11
&lt;/code>&lt;/pre>
&lt;p>Plotting &lt;code>ActiveConnections&lt;/code> in the Excel graph we have from &lt;a class="link" href="#analyzing-dns-problems-based-on-packet-capture" >Analyzing DNS problems based on packet captures&lt;/a>:&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-and-tcp-conns-and-packet-latency.png"
width="1428"
height="489"
srcset="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-and-tcp-conns-and-packet-latency_hu6833197055541243622.png 480w, https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-proc-and-tcp-conns-and-packet-latency_hu1786475887642530671.png 1024w"
loading="lazy"
alt="Graph showing active TCP connections in green. DNS resolution time from packet captures in blue. Number of dnsmasq processes in orange"
class="gallery-image"
data-flex-grow="292"
data-flex-basis="700px"
>&lt;/p>
&lt;p>Up to the limit of 20 &lt;code>dnsmasq&lt;/code> processes they follow the number of open TCP connections pretty closely. However for long periods of time there are way more open connections than &lt;code>dnsmasq&lt;/code> is allowed to spawn new child processes to handle. This also overlaps with the sudden huge increases in latency. Another thing we can infer from this is that new TCP connections are successfully being opened from &lt;code>node-local-dns&lt;/code> (CoreDNS) to &lt;code>dnsmasq&lt;/code>, even though &lt;code>dnsmasq&lt;/code> is unable to handle them yet. Probably the master &lt;code>dnsmasq&lt;/code> process accepts the connections but blocking the request until there is room to spawn a new child process.&lt;/p>
&lt;p>&lt;code>grep&lt;/code>ing for &amp;ldquo;slow&amp;rdquo; we get all packets where the connection was idle for more than 1 second:&lt;/p>
&lt;pre>&lt;code>Time: 08:08:18.064874738 TimeShort: 08:08:18 Port: 19956 Action: open Result: none Flags: FINACK IdleGroup: slow ConIdleTime: 7.535928188s ActiveConnections: 9
Time: 08:08:18.064888758 TimeShort: 08:08:18 Port: 30168 Action: open Result: none Flags: FINACK IdleGroup: slow ConIdleTime: 7.577724318s ActiveConnections: 9
Time: 08:08:18.064909758 TimeShort: 08:08:18 Port: 41718 Action: open Result: none Flags: FINACK IdleGroup: slow ConIdleTime: 7.393646609s ActiveConnections: 9
Time: 08:08:18.064911088 TimeShort: 08:08:18 Port: 30386 Action: open Result: none Flags: FINACK IdleGroup: slow ConIdleTime: 7.535962768s ActiveConnections: 9
Time: 08:08:18.064920468 TimeShort: 08:08:18 Port: 21482 Action: open Result: none Flags: FINACK IdleGroup: slow ConIdleTime: 7.535946008s ActiveConnections: 9
&lt;/code>&lt;/pre>
&lt;p>This is particularly interesting since &lt;code>node-local-dns&lt;/code> is configured to expire connections after 1 second. And in theory should be cleaned up after at most 2 seconds. I managed to keep myself from diving head first into that particular rabbit hole though.&lt;/p>
&lt;blockquote>
&lt;p>The underlying &lt;a class="link" href="https://github.com/miekg/dns/" target="_blank" rel="noopener"
>dns Go library&lt;/a> that CoreDNS uses also has a &lt;a class="link" href="https://github.com/miekg/dns/blob/master/server.go#L18" target="_blank" rel="noopener"
>limit of 128 DNS queries&lt;/a> for a single TCP connection before closing it.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="analyzing-dns-problems-based-on-packet-capture">&lt;/a>&lt;/p>
&lt;h2 id="appendix---analyzing-dns-problems-based-on-packet-captures">Appendix - Analyzing DNS problems based on packet captures
&lt;/h2>&lt;p>Now that we have some packet captures we can start dissecting and analyzing and looking for the needle in the haystack.&lt;/p>
&lt;p>I&amp;rsquo;ll be using &lt;a class="link" href="https://www.wireshark.org/" target="_blank" rel="noopener"
>Wireshark&lt;/a> for this.&lt;/p>
&lt;p>&lt;a class="link" href="https://www.youtube.com/watch?v=RRjutHjGdCY" target="_blank" rel="noopener"
>This youtube video&lt;/a> shows a few neat tricks. Thanks!&lt;/p>
&lt;ul>
&lt;li>If you are looking at the traffic between &lt;code>dnsmasq&lt;/code> and &lt;code>kube-dns&lt;/code> (&lt;code>lo&lt;/code> network interface) go to &lt;strong>Analyze&lt;/strong> -&amp;gt; &lt;strong>Decode&lt;/strong> as and add a mapping from port 10053 to DNS to have Wireshark decode the packets correctly.&lt;/li>
&lt;li>Start by applying the display filter &lt;code>dns.flags.response == 1&lt;/code> to only show DNS responses.&lt;/li>
&lt;li>Find a DNS Response packet and find the &lt;code>[Time: n.nnn seconds]&lt;/code> field, right click it and &lt;strong>Apply as Column&lt;/strong>.&lt;/li>
&lt;li>You can also add &lt;code>Transaction ID&lt;/code> and the &lt;code>Name&lt;/code> field (under &lt;strong>Queries&lt;/strong>) as columns as well.&lt;/li>
&lt;/ul>
&lt;p>Then you can export as CSV for example in &lt;strong>File&lt;/strong> -&amp;gt; &lt;strong>Export Packet Dissections&amp;hellip;&lt;/strong> -&amp;gt; As &lt;strong>CSV&lt;/strong>.&lt;/p>
&lt;p>Importing the data from the &lt;code>node-local-dns&lt;/code> packet capture into Excel (yes, there&amp;rsquo;s no way escaping Excel!) and plotting the duration of each and every DNS lookup over a 20 minute period, colored by upstream server:&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dns-lookup-time-by-upstream-server-over-time.png"
width="1948"
height="694"
srcset="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dns-lookup-time-by-upstream-server-over-time_hu1554638069529912429.png 480w, https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dns-lookup-time-by-upstream-server-over-time_hu8836196662265939971.png 1024w"
loading="lazy"
alt="Graph showing DNS resolution time colored by upstream server based on packet captures on node-local-dns Pod"
class="gallery-image"
data-flex-grow="280"
data-flex-basis="673px"
>&lt;/p>
&lt;p>We clearly see huge spikes in latency. But it seems to only affect queries being forwarded to the two &lt;code>kube-dns&lt;/code> Pods (&lt;code>172.20.1.38&lt;/code> &amp;amp; &lt;code>172.20.7.57&lt;/code>).&lt;/p>
&lt;p>Another interesting finding is that it appears to happen at the exact same time on both &lt;code>kube-dns&lt;/code> Pods. Weird. If we didn&amp;rsquo;t already know that (for at least some queries) the added duration happens inside the &lt;code>dnsmasq&lt;/code> container, I would probably suspect a problem on the network.&lt;/p>
&lt;p>Plotting the duration on requests arriving at &lt;code>dnsmasq&lt;/code> on one of the &lt;code>kube-dns&lt;/code> Pods shows the same pattern:&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-latency.png"
width="1568"
height="711"
srcset="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-latency_hu12456545354413195016.png 480w, https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-latency_hu7087990665509006188.png 1024w"
loading="lazy"
alt="Graph showing DNS resolution time based on packet captures on kube-dns Pod"
class="gallery-image"
data-flex-grow="220"
data-flex-basis="529px"
>&lt;/p>
&lt;p>Another thing I noticed is that sometimes close to when request duration would spike, Wireshark warns about &lt;code>TCP Port numbers reused&lt;/code>:&lt;/p>
&lt;pre>&lt;code>42878 19:09:03.987437048 127.0.0.1 127.0.0.1 TCP 78 44516 [TCP Port numbers reused] 44516 → 10053 [SYN] Seq=0 Win=43690 Len=0
&lt;/code>&lt;/pre>
&lt;p>However Wireshark doesn&amp;rsquo;t discriminate whether that was 20 minutes or 2 seconds ago. Only that it occurs in the same packet capture.&lt;/p>
&lt;p>One hypothesis I had was that outgoing requests from &lt;code>dnsmasq&lt;/code> to &lt;code>kube-dns&lt;/code> would be stuck waiting for available TCP ports. I plotted the source port usage over time:&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-source-port.png"
width="1569"
height="695"
srcset="https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-source-port_hu14734794848197582075.png 480w, https://blog.stian.omg.lol/p/kubernetes-dont-use-nodelocal-dnscache-on-gke-without-cloud-dns/dnsmasq-source-port_hu12858696458376013384.png 1024w"
loading="lazy"
alt="Graph showing TCP source port for connections from node-local-dns based on packet captures"
class="gallery-image"
data-flex-grow="225"
data-flex-basis="541px"
>&lt;/p>
&lt;p>It did not strengthen my suspicion and some random checking shows that the TCP Port reuse is far enough spaced in time (minutes) to avoid problems. So for the time being I&amp;rsquo;m not digging further into this.&lt;/p>
&lt;p>&lt;a id="footnotes">&lt;/a>&lt;/p>
&lt;h1 id="footnotes">Footnotes
&lt;/h1>&lt;p>&lt;a id="footnote-a">&lt;/a>
&lt;strong>[A]&lt;/strong> This doesn&amp;rsquo;t strictly mean that 20 new TCP connections are required since many of them are probably retries.&lt;/p>
&lt;p>&lt;a id="footnote-b">&lt;/a>
&lt;strong>[B]&lt;/strong> Note that even if the graph doesn&amp;rsquo;t hit 22 you may still be affected. The number of processes is counted only at the exact time the metrics are scraped, in our case 10 seconds. You can manually sample the process count by attaching a debug container to &lt;code>dnsmasq&lt;/code> (&lt;code>kubectl debug -c debug -n kube-system -it kube-dns-6fb7c8866c-bxj7f --image=ubuntu:22.04 --target=dnsmasq&lt;/code>) and running &lt;code>for i in $(seq 1 1800) ; do echo &amp;quot;$(date) Try: ${i} DnsmasqProcess: $(pidof dnsmasq | wc -w)&amp;quot;; sleep 1; done&lt;/code>.&lt;/p>
&lt;p>&lt;a id="footnote-c">&lt;/a>
&lt;strong>[C]&lt;/strong> CPU throttling would not be an issue in this case since &lt;code>kube-dns&lt;/code> does not have CPU limits set.&lt;/p>
&lt;p>&lt;a id="footnote-d">&lt;/a>
&lt;strong>[D]&lt;/strong> Although you should be cautious of calling services by name in other namespaces if they are owned by different teams. As that introduces coupling on what should be implementation details across team boundaries. In Signicat we always call the full FQDN and path if connecting to services owned by other teams.&lt;/p></description></item><item><title>Kubernetes Sidecar Config Drift</title><link>https://blog.stian.omg.lol/p/kubernetes-sidecar-config-drift/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://blog.stian.omg.lol/p/kubernetes-sidecar-config-drift/</guid><description>&lt;img src="https://blog.stian.omg.lol/p/kubernetes-sidecar-config-drift/2022-05-03-kubernetes-sidecar-config-drift.png" alt="Featured image of post Kubernetes Sidecar Config Drift" />&lt;p>&lt;em>This is a cross-post of a blog post also published on the &lt;a class="link" href="https://www.signicat.com/blog/kubernetes-your-sidecar-configurations-are-drifting" target="_blank" rel="noopener"
>Signicat Blog&lt;/a>&lt;/em>&lt;/p>
&lt;hr>
&lt;p>Huge thanks to one of my favorite clients, &lt;a class="link" href="https://www.signicat.com/" target="_blank" rel="noopener"
>Signicat&lt;/a>, and especially &lt;a class="link" href="https://www.linkedin.com/in/jon-skarpeteig/" target="_blank" rel="noopener"
>Jon&lt;/a>, for allowing me to share some of the nitty gritty details of a challenge that I believe is probably quite widespread, yet under-appreciated, in modern Kubernetes cloud environments.&lt;/p>
&lt;p>Last week, working on Signicat’s next generation cloud platform, I discovered that several individuals invented their own ways of mitigating what I now call Sidecar Configuration Drift.&lt;/p>
&lt;p>To ease the pain I created &lt;a class="link" href="https://github.com/StianOvrevage/k8s-sidecar-rollout" target="_blank" rel="noopener"
>k8s-sidecar-rollout&lt;/a> to restart the required workloads and thereby updating their sidecar configurations.&lt;/p>
&lt;p>This blog post is a bit of background information on what the causes of this problem is.&lt;/p>
&lt;h2 id="what-is-sidecar-config-drift">What is Sidecar Config Drift?
&lt;/h2>&lt;p>When using a Sidecar Injector (such as Istio), there is nothing that ensures that an update (potentially breaking) to a sidecar config template is applied/updated on Pods that have already been injected with a sidecar.&lt;/p>
&lt;p>This means that after updating a sidecar config it may take a very long time until all Pods have the updated config. They may receive the updates at any undetermined time in the future. While the updates are pending things might not work as expected and things may not be compliant. When the update is finally applied to the Pod it may surface breaking changes.&lt;/p>
&lt;p>I call this phenomena &lt;em>Sidecar Configuration Drift&lt;/em>.&lt;/p>
&lt;h2 id="background">Background
&lt;/h2>&lt;h3 id="kubernetes---declarative-vs-imperative">Kubernetes - Declarative vs imperative
&lt;/h3>&lt;p>What makes Kubernetes so powerful is also what can make it hard and confusing to work with until your mindset has shifted.&lt;/p>
&lt;p>That is the philosophy of being declarative instead of imperative like most of us are used to for the past 20 years.&lt;/p>
&lt;p>&lt;strong>Declarative means you tell Kubernetes HOW you want things to look. You DON’T tell Kubernetes WHAT to do.&lt;/strong>&lt;/p>
&lt;p>For example you do not tell Kubernetes to scale your Deployment to 5 instances. You tell Kubernetes that your Deployment should have 5 instances. The difference is subtle but extremely important. In a highly dynamic cloud environment your instances might disappear or crash for a multitude of reasons. In this declarative mindset you should not care about that since Kubernetes is tasked with ensuring 5 instances and will (try to) provision new ones when it’s needed.&lt;/p>
&lt;p>This is the apparent magic which makes Kubernetes.&lt;/p>
&lt;p>This magic is technically solved with what we call &lt;strong>Controllers&lt;/strong>. A controller is responsible for constantly comparing the &lt;strong>Actual state&lt;/strong> and &lt;strong>Desired state&lt;/strong>. To scale your Deployment to 5 instances you set &lt;code>replicas: 5&lt;/code> on the &lt;code>Deployment&lt;/code> resource. If needed a controller will create a completely new &lt;code>ReplicaSet&lt;/code> resource with 5 instances. Another controller will then create 5 &lt;code>Pods&lt;/code>. And the scheduler will finally try to place and start those &lt;code>Pods&lt;/code> on actual nodes. The &lt;code>ReplicaSet&lt;/code> controller will scale down the old once the new has reached it’s desired state.&lt;/p>
&lt;p>This constant process is called a &lt;strong>reconciliation loop&lt;/strong> and it’s a critical feature.&lt;/p>
&lt;h3 id="sidecars">Sidecars
&lt;/h3>&lt;p>Sidecar is a Kubernetes design pattern. A sidecar is simply a container that is living side-by-side with the main application container that can do tasks that are logically not part of the application. Examples of this can be log handling, monitoring agents, proxies. &lt;strong>All containers in a Pod (including sidecars) share the same filesystem, kernel namespace, IP addresses etc.&lt;/strong>&lt;/p>
&lt;p>More about sidecars: &lt;a class="link" href="https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns/" target="_blank" rel="noopener"
>The Distributed System ToolKit: Patterns for Composite Containers&lt;/a>&lt;/p>
&lt;h3 id="sidecar-injection">Sidecar injection
&lt;/h3>&lt;p>Sidecar injection is when a sidecar container is added to a Pod &lt;strong>even if the sidecar isn’t defined in any of the higher level primitives, such as &lt;code>Deployment&lt;/code> or &lt;code>StatefulSet&lt;/code>.&lt;/strong>&lt;/p>
&lt;p>When for example the &lt;code>ReplicaSet&lt;/code> controller will &lt;code>Create&lt;/code> a new &lt;code>Pod&lt;/code>. If configured, the Kubernetes API will call one or more &lt;code>MutatingWebhooks&lt;/code>. These webhooks can then change the &lt;code>Pod&lt;/code> definition before they are saved (and picked up by the scheduler).&lt;/p>
&lt;h3 id="the-problem">The Problem
&lt;/h3>&lt;p>The problem is when updating a sidecar injection template there is no system that runs a reconciliation loop.&lt;/p>
&lt;p>The webhook just updates the &lt;code>Pod&lt;/code> template. It does not keep track of which &lt;code>Pods&lt;/code> have gotten which template or check if any template change would result in a different sidecar configuration.&lt;/p>
&lt;p>The controllers ALSO does not continuously monitor if and how re-creating the same &lt;code>Pod&lt;/code> (without sidecars) would result in a different Pod once the sidecars have been injected.&lt;/p>
&lt;p>In effect sidecar injection does not follow the expected declarative pattern that the rest of Kubernetes does.&lt;/p>
&lt;h3 id="consequences">Consequences
&lt;/h3>&lt;p>If a platform team changes the istio sidecar template it will not actually take effect on a &lt;code>Pod&lt;/code> until that &lt;code>Pod&lt;/code> for some reason is re-created.&lt;/p>
&lt;p>Let’s assume the &lt;code>istio-proxy&lt;/code> sidecar template have been updated by the platform team. We roll it out and test it and it seems to work. But the change will break some applications running in the cluster.&lt;/p>
&lt;p>That breakage will go un-noticed until:&lt;/p>
&lt;pre>&lt;code>The product team commits changes that triggers a re-deploy. The deployment will suddenly fail but it might not have anything to do with the actual changes the team did to the application. This is surely confusing!
The platform team for example upgrades a pool of worker nodes causing all `Pods` to be re-created on new nodes.
A Pod is re-created when a Kubernetes worker node crashes. In this scenario it appears the failure spawned into existence out of nowhere since neither the product team nor platform team actually “did” anything to trigger it.
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>Also worth noting is that any attempts at Rolling back a Deployment now containing failing Pods will not actually fix anything.&lt;/strong>&lt;/p>
&lt;p>It’s the sidecar templates that needs to be rolled back and Pods probably need to be re-created again.&lt;/p>
&lt;h3 id="mitigations">Mitigations
&lt;/h3>&lt;p>We can mitigate drift by:&lt;/p>
&lt;pre>&lt;code>Re-starting all Pods in the cluster whenever we update sidecar injection templates.
Sometimes we might forget to re-start so regularly re-start all Pods in the cluster anyway.
&lt;/code>&lt;/pre>
&lt;h2 id="k8s-sidecar-rollout">k8s-sidecar-rollout
&lt;/h2>&lt;p>To make these restarts easy and fast I’ve created &lt;a class="link" href="https://github.com/StianOvrevage/k8s-sidecar-rollout" target="_blank" rel="noopener"
>https://github.com/StianOvrevage/k8s-sidecar-rollout&lt;/a> .&lt;/p>
&lt;p>It’s a tool that figures out (with your help) which workloads (Deployment, StatefulSet, DaemonSet) that needs to be rolled out again (re-started) and then rolls out for you. Head over to the GitHub repo for installation and complete usage instructions. Here is an example of how it can be used:&lt;/p>
&lt;pre>&lt;code>python3 sidecar-rollout.py \
--sidecar-container-name=istio-proxy \
--include-daemonset=true \
--annotation-prefix=myCompany \
--parallel-rollouts 10 \
--only-started-before=&amp;quot;2022-05-01 13:00&amp;quot; \
--exclude-namespace=kube-system \
--confirm=true
&lt;/code>&lt;/pre>
&lt;p>This will gather all Pods with a container named &lt;code>istio-sidecar&lt;/code> belonging to a Deployment or DaemonSet that was started before 2022-05-01 13:00 (which may be when we updated the istio sidecar config template) excluding the &lt;code>kube-system&lt;/code> namespace. It will patch the workloads with two annotations with &lt;code>myCompany&lt;/code> prefix and run 10 rollouts in parallel.&lt;/p>
&lt;p>The script that now re-starts Pods adds two annotations indicating that a restart to update sidecars has occurred as well as the time:&lt;/p>
&lt;pre>&lt;code>$ kubectl get pods -n product-team some-api-7cdc65482b-ged13 -o yaml | yq '.metadata.annotations'
sidecarRollout.rollout.timestamp: 2022-05-03T18:05:31
sidecarRollout.rollout.reason: Update sidecars istio-proxy
&lt;/code>&lt;/pre>
&lt;p>The idea is that if your Pods are suddenly failing, you can quickly check the annotations and see if it has anything to do with sidecar updates or not.&lt;/p>
&lt;p>These annotations will of course disappear again when a Deployment is updated.&lt;/p></description></item><item><title>A side quest in API development, observability, Kubernetes and cloud with a hint of database</title><link>https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/</link><pubDate>Sat, 06 Mar 2021 00:00:00 +0000</pubDate><guid>https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/</guid><description>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2021-03-06-a-side-quest-in-api-dev-operations-cloud-and-database.png" alt="Featured image of post A side quest in API development, observability, Kubernetes and cloud with a hint of database" />&lt;p>Quite often people ask me what I actually do. I have a hard time giving a short answer. Even to colleagues and friends in the industry.&lt;/p>
&lt;p>Here I will try to show and tell how I spent an evening digging around in a system I helped build for a client.&lt;/p>
&lt;br>
&lt;hr>
&lt;br>
&lt;p>&lt;strong>Table of contents&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="#Background" >Background&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#TheProblem" >The (initial) problem&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#FurtherReading" >Fixing the (initial) problem&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Verifying the (initial) fix&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#FurtherReading" >Baseline simple request - HTTP1 1 connections, 20000 requests&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Baseline complex request - HTTP1 1 connections, 20000 requests&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Verifying the fix for assumed workload&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#FurtherReading" >Complex request - HTTP1 6 connections, 500 requests&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Complex request - HTTP2 500 &amp;ldquo;connections&amp;rdquo;, 500 requests&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Side quest: Database optimizations&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Determining the next bottleneck&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Side quest: Cluster resources and burstable VMs&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Conclusion" >Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="Background">&lt;/a>&lt;/p>
&lt;h1 id="background">Background
&lt;/h1>&lt;p>I&amp;rsquo;m a consultant doing development, DevOps and cloud infrastructure.&lt;/p>
&lt;p>For this specific client I mainly develop APIs using Golang to support new products and features as well as various exporting, importing and processing of data in the background.&lt;/p>
&lt;p>I&amp;rsquo;m also the &amp;ldquo;ops&amp;rdquo; guy handling everything in AWS, setting up and maintaing databases, making sure the &amp;ldquo;DevOps&amp;rdquo; works and the frontend and analytics people can do their work with little friction.
99% of the time things work just fine. No data is lost. The systems very rarely have unforeseen downtime and the users can access the data they want with acceptable latency rarely exceeding 500ms.&lt;/p>
&lt;p>A couple of times a year I assess the status of the architecture and set up new environments from scratch and update any documentation that has drifted. This is also a good time to do changes and add or remove constraints in anticipation of future business needs.&lt;/p>
&lt;p>In short, the current tech stack that has evolved over a couple of years is:&lt;/p>
&lt;ul>
&lt;li>Everything hosted on Amazon Web Services (AWS).&lt;/li>
&lt;li>AWS managed Elastic Kubernetes Service (EKS) currently on K8s 1.18.&lt;/li>
&lt;li>GitHub Actions for building Docker images for frontends, backends and other systems.&lt;/li>
&lt;li>AWS Elastic Container Registry for storing Docker images.&lt;/li>
&lt;li>Deployment of each system defined as a Helm chart alongside source code.&lt;/li>
&lt;li>Actual environment configuration (Helm values) stored in repo along source code. Updated by GitHub Actions.&lt;/li>
&lt;li>ArgoCD in cluster to manage status of all environments and deployments. Development environments usually automatically deployed on change. Push a button to deploy to Production.&lt;/li>
&lt;li>Prometheus for storing metrics from the cluster and nodes itself as well as custom metrics for our own systems.&lt;/li>
&lt;li>Loki for storing logs. Makes it easier to retrieve logs from past Pods and aggregate across multiple Pods.&lt;/li>
&lt;li>Elastic APM server for tracing.&lt;/li>
&lt;li>Pyroscope for live CPU profiling/tracing of Go applications.&lt;/li>
&lt;li>Betteruptime.com for tracking uptime and hosting status pages.&lt;/li>
&lt;/ul>
&lt;p>I might write up a longer post about the details if anyone is interested.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h1 id="the-initial-problem">The (initial) problem
&lt;/h1>&lt;p>A week ago I upgraded our API from version 1, that was deployed in January, to version 2 with new features and better architecture.&lt;/p>
&lt;p>One of the endpoints of the API returns an analysis of an object we track. I have previously reduced the amount of database queries by 90% but it still requires about 50 database calls from three different databases.
Getting and analyzing the data usually completes in about 3-400 milliseconds returning an 11.000 line JSON.&lt;/p>
&lt;p>It&amp;rsquo;s also possible to just call &lt;code>/objects/analysis&lt;/code> to get the analysis for all the 500 objects we are tracking. It takes 20 seconds but is meant for exports to other processes and not interactive use, so not a problem.&lt;/p>
&lt;p>Since the product is under very active development the frontend guys just download the whole analysis for an object to show certain relevant information to users. It&amp;rsquo;s too early to decide on which information is needed more often and how to optimize for that. Not a problem.&lt;/p>
&lt;p>So we need an overview of some fields from multiple objects in a dashboard / list. We can easily pull analysis from 20 objects without any noticable delay.&lt;/p>
&lt;p>But what if we just want to show more, 50? 200? 500? The frontend already have the IDs for all the objects and fetches them from &lt;code>/objects/id/analysis&lt;/code>. So they loop over the IDs and fire of requests simultaneously.&lt;/p>
&lt;p>Analyzing the network waterfall in Chrome DevTools indicated that the requests now took 20-30 seconds to complete! But looking closer most of the time they were actually queued up in the browser. This is because
Chrome only allows 6 concurrent TCP connection to the same origin when using HTTP1 (&lt;a class="link" href="https://developers.google.com/web/tools/chrome-devtools/network/understanding-resource-timing%29" target="_blank" rel="noopener"
>https://developers.google.com/web/tools/chrome-devtools/network/understanding-resource-timing)&lt;/a>.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h2 id="fixing-the-initial-problem">Fixing the (initial) problem
&lt;/h2>&lt;p>HTTP2 should fix this problem easily. By default HTTP2 is disabled in nginx-ingress. I add a couple of lines enabling it and update the Helm deployment of the ingress controller.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h2 id="verifying-the-initial-fix">Verifying the (initial) fix
&lt;/h2>&lt;p>Some common development tools doesn&amp;rsquo;t support HTTP2, such as Postman. So I found &lt;code>h2load&lt;/code> which can both help me verify HTTP2 is working and I also get to measure the improvement, nice!&lt;/p>
&lt;blockquote>
&lt;p>Note that I&amp;rsquo;m not using the analysis endpoint since I want to measure the change from HTTP1 to HTTP2 and it will become apparent later that there are other bottlenecks preventing us from a linear performance increase when just changing from HTTP1 to HTTP2.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Also note that this is somewhat naive since it requests the same URL over and over which can give false results due to any caching. But fortunately we don&amp;rsquo;t do any caching yet.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h3 id="baseline-simple-request---http1-1-connections-20000-requests">Baseline simple request - HTTP1 1 connections, 20000 requests
&lt;/h3>&lt;p>Using 1 concurrent streams, 1 client and HTTP1 I get an estimate of performance pre-http2:&lt;/p>
&lt;pre>&lt;code>h2load --h1 --requests=20000 --clients=1 --max-concurrent-streams=1 https://api.x.com/api/v1/objects/1
&lt;/code>&lt;/pre>
&lt;p>The results are as expected:&lt;/p>
&lt;pre>&lt;code>finished in 1138.99s, 17.56 req/s, 18.41KB/s
requests: 20000 total, 20000 started, 20000 done, 19995 succeeded, 5 failed, 0 errored, 0 timeout
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-apm.png"
width="1816"
height="705"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-apm_hu17076544040877865054.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-apm_hu9690961613720477067.png 1024w"
loading="lazy"
alt="Overview from Elastic APM. Duration is very acceptable at around 20ms. No errors. And about 25% of the time spent doing database queries."
class="gallery-image"
data-flex-grow="257"
data-flex-basis="618px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-cpu.png"
width="1034"
height="314"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-cpu_hu2013984859425220242.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-cpu_hu13236457479005124721.png 1024w"
loading="lazy"
alt="Container CPU usage. Nothing special."
class="gallery-image"
data-flex-grow="329"
data-flex-basis="790px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-latency.png"
width="861"
height="356"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-latency_hu1867474644236441644.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-latency_hu15523458481306920119.png 1024w"
loading="lazy"
alt="Database query latency. The vast majority under 5ms. Acceptable."
class="gallery-image"
data-flex-grow="241"
data-flex-basis="580px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-queries.png"
width="780"
height="348"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-queries_hu17361066750422420350.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-queries_hu10420784785634132053.png 1024w"
loading="lazy"
alt="Number of DB queries per second."
class="gallery-image"
data-flex-grow="224"
data-flex-basis="537px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-latency.png"
width="863"
height="316"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-latency_hu6805455395055610702.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-latency_hu3298924550317512719.png 1024w"
loading="lazy"
alt="HTTP response latency."
class="gallery-image"
data-flex-grow="273"
data-flex-basis="655px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-requests.png"
width="778"
height="217"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-requests_hu5945712598158928612.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-requests_hu419352892190289981.png 1024w"
loading="lazy"
alt="Number of HTTP requests per second. Unsurprisingly the number of database queries are identical to the number of HTTP requests. Latency of HTTP requests also tracks the latency of the (single) database query."
class="gallery-image"
data-flex-grow="358"
data-flex-basis="860px"
>&lt;/p>
&lt;p>For http2 we set max concurrent streams to the same as number of requests:&lt;/p>
&lt;pre>&lt;code>h2load --requests=200 --clients=1 --max-concurrent-streams=200 https://api.x.com/api/v1/objects/1
&lt;/code>&lt;/pre>
&lt;p>Which results in almost half the latency:&lt;/p>
&lt;pre>&lt;code>finished in 1.23s, 162.65 req/s, 158.06KB/s
requests: 200 total, 200 started, 200 done, 200 succeeded, 0 failed, 0 errored, 0 timeout
&lt;/code>&lt;/pre>
&lt;p>So HTTP2 is working and providing significant latency improvements. Success!&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h3 id="baseline-complex-request---http1-1-connections-20000-requests">Baseline complex request - HTTP1 1 connections, 20000 requests
&lt;/h3>&lt;p>We start by establishing a baseline with 1 connection querying over and over.&lt;/p>
&lt;pre>&lt;code>h2load --h1 --requests=20000 --clients=1 --max-concurrent-streams=1
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-apm.png"
width="2087"
height="707"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-apm_hu13565166093859492250.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-apm_hu14225806095910349646.png 1024w"
loading="lazy"
alt="Latency increases as much more computation is done and data is returned. But the latency is consistent which is good. We also see that the database is becomming the bottleneck for where most time is spent."
class="gallery-image"
data-flex-grow="295"
data-flex-basis="708px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-cpu.png"
width="1200"
height="308"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-cpu_hu4067009256511003722.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-cpu_hu17950687200232907900.png 1024w"
loading="lazy"
alt="CPU usage increased to 15%. Lower increase than expected considering the complexity involved in serving the requests."
class="gallery-image"
data-flex-grow="389"
data-flex-basis="935px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-latency.png"
width="985"
height="344"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-latency_hu7089278039337102831.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-latency_hu17575998733902921949.png 1024w"
loading="lazy"
alt="Database query latency still mostly under 5ms."
class="gallery-image"
data-flex-grow="286"
data-flex-basis="687px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-queries.png"
width="894"
height="351"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-queries_hu6509951597429203579.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-queries_hu14549008744426915283.png 1024w"
loading="lazy"
alt="Number of database queries increases by a factor of 10 compared to HTTP requests."
class="gallery-image"
data-flex-grow="254"
data-flex-basis="611px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-latency.png"
width="987"
height="313"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-latency_hu565398038381364000.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-latency_hu7243746795286387547.png 1024w"
loading="lazy"
alt="HTTP latency."
class="gallery-image"
data-flex-grow="315"
data-flex-basis="756px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-requests.png"
width="895"
height="219"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-requests_hu9040739607883373017.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-requests_hu3790465645525383070.png 1024w"
loading="lazy"
alt="HTTP requests per second."
class="gallery-image"
data-flex-grow="408"
data-flex-basis="980px"
>&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h2 id="verifying-the-fix-for-assumed-workload">Verifying the fix for assumed workload
&lt;/h2>&lt;p>So we verified that HTTP2 gives us a performance boost. But what happens when we fire away 500 requests to the much heavier &lt;code>/analysis&lt;/code> endpoint?&lt;/p>
&lt;blockquote>
&lt;p>These graphs are not as pretty since the ones above. This is mainly due to the sampling interval of the metrics and that we need several datapoints to accurately determine the rate() of a counter.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h3 id="complex-request---http1-6-connections-500-requests">Complex request - HTTP1 6 connections, 500 requests
&lt;/h3>&lt;pre>&lt;code>finished in 32.25s, 14.88 req/s, 2.29MB/s
requests: 500 total, 500 started, 500 done, 500 succeeded, 0 failed, 0 errored, 0 timeout
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-apm.png"
width="1484"
height="705"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-apm_hu2573287072721533229.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-apm_hu10620414983979950514.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-apm"
class="gallery-image"
data-flex-grow="210"
data-flex-basis="505px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-cpu.png"
width="847"
height="299"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-cpu_hu1135877464954207689.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-cpu_hu17008451797633252465.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-cpu"
class="gallery-image"
data-flex-grow="283"
data-flex-basis="679px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-latency.png"
width="706"
height="359"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-latency_hu9358974391483687749.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-latency_hu16352819740947538611.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-db-latency"
class="gallery-image"
data-flex-grow="196"
data-flex-basis="471px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-queries.png"
width="637"
height="352"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-queries_hu17797633323803646239.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-queries_hu2207066698060964218.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-db-queries"
class="gallery-image"
data-flex-grow="180"
data-flex-basis="434px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-latency.png"
width="700"
height="321"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-latency_hu13253021454442065078.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-latency_hu13649080090034874975.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-http-latency"
class="gallery-image"
data-flex-grow="218"
data-flex-basis="523px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-requests.png"
width="638"
height="213"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-requests_hu12544065771888019895.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-requests_hu1798439057131787658.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-http-requests"
class="gallery-image"
data-flex-grow="299"
data-flex-basis="718px"
>&lt;/p>
&lt;p>In summary it so far seems to scale linearly with load. Most of the time is spent fetching data from the database. Still very predictable low latency on database queries and the resulting HTTP response.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h3 id="complex-request---http2-500-connections-500-requests">Complex request - HTTP2 500 &amp;ldquo;connections&amp;rdquo;, 500 requests
&lt;/h3>&lt;p>&lt;em>So now we unleash the beast. Firing all 500 requests at the same time.&lt;/em>&lt;/p>
&lt;pre>&lt;code>finished in 16.66s, 30.02 req/s, 3.55MB/s
requests: 500 total, 500 started, 500 done, 500 succeeded, 0 failed, 0 errored, 0 timeout
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-cpu.png"
width="939"
height="307"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-cpu_hu5828488102210360607.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-cpu_hu3343570902582545481.png 1024w"
loading="lazy"
alt="CPU on API still doing good. A slight hint of CPU throttling due to CFS, which is used when you set CPU limits in Kubernetes."
class="gallery-image"
data-flex-grow="305"
data-flex-basis="734px"
>&lt;/p>
&lt;blockquote>
&lt;p>Important about Kubernetes and CPU limits&lt;br />
Even with CPU limits set to 1 (100% of one CPU), your container can still be throttled at much lower CPU usage. Check out &lt;a class="link" href="https://medium.com/omio-engineering/cpu-limits-and-aggressive-throttling-in-kubernetes-c5b20bd8a718" target="_blank" rel="noopener"
>this article&lt;/a> for more information.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-latency.png"
width="782"
height="346"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-latency_hu16596262103242219393.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-latency_hu6743393326456624241.png 1024w"
loading="lazy"
alt="Whopsie. The average database query latency has increased drastically, and we have a long tail of very slow queries. Looks like we are starting to see signs of bottlenecks on the database. This might also be affected by our maximum of 60 concurrent connections to the database, resulting in queries having to wait their turn before executing."
class="gallery-image"
data-flex-grow="226"
data-flex-basis="542px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-queries.png"
width="705"
height="346"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-queries_hu17270843372303526721.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-queries_hu14745640312772792195.png 1024w"
loading="lazy"
alt="Its hard to judge the peak rate of database queries due to limited sampling of the metrics."
class="gallery-image"
data-flex-grow="203"
data-flex-basis="489px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-http-latency.png"
width="783"
height="309"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-http-latency_hu9429010041105922198.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-http-latency_hu14677623071583589615.png 1024w"
loading="lazy"
alt="Now individual HTTP requests are much slower due to waiting for the database."
class="gallery-image"
data-flex-grow="253"
data-flex-basis="608px"
>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-apm-trace.png"
width="1150"
height="1192"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-apm-trace_hu12116834356231894546.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-apm-trace_hu17401424198592652531.png 1024w"
loading="lazy"
alt="Here is just a random trace from Elastic APM to see if the increased database latency is concentrated to specific queries or tables or just general saturation. Indeed there is a single query responsible for half the time taken for the entire query! We better get back to that in a bit and dig further."
class="gallery-image"
data-flex-grow="96"
data-flex-basis="231px"
>&lt;/p>
&lt;p>In an ideal world all 500 requests should start and complete in 2-300ms regardless. Since that is not happening it&amp;rsquo;s an indication that we are now hitting some other bottleneck.&lt;/p>
&lt;p>Looking at the graphs it seems we are starting to saturate the database. The latency for every request is now largely dependent on the slowest of the 10-12 database queries it depends on. And as we are stressing the database the probability of slow queries increase. The latency for the whole process of fetching 500 requests are again largely dependent on the slowest requests.&lt;/p>
&lt;p>So this optimization gives on average better performance, but more variability of the individual requests, when the system is under heavy load.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h1 id="side-quest-database-optimizations">Side quest: Database optimizations
&lt;/h1>&lt;p>It seems we are saturating the database. Before throwing more money at the problem (by increasing database size) I like to know what the bottlenecks are. Looking at the traces from APM
I see one query that is consistently taking 10x longer than the rest. I also confirm this in the AWS RDS Performance Insights that show the top SQL queries by load.&lt;/p>
&lt;p>When designing the database schema I came up with the idea of having immutability for certain data types. So instead of overwriting row with ID 1, we add a row with ID 1 Revision 2. Now we have the history of who did what to the data and can easily track changes and roll back if needed. The most common use case is just fetching the last revision. So for simplicity I created a PostgreSQL view that only shows the last revision. That way clients don&amp;rsquo;t have to worry about the existense of revisions at all. That is now just an implementation detail.&lt;/p>
&lt;p>When it comes to performance that turns out to be an important implementation detail. The view is using &lt;code>SELECT DISTINCT ON (id) ... ORDER BY id, revision DESC&lt;/code>. However many of the queries to the view is ordering the returned data by time, and expect the data returned from database to already be ordered chronologically. Using &lt;code>EXPLAIN ANALYZE&lt;/code> on the queries this always results in a full table scan instead of using indexes, and is what&amp;rsquo;s causing this specific query to be slow. Without going into details it seems there is no simple and efficient way of having a view with the last revision and query that for a subset of rows ordered again by time.&lt;/p>
&lt;p>For the forseable future this does not actually impact real world usage. It&amp;rsquo;s only apparent under artificially large loads under the worst conditions. But now we know where we need to refactor things if performance actually becomes a problem.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h1 id="determining-the-next-bottleneck">Determining the next bottleneck
&lt;/h1>&lt;p>Whenever I fix one problem I like to know where, how and when the next problem or limit is likely to appear. When increasing the number of requests and streams I expected to see increasing latency. But instead I see errors appear like a cliff:&lt;/p>
&lt;pre>&lt;code>finished in 27.33s, 36.59 req/s, 5.64MB/s
requests: 5000 total, 1002 started, 1002 done, 998 succeeded, 4002 failed, 4000 errored, 0 timeout
&lt;/code>&lt;/pre>
&lt;p>Consulting the logs for both the nginx load balancer and the API there are no records of failing requests. Since nginx does not pass the HTTP2 connection directly to the API, but instead &amp;ldquo;unbundles&amp;rdquo; them into HTTP1 requests I suspect there might be issues with connection limits or even available ports from nginx to the API. But maybe it&amp;rsquo;s a configuration issue. By default nginx does &lt;a class="link" href="http://nginx.org/en/docs/http/ngx_http_upstream_module.html#server" target="_blank" rel="noopener"
>not limit the number of connections to a backend&lt;/a> (our API). . But, there is actually a &lt;a class="link" href="https://nginx.org/en/docs/http/ngx_http_v2_module.html#http2_max_requests" target="_blank" rel="noopener"
>default limit to the number of HTTP2 requests that can be served over a single connection&lt;/a> - And it happens to be 1000.&lt;/p>
&lt;p>I leave it at that. It&amp;rsquo;s very unlikely we&amp;rsquo;ll be hitting these limits any time soon.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h1 id="side-quest-cluster-resources-and-burstable-vms">Side quest: Cluster resources and burstable VMs
&lt;/h1>&lt;p>When load testing the first time around sometimes Grafana would also become unresponsive. That&amp;rsquo;s usually a bad sign. It might indicate that the underlying infrastructure is also reaching saturation. That is not good since it can impact what should be independent services.&lt;/p>
&lt;p>Our Kubernetes cluster is composed of 2x t3a.medium on demand nodes and 2x t3a.medium spot nodes. These VM types are burstable. You can use 20% per vCPU sustained over time without problems. If you exceed those 20% you start consuming CPU credits faster than they are granted and once you run out of CPU credits processes will be forcibly throttled.&lt;/p>
&lt;p>Of course Kubernetes does not know about this and expects 1 CPU to actually be 1 CPU. In addition Kubernetes will decide where to place workloads based on their stated resource requirements and limits, and not their actual resource usage.&lt;/p>
&lt;p>When looking at the actual metrics two of our nodes are indeed out of CPU credits and being throttled. The sum of factors leading to this is:&lt;/p>
&lt;ul>
&lt;li>We have not yet set resource requests and limits making it harder for Kubernetes to intelligently place workloads&lt;/li>
&lt;li>Using burstable nodes having some additional constraints not visible to Kubernetes&lt;/li>
&lt;li>Old deployments laying around consuming unnecessary resources&lt;/li>
&lt;li>Adding costly features without assessing the overall impact&lt;/li>
&lt;/ul>
&lt;p>I have not touched on the last point yet. I started adding &lt;a class="link" href="https://pyroscope.io/" target="_blank" rel="noopener"
>Pyroscope&lt;/a> to our systems since I simply love monitoring All The Things. The documentation does not go into specifics but emphasizes that it&amp;rsquo;s &amp;ldquo;low overhead&amp;rdquo;. Remember that our budget for CPU usage is actually 40% per node, not 200%. The Pyroscope server itself consumes 10-15% CPU which seems fair. But investigating further the Pyroscope agent also consumes 5-6% CPU per instance. This graph shows the CPU usage of a single Pod before and after turning off Pyroscope profiling.&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/pyroscope-agent-cpu.png"
width="1029"
height="271"
srcset="https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/pyroscope-agent-cpu_hu15074112692027608241.png 480w, https://blog.stian.omg.lol/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/pyroscope-agent-cpu_hu2516833612973000297.png 1024w"
loading="lazy"
alt="pyroscope-agent-cpu"
class="gallery-image"
data-flex-grow="379"
data-flex-basis="911px"
>&lt;/p>
&lt;p>5-6% CPU overhead on a highly utilized service is probably worth it. But when the baseline CPU usage is 0% CPU and we have multiple services and deployments in different environments we are suddenly using 40-60% CPU on profiling and less than 1% on actual work!&lt;/p>
&lt;p>The outcome of this is that we need to separate burstable and stable load deployments. Monitoring and supporting systems are usually more stable resource wise while the actual business systems much more variable, and suitable for burst nodes. In practice we add a node pool of non-burst VMs and use NodeAffinity to stick Prometheus, Pyroscope etc to those nodes. Another benefit of this is that the supporting systems needed to troubleshoot problems are now less likely to be impacted by the problem itself, making troubleshooting much easier.&lt;/p>
&lt;p>&lt;a id="Conclusion">&lt;/a>&lt;/p>
&lt;h1 id="conclusion">Conclusion
&lt;/h1>&lt;p>This whole adventure only took a few hours but resulted in some specific and immediate performance gains. It also highlighted the weakest links in our application, database and infrastructure architecture.&lt;/p></description></item><item><title>Mini-post: Down-scaling Azure Kubernetes Service (AKS)</title><link>https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/</link><pubDate>Tue, 04 Jun 2019 00:00:00 +0000</pubDate><guid>https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/</guid><description>&lt;img src="https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/2019-06-04-downscaling-aks.png" alt="Featured image of post Mini-post: Down-scaling Azure Kubernetes Service (AKS)" />&lt;p>We discovered today that some implicit assumptions we had about AKS at smaller scales were incorrect.&lt;/p>
&lt;p>Suddenly new workloads and jobs in our Radix CI/CD could not start due to insufficient resources (CPU &amp;amp; memory).&lt;/p>
&lt;p>Even though it only caused problems in development environments with smaller node sizes it still surprised some of our developers, since we expected the size of development clusters to have enough resources.&lt;/p>
&lt;p>I thought it would be a good chance to go a bit deeper and verify some of our assumptions and also learn more about various components that usually &amp;ldquo;just works&amp;rdquo; and isn&amp;rsquo;t really given much thought.&lt;/p>
&lt;p>First I do a &lt;code>kubectl describe node &amp;lt;node&amp;gt;&lt;/code> on 2-3 of the nodes to get an idea of how things are looking:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">Resource Requests Limits
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-------- -------- ------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cpu 930m &lt;span class="o">(&lt;/span>98%&lt;span class="o">)&lt;/span> 5500m &lt;span class="o">(&lt;/span>585%&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">memory &lt;span class="m">1659939584&lt;/span> &lt;span class="o">(&lt;/span>89%&lt;span class="o">)&lt;/span> 4250M &lt;span class="o">(&lt;/span>228%&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>So we are obviously hitting the roof when it comes to resources. But why?&lt;/p>
&lt;h2 id="node-overhead">Node overhead
&lt;/h2>&lt;p>We use &lt;code>Standard DS1 v2&lt;/code> instances as AKS nodes and they have 1 CPU core and 3.5 GiB memory.&lt;/p>
&lt;p>The output of &lt;code>kubectl describe node&lt;/code> also gives us info on the Capacity (total node size) and Allocatable (resources available to run Pods).&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Capacity:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cpu: 1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> memory: 3500452Ki
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Allocatable:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cpu: 940m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> memory: 1814948Ki
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>So we have lost &lt;strong>60 millicores / 6%&lt;/strong> of CPU and &lt;strong>1685MiB / 48%&lt;/strong> of memory. The next question is if this increases linearly with node size (the percentage of resources lost is the same regardless of node size) or is fixed (always reserves 60 millicores and 1685Mi of memory), or a combination.&lt;/p>
&lt;p>I connect to another cluster that has double the node size (&lt;code>Standard DS2 v2&lt;/code>) and compare:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Capacity:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cpu: 2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> memory: 7113160Ki
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Allocatable:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cpu: 1931m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> memory: 4667848Ki
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>So for this the loss is &lt;strong>69 millicores / 3.5%&lt;/strong> of CPU and &lt;strong>2445MiB / 35%&lt;/strong> of memory.&lt;/p>
&lt;p>So CPU reservations are close to fixed regardless of node size while memory reservations are influenced by node size but luckily not linearly.&lt;/p>
&lt;p>What causes this &amp;ldquo;waste&amp;rdquo;? Reading up on &lt;a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/" target="_blank" rel="noopener"
>kubernetes.io&lt;/a> gives a few clues. Kubelet will reserve CPU and memory resources for itself and other Kubernetes processes. It will also reserve a portion of memory to act as a buffer whenever a Pod is going beyond it&amp;rsquo;s memory limits to avoid risking System OOM, potentially making the whole node unstable.&lt;/p>
&lt;p>To figure out what these are configured to we log in to an actual AKS node&amp;rsquo;s console and run &lt;code>ps ax|grep kube&lt;/code> and the output looks like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="o">/&lt;/span>&lt;span class="n">usr&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">local&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">bin&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubelet&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">enable&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">server&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">labels&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">role&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">agent&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">role&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">agent&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">agentpool&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">nodepool1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">storageprofile&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">managed&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">storagetier&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">Premium_LRS&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">azure&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">com&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">cluster&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">MC_clusters_weekly&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">22&lt;/span>&lt;span class="n">_northeurope&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">volume&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">plugin&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">dir&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">volumeplugins&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">address&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="mf">0.0&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">allow&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">privileged&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">true&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">anonymous&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">auth&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">false&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">authorization&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">mode&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">Webhook&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">azure&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">container&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">azure&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">json&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">cgroups&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">per&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">qos&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">true&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">client&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">ca&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">file&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">certs&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">ca&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">crt&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">cloud&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">azure&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">json&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">cloud&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">provider&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">azure&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">cluster&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">dns&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">10.2&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="mf">0.10&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">cluster&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">domain&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">cluster&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">enforce&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">allocatable&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">pods&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">event&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">qps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">eviction&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">hard&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">available&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="mi">750&lt;/span>&lt;span class="n">Mi&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">nodefs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">available&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="o">%&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">nodefs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inodesFree&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="o">%&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">feature&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">gates&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">PodPriority&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">true&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">RotateKubeletServerCertificate&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">true&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">gc&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">high&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">threshold&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">85&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">gc&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">low&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">threshold&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">80&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">pull&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">progress&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">deadline&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">30&lt;/span>&lt;span class="n">m&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">keep&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">terminated&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">pod&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">volumes&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">false&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">kube&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">reserved&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">cpu&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">60&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">memory&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">896&lt;/span>&lt;span class="n">Mi&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">kubeconfig&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="k">var&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">lib&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubelet&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubeconfig&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="nb">max&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">pods&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">110&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">network&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">plugin&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">cni&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">status&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">update&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">frequency&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="n">s&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">non&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">masquerade&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">cidr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">pod&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">infra&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">container&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">k8s&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gcr&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">pause&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">amd64&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mf">3.1&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">pod&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">manifest&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">path&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">manifests&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">pod&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="nb">max&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">pids&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">rotate&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">certificates&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">false&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">streaming&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">connection&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">idle&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">timeout&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="n">m&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>To log in to the console of a node, go to the MC_resourcegroup_clustername_region resource-group and select the VM. Then go to &lt;code>Boot diagnostics&lt;/code> and enable it. Go to &lt;code>Reset password&lt;/code> to create yourself a user and then &lt;code>Serial console&lt;/code> to log in and execute commands.&lt;/p>
&lt;/blockquote>
&lt;p>We can see &lt;code>--kube-reserved=cpu=60m,memory=896Mi&lt;/code> and &lt;code>--eviction-hard=memory.available&amp;lt;750Mi&lt;/code> which adds up to &lt;code>1646Mi&lt;/code> which is pretty close to the &lt;code>1685Mi&lt;/code> that was the gap between Capacity and Allocatable.&lt;/p>
&lt;p>We also do this on a &lt;code>Standard DS2 v2&lt;/code> node and get &lt;code>--kube-reserved=cpu=69m,memory=1638Mi&lt;/code> and &lt;code>--eviction-hard=memory.available&amp;lt;750Mi&lt;/code>.&lt;/p>
&lt;p>So we can see that the memory of &lt;code>kube-reserved&lt;/code> grows almost linearly and seems to always be about 20-25% while CPU reservations are almost the same. The memory eviction buffer is always fixed at &lt;code>750Mi&lt;/code> which would mean bigger resource waste as nodes decrease in size.&lt;/p>
&lt;h4 id="cpu">CPU
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: right">Standard DS1 v2&lt;/th>
&lt;th style="text-align: right">Standard DS2 v2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>VM capacity&lt;/td>
&lt;td style="text-align: right">1.000m&lt;/td>
&lt;td style="text-align: right">2.000m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-reserved&lt;/td>
&lt;td style="text-align: right">-60m&lt;/td>
&lt;td style="text-align: right">-69m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Allocatable&lt;/td>
&lt;td style="text-align: right">940m&lt;/td>
&lt;td style="text-align: right">1.931m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Allocatable %&lt;/td>
&lt;td style="text-align: right">94%&lt;/td>
&lt;td style="text-align: right">96.5%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="memory">Memory
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: right">Standard DS1 v2&lt;/th>
&lt;th style="text-align: right">Standard DS2 v2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>VM capacity&lt;/td>
&lt;td style="text-align: right">3.500Mi&lt;/td>
&lt;td style="text-align: right">7.113Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-reserved&lt;/td>
&lt;td style="text-align: right">-896Mi&lt;/td>
&lt;td style="text-align: right">-1.638Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Eviction buf&lt;/td>
&lt;td style="text-align: right">-750Mi&lt;/td>
&lt;td style="text-align: right">-750Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Allocatable&lt;/td>
&lt;td style="text-align: right">1.814Mi&lt;/td>
&lt;td style="text-align: right">4.667Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Allocatable %&lt;/td>
&lt;td style="text-align: right">52%&lt;/td>
&lt;td style="text-align: right">65%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="node-pods-daemonsets">Node pods (DaemonSets)
&lt;/h2>&lt;p>We have some Pods that run on every node, and they are installed by default by AKS. We get the resource limits of these by describing either the pods or the daemonsets.&lt;/p>
&lt;h4 id="cpu-1">CPU
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: right">Standard DS1 v2&lt;/th>
&lt;th style="text-align: right">Standard DS2 v2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Allocatable&lt;/td>
&lt;td style="text-align: right">940m&lt;/td>
&lt;td style="text-align: right">1.931m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/calico-node&lt;/td>
&lt;td style="text-align: right">-250m&lt;/td>
&lt;td style="text-align: right">-250m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/kube-proxy&lt;/td>
&lt;td style="text-align: right">-100m&lt;/td>
&lt;td style="text-align: right">-100m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/kube-svc-redirect&lt;/td>
&lt;td style="text-align: right">-5m&lt;/td>
&lt;td style="text-align: right">-5m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available&lt;/td>
&lt;td style="text-align: right">585m&lt;/td>
&lt;td style="text-align: right">1.576m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available %&lt;/td>
&lt;td style="text-align: right">58%&lt;/td>
&lt;td style="text-align: right">81%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="memory-1">Memory
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: right">Standard DS1 v2&lt;/th>
&lt;th style="text-align: right">Standard DS2 v2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Allocatable&lt;/td>
&lt;td style="text-align: right">1.814Mi&lt;/td>
&lt;td style="text-align: right">4.667Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/kube-svc-redirect&lt;/td>
&lt;td style="text-align: right">-32Mi&lt;/td>
&lt;td style="text-align: right">-32Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available&lt;/td>
&lt;td style="text-align: right">1.782Mi&lt;/td>
&lt;td style="text-align: right">4.635Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available %&lt;/td>
&lt;td style="text-align: right">50%&lt;/td>
&lt;td style="text-align: right">61%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>So for &lt;code>Standard DS1 v2&lt;/code> nodes we have about 0.5 CPU and 1.7GiB memory per node for pods. And for &lt;code>Standard DS2 v2&lt;/code> nodes it&amp;rsquo;s about 1.5 CPU and 4.6GiB memory.&lt;/p>
&lt;h2 id="kube-system-pods">kube-system pods
&lt;/h2>&lt;p>Now lets add some standard Kubernetes pods we need to run. As far as I know these are pretty much fixed for a cluster and not related to node size or count.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Deployment&lt;/th>
&lt;th style="text-align: right">CPU&lt;/th>
&lt;th style="text-align: right">Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>kube-system/kubernetes-dashboard&lt;/td>
&lt;td style="text-align: right">100m&lt;/td>
&lt;td style="text-align: right">50Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/tunnelfront&lt;/td>
&lt;td style="text-align: right">10m&lt;/td>
&lt;td style="text-align: right">64Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/coredns (x2)&lt;/td>
&lt;td style="text-align: right">200m&lt;/td>
&lt;td style="text-align: right">140Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/coredns-autoscaler&lt;/td>
&lt;td style="text-align: right">20m&lt;/td>
&lt;td style="text-align: right">10Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/heapster&lt;/td>
&lt;td style="text-align: right">130m&lt;/td>
&lt;td style="text-align: right">230Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sum&lt;/td>
&lt;td style="text-align: right">460m&lt;/td>
&lt;td style="text-align: right">494Mi&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="third-party-pods">Third party pods
&lt;/h2>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Deployment&lt;/th>
&lt;th style="text-align: right">CPU&lt;/th>
&lt;th style="text-align: right">Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>grafana&lt;/td>
&lt;td style="text-align: right">200m&lt;/td>
&lt;td style="text-align: right">500Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>prometheus-operator&lt;/td>
&lt;td style="text-align: right">500m&lt;/td>
&lt;td style="text-align: right">1.000Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>prometheus-alertmanager&lt;/td>
&lt;td style="text-align: right">100m&lt;/td>
&lt;td style="text-align: right">225Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>flux&lt;/td>
&lt;td style="text-align: right">50m&lt;/td>
&lt;td style="text-align: right">64Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>flux-helm-operator&lt;/td>
&lt;td style="text-align: right">50m&lt;/td>
&lt;td style="text-align: right">64Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sum&lt;/td>
&lt;td style="text-align: right">900m&lt;/td>
&lt;td style="text-align: right">1.853Mi&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="radix-platform-pods">Radix platform pods
&lt;/h2>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Deployment&lt;/th>
&lt;th style="text-align: right">CPU&lt;/th>
&lt;th style="text-align: right">Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>radix-api-prod/server (x2)&lt;/td>
&lt;td style="text-align: right">200m&lt;/td>
&lt;td style="text-align: right">400Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-api-qa/server (x2)&lt;/td>
&lt;td style="text-align: right">100m&lt;/td>
&lt;td style="text-align: right">200Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-canary-golang-dev/www&lt;/td>
&lt;td style="text-align: right">40m&lt;/td>
&lt;td style="text-align: right">500Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-canary-golang-prod/www&lt;/td>
&lt;td style="text-align: right">40m&lt;/td>
&lt;td style="text-align: right">500Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-platform-prod/public-site&lt;/td>
&lt;td style="text-align: right">5m&lt;/td>
&lt;td style="text-align: right">10Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-web-console-prod/web&lt;/td>
&lt;td style="text-align: right">10m&lt;/td>
&lt;td style="text-align: right">42Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-web-console-qa/web&lt;/td>
&lt;td style="text-align: right">5m&lt;/td>
&lt;td style="text-align: right">21Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-github-webhook-prod/webhook&lt;/td>
&lt;td style="text-align: right">10m&lt;/td>
&lt;td style="text-align: right">30Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-github-webhook-prod/webhook&lt;/td>
&lt;td style="text-align: right">5m&lt;/td>
&lt;td style="text-align: right">15Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sum&lt;/td>
&lt;td style="text-align: right">415m&lt;/td>
&lt;td style="text-align: right">1.718Mi&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>If we add up the resource usage of these groups of workloads and see the total available resources on our 4 node Standard DS1 v2 clusters we are left with 0.56 CPU cores (14%) and 3GB of memory (22%):&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Workload&lt;/th>
&lt;th style="text-align: right">CPU&lt;/th>
&lt;th style="text-align: right">Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>kube-system&lt;/td>
&lt;td style="text-align: right">460m&lt;/td>
&lt;td style="text-align: right">494Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>third-party&lt;/td>
&lt;td style="text-align: right">900m&lt;/td>
&lt;td style="text-align: right">1.853Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-platform&lt;/td>
&lt;td style="text-align: right">415m&lt;/td>
&lt;td style="text-align: right">1.718Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sum&lt;/td>
&lt;td style="text-align: right">1.760m&lt;/td>
&lt;td style="text-align: right">4.020Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available on 4x DS1&lt;/td>
&lt;td style="text-align: right">2.340m&lt;/td>
&lt;td style="text-align: right">7.128Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available for workloads&lt;/td>
&lt;td style="text-align: right">565m&lt;/td>
&lt;td style="text-align: right">3.063Mi&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Though surprising that we lost this much resources before being able to deploy our actual customer applications, it should still be a bit of headroom.&lt;/p>
&lt;p>Going further I checked the resource requests on 8 customer pods deployed in 4 environments (namespaces). Even though none of them had a resource configuration in their &lt;code>radixconfig.yaml&lt;/code> files they still had resource requests and limits. Not surprising since we use LimitRange to set default resource requests and limits. The surprise was that half of them had 50Mi of memory and the other half 500Mi, seemingly at random.&lt;/p>
&lt;p>It turns out that we did an update to the LimitRange values a few days ago but that only applies to new Pods, so depending on if the Pods got re-created for any reason they may or may not have the old request of 500Mi, which in our case of small clusters will quickly drain the available resources.&lt;/p>
&lt;blockquote>
&lt;p>Read more about LimitRange here: &lt;a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/" target="_blank" rel="noopener"
>kubernetes.io&lt;/a> , and here is the commit that eventually trickled down to reduce memory usage: &lt;a class="link" href="https://github.com/equinor/radix-operator/commit/f022fcde993efdf6cbcafb2c6632707a823a2a27" target="_blank" rel="noopener"
>github.com&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h2 id="pod-scheduling">Pod scheduling
&lt;/h2>&lt;p>Depending on the weight between CPU and memory requests and how often things get destroyed and re-created you may find yourself in a situation where you have enough resources in your cluster but new workloads are still Pending. This can happen when one resource type (e.g. CPU) is filled before another (e.g. memory), leading one or more resources to be stranded and unlikely to be utilized.&lt;/p>
&lt;p>Imagine for example a cluster that is already utilized like this:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>CPU&lt;/th>
&lt;th>Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>node0&lt;/td>
&lt;td>94%&lt;/td>
&lt;td>86%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>node1&lt;/td>
&lt;td>80%&lt;/td>
&lt;td>89%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>node2&lt;/td>
&lt;td>98%&lt;/td>
&lt;td>60%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Scheduling a workload that requests 15% CPU and 20% memory cannot be scheduled since there are no nodes fulfilling both requirements. In theory there is probably a CPU intensive Pod on node2 that could be moved to node1 but Kubernetes does not do re-scheduling to optimize utilization. It can do re-scheduling based on Pod priority (&lt;a class="link" href="https://medium.com/@dominik.tornow/the-kubernetes-scheduler-cd429abac02f" target="_blank" rel="noopener"
>medium.com&lt;/a>) and there is an incubator project (&lt;a class="link" href="https://akomljen.com/meet-a-kubernetes-descheduler/" target="_blank" rel="noopener"
>akomljen.com&lt;/a>) that can try to drain nodes with low utilization.&lt;/p>
&lt;p>So for the foreseable future keeping in mind that resources can get stranded and that looking at the sum of cluster resources and sum of cluster resource demand might be misleading.&lt;/p>
&lt;h2 id="calico-node">calico-node
&lt;/h2>&lt;p>The biggest source of waste on our small clusters is &lt;code>calico-node&lt;/code> which is installed on every node and requests 25% of a CPU core while only using 2.5-3% CPU:&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu.png"
width="1291"
height="392"
srcset="https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu_hu1477443119961749415.png 480w, https://blog.stian.omg.lol/p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu_hu8581044549333248792.png 1024w"
loading="lazy"
alt="calico-node cpu usage"
class="gallery-image"
data-flex-grow="329"
data-flex-basis="790px"
>&lt;/p>
&lt;p>The request is originally set here &lt;a class="link" href="https://github.com/Azure/aks-engine/blob/master/parts/k8s/containeraddons/kubernetesmasteraddons-calico-daemonset.yaml" target="_blank" rel="noopener"
>github.com&lt;/a> but I have not got into why that number was choosen. Next steps would be to do some benchmarking of &lt;code>calico-node&lt;/code> to smoke out it&amp;rsquo;s performance characteristics to see if it would be safe to lower the resource requests, but that is out of scope for now.&lt;/p>
&lt;h1 id="conclusion">Conclusion
&lt;/h1>&lt;ul>
&lt;li>By increasing node size from &lt;code>Standard DS1 v2&lt;/code> to &lt;code>Standard DS2 v2&lt;/code> we also increase the available CPU from 58% per node to 81% per node. Available memory increases from 50% to 61% per node.&lt;/li>
&lt;li>With a total platform requirement of 3-4GB of memory and 4.6GB available on &lt;code>Standard DS2 v2&lt;/code> we might have more resources for actual workloads on a 1-node &lt;code>Standard DS2 v2&lt;/code> cluster than a 3-node &lt;code>Standard DS1 v2&lt;/code> cluster!&lt;/li>
&lt;li>Beware of stranded resources limiting the utilization you can achieve across a cluster.&lt;/li>
&lt;/ul></description></item><item><title>Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking</title><link>https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/</link><pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate><guid>https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/</guid><description>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/2019-02-23-disk-performance-on-aks-part-1.png" alt="Featured image of post Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking" />&lt;p>Understanding the characteristics of disk performance of a platform might be more important than you think. If disk resources are not correctly matched to your workload, your performance will suffer and might lead you to incorrectly diagnose a problem as being related to CPU or memory.&lt;/p>
&lt;p>The defaults might also not give you the performance you expect.&lt;/p>
&lt;p>In this first post on troubleshooting some disk performance issues on Azure Kubernetes Service (AKS) we will benchmark Azure Premium SSD to find how workloads affect performance and which metrics to monitor to know when troubleshooting potential disk issues.&lt;/p>
&lt;p>TLDR:&lt;/p>
&lt;ul>
&lt;li>Disable Azure cache for workloads with high number of random writes&lt;/li>
&lt;li>Use a P15 (256GB) or larger Premium SSD even though you might only need a fraction of it.&lt;/li>
&lt;/ul>
&lt;p>Table of contents&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="#Background" >Background&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#MetricsMethodologies" >Metric Methodologies&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#StorageBackground" >Storage Background&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#WhatToMeasure" >What to measure?&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#HowToMeasureDisk" >How to measure disk&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="HowToMeasureDiskOnAKS" >How to measure disk on Azure Kubernetes Service&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Tests" >Test results&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Test1" >Test 1 - Learning to dislike Azure Cache&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test2" >Test 2 - Disable Azure Cache - enable OS cache&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test3" >Test 3 - Disable OS cache&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test4" >Test 4 - Increase IO depth&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test5" >Test 5 - Larger block size, smaller IO depth&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test6" >Test 6 - Enable OS cache&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test7" >Test 7 - Random writes, small block size&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test8" >Test 8 - Large block size&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Conclusion" >Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="microsoft-azure">Microsoft Azure
&lt;/h4>&lt;blockquote>
&lt;p>&lt;a class="link" href="https://azure.microsoft.com/en-us/free/" target="_blank" rel="noopener"
>If you don&amp;rsquo;t have a Azure subscription already you can try services for $200 for 30 days.&lt;/a> The VM size &lt;strong>Standard_B2s&lt;/strong> is Burstable, has 2vCPU, 4GB RAM, 8GB temp storage and costs roughly $38 / month. For $200 you can have a cluster of 3-4 B2s nodes plus traffic, loadbalancers and other additional costs.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>See my blog post &lt;a class="link" href="2017-12-23-managed-kubernetes-on-azure.md" >Managed Kubernetes on Microsoft Azure (English)&lt;/a> for information on how to get up and running with Kubernetes on Azure.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;em>I have no affiliation with Microsoft Azure except using them through work.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="corrections">Corrections
&lt;/h2>&lt;p>&lt;strong>February 2020&lt;/strong>: Some of my previous knowledge and assumptions were not correct when applied to a cloud + Docker environment, as &lt;a class="link" href="https://github.com/jnoller/kubernaughty/issues/46" target="_blank" rel="noopener"
>explained by
AKS PM Jesse Noller on GitHub&lt;/a>.&lt;/p>
&lt;p>One of the issues is that even accessing a &amp;ldquo;data disk&amp;rdquo; will incur IOPS on the OS disk, and throttling of the OS disk will also constraint IOPS on the data disks.&lt;/p>
&lt;p>&lt;a id="Background">&lt;/a>&lt;/p>
&lt;h2 id="background">Background
&lt;/h2>&lt;p>I&amp;rsquo;m part of a team at Equinor building an internal PaaS based on Kubernetes running on AKS (Azure managed Kubernetes). We use Prometheus for monitoring each cluster as well as InfluxDB for collecting metrics from k6io which runs continous tests on our public endpoints.&lt;/p>
&lt;p>A couple of weeks ago we discovered some potential problems with both Prometheus and InfluxDB with memory usage and restarts. High CPU usage of type &lt;code>iowait&lt;/code> suggested that there might be some disk issues contributing to the problems.&lt;/p>
&lt;blockquote>
&lt;p>iowait: &amp;ldquo;Percentage of time that the CPU or CPUs were idle during which the system had an outstanding disk I/O request.&amp;rdquo; (&lt;a class="link" href="https://support.hpe.com/hpsc/doc/public/display?docId=c02783994" target="_blank" rel="noopener"
>hpe.com&lt;/a>). You can see &lt;code>iowait&lt;/code> on your Linux system by running &lt;code>top&lt;/code> and looking at the &lt;code>wa&lt;/code> percentage.&lt;/p>
&lt;p>PS: You can have a disk IO bottleneck even with low &lt;code>iowait&lt;/code>, and a high &lt;code>iowait&lt;/code> does not always indicate a disk IO bottleneck (&lt;a class="link" href="https://www.ibm.com/developerworks/community/blogs/AIXDownUnder/entry/iowait_a_misleading_indicator_of_i_o_performance54?lang=en" target="_blank" rel="noopener"
>ibm.com&lt;/a>).&lt;/p>
&lt;/blockquote>
&lt;p>First off we need to benchmark the underlying disk to get an understanding of it&amp;rsquo;s performance limits and characteristics. That is what we will cover in this post.&lt;/p>
&lt;p>&lt;a id="MetricsMethodologies">&lt;/a>&lt;/p>
&lt;h3 id="metric-methodologies">Metric Methodologies
&lt;/h3>&lt;p>There are two helpful methodologies when monitoring information systems. The first one is Utilization, Saturation and Errors (USE) from &lt;a class="link" href="http://www.brendangregg.com/usemethod.html" target="_blank" rel="noopener"
>Brendan Gregg&lt;/a> and the second one is Rate, Errors, Duration (RED) from &lt;a class="link" href="https://www.slideshare.net/weaveworks/monitoring-microservices" target="_blank" rel="noopener"
>Tom Wilkie&lt;/a>. RED is best suited when observing workloads and transactions while USE is best suited for observing resources.&lt;/p>
&lt;p>I&amp;rsquo;ll be using the USE method here. USE can be summarised as:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>For every resource, check utilization, saturation, and errors.&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>resource&lt;/strong>: all physical server functional components (CPUs, disks, busses, &amp;hellip;)&lt;/li>
&lt;li>&lt;strong>utilization&lt;/strong>: the average time that the resource was busy servicing work&lt;/li>
&lt;li>&lt;strong>saturation&lt;/strong>: the degree to which the resource has extra work which it can&amp;rsquo;t service, often queued&lt;/li>
&lt;li>&lt;strong>errors&lt;/strong>: the count of error events&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="StorageBackground">&lt;/a>&lt;/p>
&lt;h3 id="storage-background">Storage Background
&lt;/h3>&lt;p>Disk usage has two dimensions, throughput/bandwidth(BW) and operations per second (IOPS), and the underlying storage system will have upper limits of how much data it can receive (BW) and the number of operations it can perform per second (IOPS).&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Background - harddrive types&lt;/strong>: harddrives come in two types, Solid State Disks (SSD) and spindle (HDD). A SSD disk is a microship capable of permanently storing data while a HDD uses spinning platters to store data. HDDs have a fixed rate of rotation (RPM), typically 5.400 and 7.200 RPM for lower cost drives for home use and higher cost 10.000 and 15.000 RPM drives for server use. Over the last 20 years of HDDs their storage density has increased, but the RPM has largely stayed the same. A disk with twice the density (500GB to 1TB for example) can read twice as much data on a single rotation and thus increase the bandwidth significantly. However, reading or writing a random block still requires waiting for the disk to spin enough to reach the relevant sector on the disk. So IOPS has not increased much for HDDs and is still a low 125-150 IOPS for a 10.000 RPM enterprise disk. A SSD does not have any moving parts so is able to reach MUCH higher IOPS. A low end Samsung 960 EVO with 500GB capacity costs $150 and can achieve a whopping 330.000 IOPS! (&lt;a class="link" href="https://en.wikipedia.org/wiki/IOPS" target="_blank" rel="noopener"
>wikipedia.com&lt;/a>)&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Background - access patterns&lt;/strong>: The way a program uses storage also has a huge impact on the performance one can achieve. Sequential access is when we read or write a large file. When this happens the operating system and harddrive can optimize and &amp;ldquo;merge&amp;rdquo; operations so that we can read or write a much bigger chunk of data at a time. If we can read 1MB at a time 150 times per second we get 150MB/s of bandwidth. However, fully random access where the smallest chunk we read or write is a 4KB block the same 150 IOPS would only give a bandwidth of 0.6MB/s!&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Background - cloud vs physical&lt;/strong>: Now we know what HDDs are limited to a low IOPS and low IOPS combined with a random access pattern gives us a low overall bandwidth. There is a huge gotcha here when it comes to cloud. On Azure when using Premium Managed SSD the IOPS you are given is a factor of the disk size you provision (&lt;a class="link" href="https://azure.microsoft.com/en-us/pricing/details/managed-disks/" target="_blank" rel="noopener"
>microsoft.com&lt;/a>). A 512GB disk is limited to 2.300 IOPS and 150MB/s. With 100% random access that only gives about 9MB/s of bandwidth!&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Background - OS caching&lt;/strong>: To overcome some of the limitations of the underlying disk (mostly IOPS) there are potentially several layers of caching involved. Linux file systems can have &lt;code>writeback&lt;/code> enabled which causes Linux to temporarily store data that is going to be written to disk in memory. This can give a big performance increase when there are sudden spikes of writes exceeding the performance of the underlying disk. It also increases the chance that operations can be &lt;code>merged&lt;/code> where several write operations to areas of the disk that are nearby can be executed as one. This caching works best for sudden peaks and will not necessarily be enough if there is continous random writes to disk. This caching also means that even though an application thinks it has saved some data to disk it can be lost in the case of a power outage or other failure. Applications can also explicitly request &lt;code>direct&lt;/code> access where every operation is persisted to disk before receiving a confirmation. This is a trade-off between performance and durability that needs to be decided based on the application itself and the environment.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Background - Azure caching&lt;/strong>: Azure also provides read and write cache for its &lt;code>disks&lt;/code> which is enabled by default. As we will see soon for our use case it&amp;rsquo;s not a good idea to use.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="WhatToMeasure">&lt;/a>&lt;/p>
&lt;h2 id="what-to-measure">What to measure?
&lt;/h2>&lt;blockquote>
&lt;p>These metrics are collected by the Prometheus &lt;code>node-exporter&lt;/code> and follows it&amp;rsquo;s naming. I&amp;rsquo;ve also created a dashboard that is available on &lt;a class="link" href="https://grafana.com/dashboards/9852" target="_blank" rel="noopener"
>Grafana.com&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>With the USE methodology as a guideline and the two separate but related &amp;ldquo;resources&amp;rdquo;, bandwidth and IOPS we can look for some useful metrics.&lt;/p>
&lt;p>Utilization:&lt;/p>
&lt;ul>
&lt;li>&lt;code>rate(node_disk_written_bytes_total)&lt;/code> - Write bandwidth. The maximum is given by Azure and is 25MB/s for our disk size.&lt;/li>
&lt;li>&lt;code>rate(node_disk_writes_completed_total)&lt;/code> - Write operations. The maximum is given by Azure and is 120 IOPS for our disk size.&lt;/li>
&lt;li>&lt;code>rate(node_disk_io_time_seconds_total)&lt;/code> - Disk active time in percent. The time the disk was busy servicing requests. 100% means fully utilized.&lt;/li>
&lt;/ul>
&lt;p>Saturation:&lt;/p>
&lt;ul>
&lt;li>&lt;code>rate(node_cpu_seconds_total{mode=&amp;quot;iowait&amp;quot;}&lt;/code> - CPU iowait. The percentage of time a CPU core is blocked from doing useful work because it&amp;rsquo;s waiting for an IO operation to complete (typically disk, but can also be network).&lt;/li>
&lt;/ul>
&lt;p>Useful calculated metrics:&lt;/p>
&lt;ul>
&lt;li>&lt;code>rate(node_disk_write_time_seconds_total) / rate(node_disk_writes_completed_total)&lt;/code> - Write latency. How long from a write is requested until it&amp;rsquo;s completed.&lt;/li>
&lt;li>&lt;code>rate(node_disk_written_bytes_total) / rate(node_disk_writes_completed_total)&lt;/code> - Write size. How big the &lt;strong>average&lt;/strong> write operation is. 4KB is minimum and indicates 100% random access while 512KB is maximum and indicates sequential access.&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="HowToMeasureDisk">&lt;/a>&lt;/p>
&lt;h2 id="how-to-measure-disk">How to measure disk
&lt;/h2>&lt;p>The best tool for measuring disk performance is &lt;code>fio&lt;/code>, even though it might seem a bit intimidating at first due to it&amp;rsquo;s insane number of options.&lt;/p>
&lt;p>Installing &lt;code>fio&lt;/code> on Ubuntu:&lt;/p>
&lt;pre>&lt;code>apt-get install fio
&lt;/code>&lt;/pre>
&lt;p>&lt;code>fio&lt;/code> executes &lt;code>jobs&lt;/code> described in a file. Here is the top of our jobs file:&lt;/p>
&lt;pre>&lt;code>[global]
ioengine=libaio # sync|libaio|mmap
group_reporting
thread
size=10g # Size of test file
cpus_allowed=1 # Only use this CPU core
runtime=300s # Run test for 5 minutes
[test1]
filename=/tmp/fio-test-file
direct=1 # If value is true, use non-buffered I/O. Non-buffered I/O usually means O_DIRECT
readwrite=write # read|write|randread|randwrite|readwrite|randrw
iodepth=1 # How many operations to queue to the disk
blocksize=4k
&lt;/code>&lt;/pre>
&lt;p>The fields we will be changing for the various tests are &lt;code>direct&lt;/code>, &lt;code>readwrite&lt;/code>, &lt;code>iodepth&lt;/code> and &lt;code>blocksize&lt;/code>. Save the contents in a file named &lt;code>jobs.fio&lt;/code> and we run a test with &lt;code>fio --sector test1 jobs.fio&lt;/code> and wait until the test completes.&lt;/p>
&lt;blockquote>
&lt;p>PS: To run these tests on higher performance hardware and better caching you might want to set &lt;code>runtime&lt;/code> to &lt;code>0&lt;/code> to have the test run continously and monitor the metrics until performance reaches a steady-state.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="HowToMeasureDiskOnAKS">&lt;/a>&lt;/p>
&lt;h2 id="how-to-measure-disk-on-azure-kubernetes-service">How to measure disk on Azure Kubernetes Service
&lt;/h2>&lt;p>For this testing we use a standard Prometheus installation collecting data from &lt;code>node-exporter&lt;/code> and visualizing data in Grafana. The dashboard I created for the testing can be found here: &lt;a class="link" href="https://grafana.com/dashboards/9852" target="_blank" rel="noopener"
>https://grafana.com/dashboards/9852&lt;/a>.&lt;/p>
&lt;p>By default Kubernetes will schedule a Pod to any node that has enough memory and CPU for our workload. Since one of the tests we are going to run are on the OS disk we do not want the Pod to run on the same node as any other disk-intensive application, such as Prometheus.&lt;/p>
&lt;p>Look at which Pods are running with &lt;code>kubectl get pods -o wide&lt;/code> and look for a node that does not have any disk-intensive application.&lt;/p>
&lt;p>Then we tag that node with &lt;code>kubectl label nodes aks-nodepool1-37707184-2 tag=disktest&lt;/code>. This allows us later to specify that we want to run our testing Pod on that specific node.&lt;/p>
&lt;hr>
&lt;p>A StorageClass in Kubernetes is a specification of a underlying disk that Pods can request usage of through &lt;code>volumeClaimTemplates&lt;/code>. AKS comes with a default StorageClass &lt;code>managed-premium&lt;/code> that has caching enabled. Most of these tests require the Azure cache disabled so create a new StorageClass &lt;code>managed-premium-retain-nocache&lt;/code>:&lt;/p>
&lt;pre>&lt;code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: managed-premium-retain-nocache
provisioner: kubernetes.io/azure-disk
reclaimPolicy: Retain
parameters:
storageaccounttype: Premium_LRS
kind: Managed
cachingmode: None
&lt;/code>&lt;/pre>
&lt;p>You can add it to your cluster with:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2019-02-23-disk-performance-on-aks-part-1/storageclass.yaml
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>Next we create a StatefulSet that uses a &lt;code>volumeClaimTemplate&lt;/code> to request a 250GB Azure disk. This provisions a P15 Azure Premium SSD with 125MB/s bandwidth and 1100 IOPS:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2019-02-23-disk-performance-on-aks-part-1/ubuntu-statefulset.yaml
&lt;/code>&lt;/pre>
&lt;p>Follow the progress of the Pod creation with &lt;code>kubectl get pods -w&lt;/code> and wait until it is &lt;code>Running&lt;/code>.&lt;/p>
&lt;hr>
&lt;p>When the Pod is &lt;code>Running&lt;/code> we can start a shell on it with &lt;code>kubectl exec -it disk-test-0 bash&lt;/code>&lt;/p>
&lt;p>Once inside &lt;code>bash&lt;/code> on the Pod, we install &lt;code>fio&lt;/code>:&lt;/p>
&lt;pre>&lt;code>apt-get update &amp;amp;&amp;amp; apt-get install -y fio wget
&lt;/code>&lt;/pre>
&lt;p>And save the contents of in the Pod:&lt;/p>
&lt;pre>&lt;code>wget https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2019-02-23-disk-performance-on-aks-part-1/jobs.fio
&lt;/code>&lt;/pre>
&lt;p>Now we can run the different test sections one by one. &lt;strong>PS: If you don&amp;rsquo;t specify a section &lt;code>fio&lt;/code> will run all the tests &lt;em>simultaneously&lt;/em>, which is not what we want.&lt;/strong>&lt;/p>
&lt;pre>&lt;code>fio --section=test1 jobs.fio
fio --section=test2 jobs.fio
fio --section=test3 jobs.fio
fio --section=test4 jobs.fio
fio --section=test5 jobs.fio
fio --section=test6 jobs.fio
fio --section=test7 jobs.fio
fio --section=test8 jobs.fio
fio --section=test9 jobs.fio
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="Tests">&lt;/a>&lt;/p>
&lt;h2 id="test-results">Test results
&lt;/h2>&lt;p>&lt;a id="Test1">&lt;/a>&lt;/p>
&lt;h3 id="test-1---learning-to-dislike-azure-cache">Test 1 - Learning to dislike Azure Cache
&lt;/h3>&lt;p>&lt;em>Sequential write, 4K block size, Azure Cache enabled, OS cache disabled. See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test1.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>I run the first tests on the OS disk of a Kubernetes node. The OS disks have Azure caching enabled.&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1.png"
width="1697"
height="1232"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1_hu1499039068193334488.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1_hu13957841185659198382.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="330px"
>&lt;/p>
&lt;p>The first 1-2 minutes of the test I get very good performance of 45MB/s and ~11.500 IOPS but that drops to 0 very quickly as the cache is full and busy writing things to the underlying disk. When that happens everything freezes and I cannot even execute shell commands. After stopping the test the system still hangs for a bit while the cache empties.&lt;/p>
&lt;p>The maximum latency measured by &lt;code>fio&lt;/code> was 108751k usec. Or about 108 seconds!&lt;/p>
&lt;blockquote>
&lt;p>For the first try of these tests a 20-30 second period of very fast writes (250MB/s) caused a 7-8 minutes hang while the cache emptied. Trying again caused another pattern of lower peak performance with shorter hangs in between. Very unpredictable.
I&amp;rsquo;m not sure what to make of this. It&amp;rsquo;s not acceptable that a Kubernetes node becomes unresponsive for many minutes following a short burst of writing. There are scattered recommendations online of disabling caching for write-heavy applications. Since I have not found any way to measure the Azure cache itself, the results are unpredictable and potentially very impactful as well as making it very hard to use the metrics we do have to evaluate application and storage behaviour I&amp;rsquo;ve concluded that it&amp;rsquo;s best to use data disks with caching disabled for our workloads (you cannot disable caching on an AKS node OS disk).&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test2">&lt;/a>&lt;/p>
&lt;h3 id="test-2---disable-azure-cache---enable-os-cache">Test 2 - Disable Azure Cache - enable OS cache
&lt;/h3>&lt;p>&lt;em>Sequential write, 4K block size. &lt;strong>Change: Azure cache disabled, OS caching enabled.&lt;/strong> See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test2.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test2.png"
width="1703"
height="1226"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test2_hu9653487243883720288.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test2_hu12369574169741432704.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="333px"
>&lt;/p>
&lt;p>If we swap the Azure cache for the Linux OS cache we see that &lt;code>iowait&lt;/code> increases while the writing occurs. The application sees high write performance until the number of &lt;code>Dirty bytes&lt;/code> reaches a threshold of about 3.7GB of memory. The performance of the underlying disk is 125MB/s and 250 IOPS. Here we are throttled by the 125MB/s limit of the Azure P15 Premium SSD.&lt;/p>
&lt;p>Also notice that on sequential writes of 4K with OS caching the actual blocks written to disk is 512K which saves us a lot of IOPS. This will become important later.&lt;/p>
&lt;p>&lt;a id="Test3">&lt;/a>&lt;/p>
&lt;h3 id="test-3---disable-os-cache">Test 3 - Disable OS cache
&lt;/h3>&lt;p>&lt;em>Sequential write, 4K block size. &lt;strong>Change: OS caching disabled.&lt;/strong> See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test3.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test3.png"
width="1698"
height="1233"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test3_hu9187350742741251895.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test3_hu14645523390977373096.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="330px"
>&lt;/p>
&lt;blockquote>
&lt;p>By disabling the OS cache (&lt;code>direct=1&lt;/code>) the results are consistent and predictable. There is no &lt;code>iowait&lt;/code> since the application does not have multiple writes pending at the same time. Because of the 2-3ms latency of the disks we are not able to get more than about 400 IOPS. This gives us a meager 1.5MB/s even though the disk is limited to 1100 IOPS and 125MB/s. To reach that we need multiple simultaneous writes or a bigger IO depth (queue). &lt;code>Disk active time&lt;/code> is also 0% which indicates that the disk is not saturated.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test4">&lt;/a>&lt;/p>
&lt;h3 id="test-4---increase-io-depth">Test 4 - Increase IO depth
&lt;/h3>&lt;p>&lt;em>Sequential write, 4K block size, OS caching disabled. &lt;strong>Change: IO depth 16.&lt;/strong> See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test4.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test4.png"
width="1698"
height="1237"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test4_hu5722226652359263237.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test4_hu2996385930643672719.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="329px"
>&lt;/p>
&lt;blockquote>
&lt;p>For this test we only increase the IO depth from 1 to 16. IO depth is the number of write operations &lt;code>fio&lt;/code> will execute simultaneously. Since we are using &lt;code>direct&lt;/code> these will be queued by the OS for writing. We are now able to hit the performance limit of 1100 IOPS. &lt;code>Disk active time&lt;/code> is now steady at 100% indicating that we have saturated the disk.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test5">&lt;/a>&lt;/p>
&lt;h3 id="test-5---larger-block-size-smaller-io-depth">Test 5 - Larger block size, smaller IO depth
&lt;/h3>&lt;p>&lt;em>Sequential write, OS caching disabled. &lt;strong>Change: 128K block size, IO depth 1.&lt;/strong> See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test5.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test5.png"
width="1699"
height="1229"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test5_hu14954921568768718351.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test5_hu3268361891850381379.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;blockquote>
&lt;p>We increase the block size to 128KB and reduce the IO depth to 1 again. The write latency for larger blocks increase to ~5ms which gives us 200 IOPS and 28MB/s. The disk is not saturated.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test6">&lt;/a>&lt;/p>
&lt;h3 id="test-6---enable-os-cache">Test 6 - Enable OS cache
&lt;/h3>&lt;p>&lt;em>Sequential write, 256K block size, IO depth 1. &lt;strong>Change: OS caching enabled.&lt;/strong> See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test6.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test6.png"
width="1697"
height="1230"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test6_hu5250198874418650337.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test6_hu6421477374997710977.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="331px"
>&lt;/p>
&lt;blockquote>
&lt;p>We have now enabled the OS cache/buffer (&lt;code>direct=0&lt;/code>). We can see that the writes hitting the disk are now merged to 512KB blocks. We are hitting the 125MB/s limit with about 250 IOPS. Enabling the cache also has other effects: CPU suddenly shows significant IO wait. The write latency shoots through the roof. Also note that the writing continued for 30-40 seconds after the test was done. &lt;strong>This also means that the bandwidth and IOPS that &lt;code>fio&lt;/code> sees and reports is higher than what is actually hitting the disk.&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test7">&lt;/a>&lt;/p>
&lt;h3 id="test-7---random-writes-small-block-size">Test 7 - Random writes, small block size
&lt;/h3>&lt;p>&lt;em>IO depth 1, OS caching enabled. &lt;strong>Change: Random write, 4K block size.&lt;/strong> See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test7.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test7.png"
width="1702"
height="1239"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test7_hu9566364161109018395.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test7_hu821024261791687868.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="329px"
>&lt;/p>
&lt;blockquote>
&lt;p>Here we go from sequential writes to random writes. We are limited by IOPS. The average size of the blocks actually written to disks, and the IOPS required to hit the bandwidth limit is actually varying a bit throughout the test. The time taken to empty the cache is about as long as I ran the test (4-5 minutes).&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test8">&lt;/a>&lt;/p>
&lt;h3 id="test-8---large-block-size">Test 8 - Large block size
&lt;/h3>&lt;p>&lt;em>Random write, OS caching enabled. &lt;strong>Change: 256K block size, IO depth 16.&lt;/strong> See &lt;a class="link" href="https://blog.stian.omg.lol/attachments/2019-02-23-disk-performance-on-aks-part-1/test8.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test8.png"
width="1699"
height="1235"
srcset="https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test8_hu1300341601469829731.png 480w, https://blog.stian.omg.lol/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test8_hu923727430283491984.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="330px"
>&lt;/p>
&lt;blockquote>
&lt;p>Increasing the block size to 256K makes us bandwidth limited to 125MB/s.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Conclusion">&lt;/a>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>Access patterns and block sizes have a tremendous impact on the amount of data we are able to write to disk.&lt;/p></description></item><item><title>Managed Kubernetes on Microsoft Azure (English)</title><link>https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/</link><pubDate>Fri, 29 Dec 2017 00:00:00 +0000</pubDate><guid>https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/</guid><description>&lt;img src="https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/2017-12-23-managed-kubernetes-on-azure-eng.png" alt="Featured image of post Managed Kubernetes on Microsoft Azure (English)" />&lt;p>&lt;em>A few days ago I wrote a walkthrough of &lt;a class="link" href="https://blog.stian.omg.lol/2017/12/25/managed-kubernetes-on-azure.html" >setting up Azure Container Service (AKS) in Norwegian&lt;/a>. Someone asked me for an English version of that, and here it is.&lt;/em>&lt;/p>
&lt;p>Kubernetes(K8s) is becoming the de-facto standard for deploying container-based applications and workloads. Microsoft is currently in preview of their managed Kubernetes offering (Azure Kubernetes Service, AKS) which makes it easy to create a Kubernetes cluster and deploy workloads without the skill and time required to manage day-to-day operations of a Kubernetes-cluster, which today can be complex and time consuming.&lt;/p>
&lt;p>In this post we will set up a Kubernetes cluster from scratch using Azure CLI.&lt;/p>
&lt;p>Table of contents&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="#Background" >Background&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Dockercontainers" >Docker containers&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Containerorchestration" >Container orchestration&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#QuickstartAKS" >Getting started with Azure Kubernetes - AKS&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Caveats" >Caveats&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Preparations" >Preparations&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#AzureLogin" >Azure login&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#ActivateContainerService" >Activate ContainerService&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#CreateResourceGroup" >Create a resource group&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#CreateK8sCluster" >Create a Kubernetes cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#InstallKubectl" >Install kubectl&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#InspectCluster" >Inspect cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#StartNginx" >Start some nginx containere&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#NginxService" >Making nginx available with a service&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#ScaleCluster" >Scale cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#DeleteCluster" >Delete cluster&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Bonusmaterial" >Bonus material&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#HelmIntro" >Deploying services with Helm&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#HelmMinecraft" >Deploy MineCraft with Helm&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#KubernetesDashboard" >Kubernetes Dashboard&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Conclusion" >Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="microsoft-azure">Microsoft Azure
&lt;/h4>&lt;blockquote>
&lt;p>&lt;a class="link" href="https://azure.microsoft.com/en-us/free/" target="_blank" rel="noopener"
>If you don&amp;rsquo;t have a Azure subscription already you can try services for $200 for 30 days.&lt;/a> The VM size &lt;strong>Standard_B2s&lt;/strong> is Burstable, has 2vCPU, 4GB RAM, 8GB temp storage and costs roughly $38 / month. For $200 you can have a cluster of 3-4 B2s nodes plus traffic, loadbalancers and other additional costs.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;em>We have no affiliation with Microsoft Azure except their sponsorship of our startup &lt;a class="link" href="http://www.datadynamics.no/" target="_blank" rel="noopener"
>DataDynamics&lt;/a> with cloud services for 24 months in their &lt;a class="link" href="https://bizspark.microsoft.com/" target="_blank" rel="noopener"
>BizSpark program&lt;/a>.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Background">&lt;/a>&lt;/p>
&lt;h2 id="background">Background
&lt;/h2>&lt;p>&lt;a id="Dockercontainers">&lt;/a>&lt;/p>
&lt;h3 id="docker-containers">Docker containers
&lt;/h3>&lt;p>&lt;em>We will not do a deep dive on Docker containers in this post, but here is a summary for those who are not familiar with it.&lt;/em>&lt;/p>
&lt;p>Docker is a way to package software so that it can run on the most popular platforms without worrying about installation, dependencies and to a certain degree, configuration.&lt;/p>
&lt;p>In addition, a Docker container uses the operating system of the host machine when it runs. Because of this it&amp;rsquo;s possible to run many more containers on the same host machine compared to running virtual machines.&lt;/p>
&lt;p>Here is a incomplete and rough comparison between a Docker container and a virtual machine:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Virtual machine&lt;/th>
&lt;th>Docker container&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Image size&lt;/td>
&lt;td>from 200MB to many GB&lt;/td>
&lt;td>from 10MB to 3-400MB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Startup time&lt;/td>
&lt;td>60 seconds +&lt;/td>
&lt;td>1-10 seconds&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Memory usage&lt;/td>
&lt;td>256MB-512MB-1GB +&lt;/td>
&lt;td>2MB +&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Security&lt;/td>
&lt;td>Good isolation between VMs&lt;/td>
&lt;td>Not as good isolation between containers&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Building image&lt;/td>
&lt;td>Minutes&lt;/td>
&lt;td>Seconds&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> The numbers for virtual machines is taken from memory. I tried starting a MySQL virtual appliance on my laptop but VMware Player refuses to run because of Windows Hyper-V incompatibility. VMware Workstation refuses to run because of license issues and Oracle VirtualBox repeatedly gives me a nasty bluescreen. Hooray!&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong> The smallest and fastest Docker images are built on Alpine Linux. For the webserver Nginx the Alpine-based image is 15MB compared to 108MB for the normal Debian-based image. PostgreSQL:Alpine is 38MB compared to 287MB with &amp;ldquo;full&amp;rdquo; OS. Last version of MySQL is 343MB but will in version 8 support Alpine Linux as well.&lt;/p>
&lt;/blockquote>
&lt;p>To recap, some of the advantages of Docker containers are:&lt;/p>
&lt;ul>
&lt;li>Compatibility across platforms, Linux, Windows, MacOS.&lt;/li>
&lt;li>10-100x smaller size. Faster to download, build and upload.&lt;/li>
&lt;li>Memory usage only for application and not base OS.
&lt;ul>
&lt;li>Advantage when developing. Ability to run 10-20-30 containers on a development laptop.&lt;/li>
&lt;li>Advantage in production. Can reduce hardware/cloud costs considerably.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Near instant startup. Makes dynamic scaling of applications easier.&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://store.docker.com/editions/community/docker-ce-desktop-windows" target="_blank" rel="noopener"
>Download Docker for Windows here.&lt;/a>&lt;/p>
&lt;p>To start a MySQL database container from Windows CMD or Powershell:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">docker run --name mysql -p 3306:3306 -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Stop the container with:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">docker kill mysql
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>You can search for already built Docker images on &lt;a class="link" href="https://hub.docker.com/" target="_blank" rel="noopener"
>Docker Hub&lt;/a>. It&amp;rsquo;s also possible to create private Docker repositories for your own software that you don&amp;rsquo;t want to be publicly available.&lt;/p>
&lt;p>&lt;a id="Containerorchestration">&lt;/a>&lt;/p>
&lt;h4 id="container-orchestration">Container orchestration
&lt;/h4>&lt;p>Now that Docker container images has become the preferred way to package and distribute software on the Linux platform, there has emerged a need for systems to coordinate running and deploying these containers. Similar to the ecosystem of products VMware has built up around development and operation of virtual machines.&lt;/p>
&lt;p>Container orchestration systems have the responsibility for:&lt;/p>
&lt;ul>
&lt;li>Load balancing.&lt;/li>
&lt;li>Service discovery.&lt;/li>
&lt;li>Health checks.&lt;/li>
&lt;li>Automatic scaling and restarting of host nodes and containers.&lt;/li>
&lt;li>Zero downtime upgrades (rolling deploys).&lt;/li>
&lt;/ul>
&lt;p>Until recently the ecosystem around container orchestration has been fragmented, and the most popular alternatives have been:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://kubernetes.io/" target="_blank" rel="noopener"
>Kubernetes&lt;/a> (Originaly from Google, now managed by CNCF, the Cloud Native Computing Foundation)&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.docker.com/engine/swarm/" target="_blank" rel="noopener"
>Swarm&lt;/a> (From the maker of Docker)&lt;/li>
&lt;li>&lt;a class="link" href="http://mesos.apache.org/" target="_blank" rel="noopener"
>Mesos&lt;/a> (From Apache Software Foundation)&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/coreos/fleet" target="_blank" rel="noopener"
>Fleet&lt;/a> (From CoreOS)&lt;/li>
&lt;/ul>
&lt;p>But the last year there has been a convergence towards Kubernetes as the preferred solution.&lt;/p>
&lt;ul>
&lt;li>7 February
&lt;ul>
&lt;li>&lt;a class="link" href="https://coreos.com/blog/migrating-from-fleet-to-kubernetes.html" target="_blank" rel="noopener"
>CoreOS announces that they are removing Fleet from Container Linux and recommends Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>27 July
&lt;ul>
&lt;li>&lt;a class="link" href="https://azure.microsoft.com/en-us/blog/announcing-cncf/" target="_blank" rel="noopener"
>Microsoft joins the CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>9 August
&lt;ul>
&lt;li>&lt;a class="link" href="https://techcrunch.com/2017/08/09/aws-joins-the-cloud-native-computing-foundation/" target="_blank" rel="noopener"
>Amazon Web Services join the CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>29 August
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.geekwire.com/2017/now-vmware-pivotal-cncf-becoming-hub-enterprise-tech/" target="_blank" rel="noopener"
>VMware and Pivotal joins the CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>17 September
&lt;ul>
&lt;li>&lt;a class="link" href="https://techcrunch.com/2017/09/13/oracle-joins-the-cloud-native-computing-foundation-as-a-platinum-member/" target="_blank" rel="noopener"
>Oracle joins the CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>17 October
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.theregister.co.uk/2017/10/17/docker_ee_kubernetes_support/" target="_blank" rel="noopener"
>Docker announces native support for Kubernetes in addition to it&amp;rsquo;s own Swarm product&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>24 October
&lt;ul>
&lt;li>&lt;a class="link" href="https://azure.microsoft.com/en-us/blog/introducing-azure-container-service-aks-managed-kubernetes-and-azure-container-registry-geo-replication/" target="_blank" rel="noopener"
>Microsoft Azure announces the managed Kubernetes service AKS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>29 November
&lt;ul>
&lt;li>&lt;a class="link" href="https://aws.amazon.com/blogs/aws/amazon-elastic-container-service-for-kubernetes/" target="_blank" rel="noopener"
>Amazon Web Services announces the managed Kubernetes service EKS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Especially the last two news items are important. Deploying and running your own Kubernetes-installation requires time and skills (&lt;a class="link" href="https://stripe.com/blog/operating-kubernetes" target="_blank" rel="noopener"
>Read how Stripe used 5 months to trust running Kubernetes in production, just for batch jobs.&lt;/a>)&lt;/p>
&lt;p>Until now the choice has been running your own Kubernetes cluster or using Google Container Engine which has been &lt;a class="link" href="https://cloudplatform.googleblog.com/2014/11/unleashing-containers-and-kubernetes-with-google-compute-engine.html" target="_blank" rel="noopener"
>using Kubernetes since 2014&lt;/a>. Many of us feel a certain discomfort by locking ourselves to one provider. But this is now changing when you can develop infrastructure on Kubernetes and choose between the 3 large cloud providers in addition to running your own cluster if wanted. &lt;strong>*&lt;/strong>&lt;/p>
&lt;p>&lt;strong>*&lt;/strong> Kubernetes is a fast moving project, and features might be available on the different platforms on different timelines.&lt;/p>
&lt;p>&lt;a id="QuickstartAKS">&lt;/a>&lt;/p>
&lt;h2 id="getting-started-with-azure-kubernetes---aks">Getting started with Azure Kubernetes - AKS
&lt;/h2>&lt;p>&lt;a id="Caveats">&lt;/a>&lt;/p>
&lt;h3 id="caveats">Caveats
&lt;/h3>&lt;blockquote>
&lt;p>This guide is based on the documentation on &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough" target="_blank" rel="noopener"
>Microsoft.com&lt;/a>. Setting up a Azure Kubernetes cluster did not work in the beginning of December, but today, 23. December, it seems to work fairly well. But, upgrading the cluster from Kubernetes 1.7 to 1.8 for example does NOT work.&lt;/p>
&lt;p>AKS is in Preview and Azure are working continuously to make AKS stable and to support as many Kubernetes-features as possible. Amazon Web Services has a similar closed invite-only Preview currently while working on stability and features.&lt;/p>
&lt;p>Both Azure and AWS expresses expectations about their Kubernetes offerings will be ready for production in 2018.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Preparations">&lt;/a>&lt;/p>
&lt;h3 id="preparations">Preparations
&lt;/h3>&lt;p>You need Azure-CLI (version 2.0.21 or newer) to execute the &lt;code>az&lt;/code> commands:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://aka.ms/InstallAzureCliWindows" target="_blank" rel="noopener"
>Download Azure-CLI here&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest" target="_blank" rel="noopener"
>Information about Azure-CLI on MacOS and Linux here&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>All commands executed in Windows PowerShell.&lt;/p>
&lt;p>&lt;a id="AzureLogin">&lt;/a>&lt;/p>
&lt;h3 id="azure-login">Azure login
&lt;/h3>&lt;p>Log on to Azure:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az login
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>You will get a link to open in your browser together with an authentication code. Enter the code on the webpage and &lt;code>az login&lt;/code> will save the login information so that you will not have to authenticate again on the same machine.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> The login information gets saved in &lt;code>C:\Users\Username\.azure\&lt;/code>. You have to make sure nobody can access these files. They will then have full access to your Azure account.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="ActivateContainerService">&lt;/a>&lt;/p>
&lt;h3 id="activate-containerservice">Activate ContainerService
&lt;/h3>&lt;p>Since AKS is in Preview/Beta, you explicitly have to activate it in your subscription to get access to the &lt;code>aks&lt;/code> subcommands.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az provider register -n Microsoft.ContainerService
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">az provider show -n Microsoft.ContainerService
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="CreateResourceGroup">&lt;/a>&lt;/p>
&lt;h3 id="create-a-resource-group">Create a resource group
&lt;/h3>&lt;p>Here we create a resource group named &amp;ldquo;my_aks_rg&amp;rdquo; in Azure region West Europe.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az group create --name my_aks_rg --location westeurope
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong>
To see a list of all available Azure regions, use the command &lt;code>az account list-locations --output table&lt;/code>. &lt;strong>PS&lt;/strong> AKS might not be available in all regions yet!&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="CreateK8sCluster">&lt;/a>&lt;/p>
&lt;h3 id="create-kubernetes-cluster">Create Kubernetes cluster
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks create --resource-group my_aks_rg --name my_cluster --node-count 3 --generate-ssh-keys --node-vm-size Standard_B2s --node-osdisk-size 128 --kubernetes-version 1.8.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;code>--node-count&lt;/code>
&lt;ul>
&lt;li>Number of agent(host) nodes available to run containers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--generate-ssh-keys&lt;/code>
&lt;ul>
&lt;li>Creates and prints a SSH key which can be used for SSHing directly to the agent nodes.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--node-vm-size&lt;/code>
&lt;ul>
&lt;li>Which size Azure VMs the agent nodes should be created as. To see available sizes use &lt;code>az vm list-sizes -l westeurope --output table&lt;/code> and &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes" target="_blank" rel="noopener"
>Microsofts webpages&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--node-osdisk-size&lt;/code>
&lt;ul>
&lt;li>Disk size of the agent nodes in GB. &lt;strong>PS&lt;/strong> Containers can be stopped and moved to another host if Kubernetes finds it necessary or if a agent node disappears. All data saved locally in the container will be gone. If saving data permanently use Kubernetes PersistentVolumes and not the local agent node or container disks.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--kubernetes-version&lt;/code>
&lt;ul>
&lt;li>Which Kubernetes version to install. Azure does NOT necessarily install the last version by default, and currently upgrading with &lt;code>az aks upgrade&lt;/code> does not work. Latest version available right now is 1.8.2. It&amp;rsquo;s recommended to use the latest available version since there is a lot of changes from version to version. The documentation is also much better for newer versions.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Save the output of the command in a file in a secure location. It contains keys that can be used to connect to the cluster with SSH. Even though that should not in theory be necessary.&lt;/p>
&lt;p>&lt;a id="InstallKubectl">&lt;/a>&lt;/p>
&lt;h3 id="install-kubectl">Install kubectl
&lt;/h3>&lt;p>&lt;code>kubectl&lt;/code> is the client which performs all operations against your Kubernetes cluster. Azure CLI can install &lt;code>kubectl&lt;/code> for you:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks install-cli
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>After &lt;code>kubectl&lt;/code> is installed we need to get login information so that &lt;code>kubectl&lt;/code> can communicate with the Kubernetes cluster.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks get-credentials --resource-group my_aks_rg --name my_cluster
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>The login information is saved in &lt;code>C:\Users\Username\.kube\config&lt;/code>. Keep these files secure as well.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong> When you have several Kubernetes clusters you can change which one &lt;code>kubectl&lt;/code> talks to with &lt;code>kubectl config get-contexts&lt;/code> and &lt;code>kubectl config set-context my_cluster&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="InspectCluster">&lt;/a>&lt;/p>
&lt;h3 id="inspect-cluster">Inspect cluster
&lt;/h3>&lt;p>To check that the cluster and &lt;code>kubectl&lt;/code> works we start with a couple of commands.&lt;/p>
&lt;p>See all agent nodes and status:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get nodes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME STATUS AGE VERSION
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-0 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-1 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-2 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>See all services, pods and deployments:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get all --all-namespaces
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system po/kubernetes-dashboard-6fc8cf9586-frpkn 1/1 Running 0 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system svc/kubernetes-dashboard 10.0.161.132 &amp;lt;none&amp;gt; 80/TCP 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system deploy/kubernetes-dashboard 1 1 1 1 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME DESIRED CURRENT READY AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system rs/kubernetes-dashboard-6fc8cf9586 1 1 1 3d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>This is just some of the output from this command. You do not have to know what the resources in the &lt;code>kube-system&lt;/code> namespace does. That is part of the intention when Microsoft is managing our cluster for us.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Namespaces&lt;/strong>
In Kubernetes there is something called Namespaces. Resources in one namespace does not have automatic access to resources in another namespace. The services that runs Kubernetes itself use the namespace &lt;code>kube-system&lt;/code>. The &lt;code>kubectl&lt;/code> command by default only shows you resources in the &lt;code>default&lt;/code> namespace, unless you specify &lt;code>--all-namespaces&lt;/code> or &lt;code>--namespace=xx&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="StartNginx">&lt;/a>&lt;/p>
&lt;h3 id="start-some-nginx-containers">Start some nginx containers
&lt;/h3>&lt;blockquote>
&lt;p>An instance of a running container in Kubernetes is called a &lt;strong>Pod&lt;/strong>.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;code>nginx&lt;/code> is a fast and flexible web server.&lt;/p>
&lt;/blockquote>
&lt;p>Now that the clsuter is up we can start rolling out services and deployments on it.&lt;/p>
&lt;p>Lets start with creating a Deployment consiting of 3 containers all running the &lt;code>nginx:mainline-alpine&lt;/code> image from &lt;a class="link" href="https://hub.docker.com/r/_/nginx/" target="_blank" rel="noopener"
>Docker hub&lt;/a>.&lt;/p>
&lt;p>&lt;strong>nginx-dep.yaml&lt;/strong> looks like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">apiVersion: apps/v1beta2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kind: Deployment
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: nginx-deployment
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> replicas: 3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> selector:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> matchLabels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> template:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> containers:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - name: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> image: nginx:mainline-alpine
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ports:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - containerPort: 80
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Load this into the cluster with &lt;code>kubectl create&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-dep.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>This command creates the resources described in the file. &lt;code>kubectl&lt;/code> can read files either from your local disk or from a web URL.&lt;/p>
&lt;blockquote>
&lt;p>After making changes to a resource definition (&lt;code>.yaml&lt;/code> file), you can update the resources in the cluster with &lt;code>kubetl replace -f resource.yaml&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>We can verify that the Deployment is ready:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get deploy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment 3 3 3 3 10m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>We can also get the actual Pods that are running:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get pods
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-dqwx5 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-xwzpw 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-z5tfk 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Logger&lt;/strong> We can view logs from one pod with &lt;code>kubectl logs nginx-deployment-569477d6d8-xwzpw&lt;/code>. But since we in this case don&amp;rsquo;t know which Pod ends up getting an incomming request we can view logs from all the Pods which have &lt;code>app=nginx&lt;/code> label: &lt;code>kubectl logs -lapp=nginx&lt;/code>. The use of &lt;code>app=nginx&lt;/code> is our choice in &lt;code>nginx-dep.yaml&lt;/code> when we configured &lt;code>spec.template.metadata.labels: app: nginx&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="NginxService">&lt;/a>&lt;/p>
&lt;h3 id="making-nginx-available-with-a-service">Making nginx available with a service
&lt;/h3>&lt;p>To send traffic to our new Pods we need to create a &lt;strong>Service&lt;/strong>. A service consists of one or more Pods which are chosen based on different criteria, for example which labels they have and whether the Pods are Running and Ready.&lt;/p>
&lt;p>Lets create a service which forwards traffic to all Pods with label &lt;code>app: nginx&lt;/code> and are listening to port 80. In addition we make the service available via a LoadBalancer:&lt;/p>
&lt;p>&lt;strong>nginx-svc.yaml&lt;/strong> looks like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">apiVersion: v1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kind: Service
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> type: LoadBalancer
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ports:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - port: 80
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: http
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> targetPort: 80
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> selector:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>We tell Kubernetes to create our service with &lt;code>kubectl create&lt;/code> as usual:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-svc.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>We can then wait and see which IP-address Azure assigns our service:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get svc -w
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx 10.0.24.11 13.95.173.255 80:31522/TCP 15m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> It can take a few minutes for Azure to allocate and assign a Public IP for us. In the mean time &lt;code>&amp;lt;pending&amp;gt;&lt;/code> will appear under EXTERNAL-IP.&lt;/p>
&lt;/blockquote>
&lt;p>A simple &lt;strong>Welcome to nginx&lt;/strong> webpage should now be available on http://13.95.173.255 (&lt;em>remember to replace with your own External-IP&lt;/em>).&lt;/p>
&lt;p>We can also delete the service and deployment afterwards:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl delete svc nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete deploy nginx-deployment
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="ScaleCluster">&lt;/a>&lt;/p>
&lt;h3 id="scaling-the-cluster">Scaling the cluster
&lt;/h3>&lt;p>If we want to change the number of agent nodes running Pods we can do that via Azure-CLI:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks scale --name my_cluster --resource-group my_aks_rg --node-count 5
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>Currently all nodes will be created with the same size as when we created the cluster. AKS will probably get support for &lt;a class="link" href="https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools" target="_blank" rel="noopener"
>&lt;strong>node-pools&lt;/strong>&lt;/a> next year. That will allow for creating different groups of nodes with different size and operating systems, both Linux and Windows.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="DeleteCluster">&lt;/a>&lt;/p>
&lt;h3 id="delete-cluster">Delete cluster
&lt;/h3>&lt;p>You can delete the whole cluster like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks delete --name my_cluster --resource-group my_aks_rg --yes
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="Bonusmaterial">&lt;/a>&lt;/p>
&lt;h2 id="bonus-material">Bonus material
&lt;/h2>&lt;p>Here is some bonus material if you want to go a bit further with Kubernetes.&lt;/p>
&lt;p>&lt;a id="HelmIntro">&lt;/a>&lt;/p>
&lt;h3 id="deploying-services-with-helm">Deploying services with Helm
&lt;/h3>&lt;p>&lt;a class="link" href="https://helm.sh/" target="_blank" rel="noopener"
>Helm&lt;/a> is a package manager and library of software that is ready to be deployed on a Kubernetes cluster.&lt;/p>
&lt;p>Start by downloading the &lt;a class="link" href="https://github.com/kubernetes/helm/releases" target="_blank" rel="noopener"
>Helm-client&lt;/a>. It will read login information etc. from the same location as &lt;code>kubectl&lt;/code> automatically.&lt;/p>
&lt;p>Install the Helm-server (&lt;strong>Tiller&lt;/strong>) on the Kubernetes cluster and update the package library:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">helm init
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm repo update
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>See available packages (&lt;strong>Charts&lt;/strong>) with &lt;code>helm search&lt;/code>.&lt;/p>
&lt;p>&lt;a id="HelmMinecraft">&lt;/a>&lt;/p>
&lt;h4 id="deploy-minecraft-with-helm">Deploy MineCraft with Helm
&lt;/h4>&lt;p>Lets deploy a MineCraft server installation on our cluster, just because we can :-)&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">helm install --name stians --set minecraftServer.eula=true stable/minecraft
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;code>--set&lt;/code> overrides one or more of the standard values configured in the package. The MineCraft package is made in a way where it does not start without accepting the user license agreement by setting the variable &lt;code>minecraftServer.eula&lt;/code>. All the variables that can be set in the MineCraft package are &lt;a class="link" href="https://github.com/kubernetes/charts/blob/master/stable/minecraft/values.yaml" target="_blank" rel="noopener"
>documented here&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>Then we wait for Azure to assign us a Public IP:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get svc -w
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">stians-minecraft 10.0.237.0 13.95.172.192 25565:30356/TCP 3m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Now we can connect to our MineCraft server on &lt;code>13.95.172.192:25565&lt;/code>!&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/minecraft-k8s.png"
width="856"
height="507"
srcset="https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/minecraft-k8s_hu13197832490752396686.png 480w, https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/minecraft-k8s_hu16352153299487520650.png 1024w"
loading="lazy"
alt="Kubernetes in MineCraft on Kubernetes"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="405px"
>&lt;/p>
&lt;p>&lt;a id="KubernetesDashboard">&lt;/a>&lt;/p>
&lt;h3 id="kubernetes-dashboard">Kubernetes Dashboard
&lt;/h3>&lt;p>Kubernetes also has a graphic web user-interface which makes it a bit easier to see which resources are in the cluster, view logs and even open a remote shell inside a running Pod, among other things.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl proxy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Starting to serve on 127.0.0.1:8001
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>kubectl&lt;/code> encrypts and tunnels the traffic to the Kubernetes API servers. The dashboard is available on &lt;a class="link" href="http://127.0.0.1:8001/ui/" target="_blank" rel="noopener"
>http://127.0.0.1:8001/ui/&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/k8s-dash.png"
width="899"
height="588"
srcset="https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/k8s-dash_hu15091082541702636396.png 480w, https://blog.stian.omg.lol/p/managed-kubernetes-on-microsoft-azure-english/k8s-dash_hu17318776918264020386.png 1024w"
loading="lazy"
alt="Kubernetes Dashboard"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>&lt;/p>
&lt;p>&lt;a id="Conclusion">&lt;/a>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>I hope you enjoy Kubernetes as much as I have. The learning curve can be a bit steep in the beginning, but it does not take long before you are productive.&lt;/p>
&lt;p>Look at the &lt;a class="link" href="https://v1-8.docs.kubernetes.io/docs/tutorials/" target="_blank" rel="noopener"
>official guides on Kubernetes.io&lt;/a> to learn more about defining different types of resources and services to run on Kubernetes. &lt;strong>PS: There are big changes from version to version so make sure you use the documentation for the correct version!&lt;/strong>&lt;/p>
&lt;p>Kubernetes also have a very active Slack-community on &lt;a class="link" href="http://slack.k8s.io/" target="_blank" rel="noopener"
>kubernetes.slack.com&lt;/a> that is worthwhile to check out.&lt;/p></description></item><item><title>Managed Kubernetes på Microsoft Azure (Norwegian)</title><link>https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/</link><pubDate>Mon, 25 Dec 2017 00:00:00 +0000</pubDate><guid>https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/</guid><description>&lt;img src="https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/2017-12-23-managed-kubernetes-on-azure.png" alt="Featured image of post Managed Kubernetes på Microsoft Azure (Norwegian)" />&lt;p>&lt;em>Update 29. Dec: There is an &lt;a class="link" href="https://blog.stian.omg.lol/2017/12/29/managed-kubernetes-on-azure-eng.html" >English version of this post here.&lt;/a>&lt;/em>&lt;/p>
&lt;p>Kubernetes (K8s) er i ferd med å bli de-facto standard for deployments av kontainer-baserte applikasjoner. Microsoft har nå preview av deres managed Kubernetes tjeneste (Azure Kubernetes Service, AKS) som gjør det enkelt å opprette et Kubernetes cluster og rulle ut tjenester uten å måtte ha kompetanse og tid til den daglige driften av selve Kubernetes-clusteret, som per i dag kan være relativt komplisert og tidkrevende.&lt;/p>
&lt;p>I denne posten setter vi opp et Kubernetes cluster fra scratch ved bruk av Azure CLI.&lt;/p>
&lt;p>Table of contents&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="#Bakgrunn" >Bakgrunn&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Dockercontainers" >Docker containers&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Containerorchestration" >Container orchestration&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#OppretteAKS" >Kom i gang med Azure Kubernetes - AKS&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Forbehold" >Forbehold&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Forberedelser" >Forberedelser&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Azureinnlogging" >Azure innlogging&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#AktiverContainerService" >Aktiver ContainerService&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#OpprettResourceGroup" >Opprett en resource group&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#OpprettK8sCluster" >Opprette Kubernetes cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#InstallerKubectl" >Installer kubectl&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#InspiserCluster" >Inspiser cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#StarteNginx" >Starte noen nginx containere&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#NginxService" >Gjøre nginx tilgjengelig med en tjeneste&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#ScaleCluster" >Skalere cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#DeleteCluster" >Slette cluster&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Bonusmateriale" >Bonusmateriale&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#HelmIntro" >Rulle ut tjenester med Helm pakker&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#HelmMinecraft" >MineCraft server med Helm&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#KubernetesDashboard" >Kubernetes Dashboard&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Konklusjon" >Konklusjon&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="microsoft-azure">Microsoft Azure
&lt;/h4>&lt;blockquote>
&lt;p>&lt;a class="link" href="https://azure.microsoft.com/en-us/free/" target="_blank" rel="noopener"
>Hvis du ikke har Azure fra før kan du prøve tjenester for $200 i 30 dager.&lt;/a> VM typen &lt;strong>Standard_B2s&lt;/strong> er Burstable, har 2vCPU, 4GB RAM, 8GB temp storage og koster ~$38 / mnd. For $200 kan du ha et cluster på 3-4 B2s noder plus trafikkostnad, lastbalanserere og andre nødvendige tjenester.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;em>Vi har ingen tilknytning til Microsoft bortsett fra at de sponser vår startup &lt;a class="link" href="http://www.datadynamics.no/" target="_blank" rel="noopener"
>DataDynamics&lt;/a> med cloud-tjenester i 24 mnd i deres &lt;a class="link" href="https://bizspark.microsoft.com/" target="_blank" rel="noopener"
>BizSpark program&lt;/a>.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Bakgrunn">&lt;/a>&lt;/p>
&lt;h2 id="bakgrunn">Bakgrunn
&lt;/h2>&lt;p>&lt;a id="Dockercontainers">&lt;/a>&lt;/p>
&lt;h3 id="docker-containers">Docker containers
&lt;/h3>&lt;p>&lt;em>Vi tar ikke for oss Docker containers i dybden i denne posten, men her er en kort oppsummering for de som ikke er kjent med teknologien.&lt;/em>&lt;/p>
&lt;p>Docker er en måte å pakketere programvare slik at det kan kjøres på samtlige populære platformer uten å måtte bruke mye tid på dependencies, oppsett og konfigurasjon.&lt;/p>
&lt;p>I tillegg bruker en Docker container operativsystemet på vertsmaskinen når den kjører. Dette gjør at en kan kjøre mange flere containere på samme vertsmaskin sammenlignet med virtuelle maskiner.&lt;/p>
&lt;p>Her er en ufullstendig og grov sammenligning mellom en Docker container og en virtuell maskin:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Virtuel maskin&lt;/th>
&lt;th>Docker container&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Image størrelse&lt;/td>
&lt;td>fra 200MB til mange GB&lt;/td>
&lt;td>fra 10MB til 3-400MB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Oppstartstid&lt;/td>
&lt;td>60 sekunder +&lt;/td>
&lt;td>1-10 sekunder&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Minnebruk&lt;/td>
&lt;td>256MB-512MB-1GB +&lt;/td>
&lt;td>2MB +&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sikkerhet&lt;/td>
&lt;td>God isolasjon mellom VM&lt;/td>
&lt;td>Dårligere isolasjon mellom containere&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Bygge image&lt;/td>
&lt;td>Minutter&lt;/td>
&lt;td>Sekunder&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> Tallene for virtuelle maskiner er tatt fra hukommelsen. Jeg forsøkte å starte en MySQL virtuell appliance på min laptop men VMware Player nekter å kjøre pga inkompatibilitet med Windows Hyper-V. VMware Workstation nekter å kjøre pga utgått lisens og Oracle VirtualBox gir en nasty bluescreen gang på gang. Hooray!&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong> De minste og raskeste Docker imagene er bygget på Alpine Linux. For webserveren Nginx er det Alpine-baserte imaget 15MB mot det Debian-baserte imaget på 108MB. PostgreSQL:Alpine er 38MB mot 287MB. Siste versjon av MySQL er 343MB men vil i versjon 8 støtte Alpine Linux også.&lt;/p>
&lt;/blockquote>
&lt;p>Noen av fordelene med Docker containers er altså:&lt;/p>
&lt;ul>
&lt;li>Kompatibilitet på tvers av platformer, Linux, Windows og MacOS.&lt;/li>
&lt;li>10-100x mindre størrelse. Raskere å laste ned, raskere å bygge, raskere å laste opp.&lt;/li>
&lt;li>Minnebruk kun for applikasjon og ikke eget OS.
&lt;ul>
&lt;li>Fordel under utvikling, kan kjøre 10-20-30 Docker containere samtidig på en laptop.&lt;/li>
&lt;li>Fordel i produksjon, kan redusere hardware utgifter betraktelig.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Oppstart på få sekunder. Gjør dynamisk skalering av applikasjoner mye enklere.&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://store.docker.com/editions/community/docker-ce-desktop-windows" target="_blank" rel="noopener"
>Last ned Docker for Windows her.&lt;/a>&lt;/p>
&lt;p>Og start en MySQL database fra Windows CMD eller Powershell:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">docker run --name mysql -p 3306:3306 -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Stop containeren med:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">docker kill mysql
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>En kan søke etter ferdige Docker images på &lt;a class="link" href="https://hub.docker.com/" target="_blank" rel="noopener"
>Docker Hub&lt;/a>. Det er også mulig å lage private Docker repositories for egen programvare som ikke skal være tilgjengelig for omverden.&lt;/p>
&lt;p>&lt;a id="Containerorchestration">&lt;/a>&lt;/p>
&lt;h4 id="container-orchestration">Container orchestration
&lt;/h4>&lt;p>Etter som Docker containers har blitt den foretrukne måten å pakke og distribuere programvare på Linux platformen de siste par årene har det vokst frem et behov for systemer som kan samkjøre drift og utrulling av disse containerene. Ikke ulikt det økosystemet av produkter VMware har bygget opp rundt utvikling og drift av virtuelle maskiner.&lt;/p>
&lt;p>Container orchestration systemene har som oppgave å sørge for:&lt;/p>
&lt;ul>
&lt;li>Lastbalansering.&lt;/li>
&lt;li>Service discovery.&lt;/li>
&lt;li>Health checks.&lt;/li>
&lt;li>Automatisk skalering og restarting av vertsmaskiner og containere.&lt;/li>
&lt;li>Oppgraderinger uten nedetid (rolling deploy).&lt;/li>
&lt;/ul>
&lt;p>Frem til nylig har økosystemet rundt container orchestration vært fragmentert og de mest populære alternativene har vært:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://kubernetes.io/" target="_blank" rel="noopener"
>Kubernetes&lt;/a> (Opprinnelig fra Google, nå styrt av CNCF, Cloud Native Computing Foundation)&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.docker.com/engine/swarm/" target="_blank" rel="noopener"
>Swarm&lt;/a> (Fra produsenten bak Docker)&lt;/li>
&lt;li>&lt;a class="link" href="http://mesos.apache.org/" target="_blank" rel="noopener"
>Mesos&lt;/a> (Fra Apache Software Foundation)&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/coreos/fleet" target="_blank" rel="noopener"
>Fleet&lt;/a> (Fra CoreOS)&lt;/li>
&lt;/ul>
&lt;p>Men det siste året har det vært en konvergens mot Kubernetes som foretrukket løsning.&lt;/p>
&lt;ul>
&lt;li>7 februar
&lt;ul>
&lt;li>&lt;a class="link" href="https://coreos.com/blog/migrating-from-fleet-to-kubernetes.html" target="_blank" rel="noopener"
>CoreOS annonserer at de fjerner Fleet fra Container Linux og anbefaler Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>27 juli
&lt;ul>
&lt;li>&lt;a class="link" href="https://azure.microsoft.com/en-us/blog/announcing-cncf/" target="_blank" rel="noopener"
>Microsoft slutter seg til CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>9 august
&lt;ul>
&lt;li>&lt;a class="link" href="https://techcrunch.com/2017/08/09/aws-joins-the-cloud-native-computing-foundation/" target="_blank" rel="noopener"
>Amazon Web Services slutter seg til CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>29 august
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.geekwire.com/2017/now-vmware-pivotal-cncf-becoming-hub-enterprise-tech/" target="_blank" rel="noopener"
>VMware og Pivotal slutter seg til CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>17 september
&lt;ul>
&lt;li>&lt;a class="link" href="https://techcrunch.com/2017/09/13/oracle-joins-the-cloud-native-computing-foundation-as-a-platinum-member/" target="_blank" rel="noopener"
>Oracle slutter seg til CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>17 oktober
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.theregister.co.uk/2017/10/17/docker_ee_kubernetes_support/" target="_blank" rel="noopener"
>Docker annonserer native støtte for Kubernetes i tillegg til sitt eget Swarm produkt&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>24 oktober
&lt;ul>
&lt;li>&lt;a class="link" href="https://azure.microsoft.com/en-us/blog/introducing-azure-container-service-aks-managed-kubernetes-and-azure-container-registry-geo-replication/" target="_blank" rel="noopener"
>Microsoft Azure annonserer managed Kubernetes med tjenesten AKS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>29 november
&lt;ul>
&lt;li>&lt;a class="link" href="https://aws.amazon.com/blogs/aws/amazon-elastic-container-service-for-kubernetes/" target="_blank" rel="noopener"
>Amazon Web Services annonserer managed Kubernetes med tjenesten EKS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>De to siste nyhetene er spesielt viktige. Å drifte sin egen Kubernetes-installasjon krever tid og kompetanse. (&lt;a class="link" href="https://stripe.com/blog/operating-kubernetes" target="_blank" rel="noopener"
>Les hvordan Stripe brukte 5 måneder på å bli fortrolig med å drifte sitt eget Kubernetes cluster, bare for batch jobs.&lt;/a>)&lt;/p>
&lt;p>Frem til nå har valget vært mellom å drifte sitt eget Kubernetes cluster eller bruke Google Container Engine som har &lt;a class="link" href="https://cloudplatform.googleblog.com/2014/11/unleashing-containers-and-kubernetes-with-google-compute-engine.html" target="_blank" rel="noopener"
>brukt Kubernetes siden 2014&lt;/a>. Mange av oss føler et visst ubehag ved å låse oss til én tilbyder. Men dette er nå anderledes når en kan utvikle infrastruktur på Kubernetes, og velge tilnærmet fritt &lt;strong>*&lt;/strong> mellom de 3 store cloud-tilbyderene i tillegg til å drifte selv om ønskelig.&lt;/p>
&lt;p>&lt;strong>*&lt;/strong> Kubernetes utvikles raskt, og funksjonalitet blir ofte ikke tilgjengelig på de ulike platformene samtidig.&lt;/p>
&lt;p>&lt;a id="OppretteAKS">&lt;/a>&lt;/p>
&lt;h2 id="opprette-azure-kubernetes-cluster">Opprette Azure Kubernetes Cluster
&lt;/h2>&lt;p>&lt;a id="Forbehold">&lt;/a>&lt;/p>
&lt;h3 id="forbehold">Forbehold
&lt;/h3>&lt;blockquote>
&lt;p>Denne gjennomgangen tar utgangspunkt i dokumentasjonen på &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough" target="_blank" rel="noopener"
>Microsoft.com&lt;/a>. Å sette opp et Azure Kubernetes cluster fungerte ikke i starten av desember, men per dags dato, 23. desember, ser det ut til å fungere relativt bra. Men, oppgradering av cluster fra Kubernetes 1.7 til 1.8 fungerer for eksempel IKKE.&lt;/p>
&lt;p>AKS er i Preview og Azure jobber kontinuerlig med å gjøre AKS stabilt og støtte så mange Kubernetes-funksjoner som mulig. Amazon Web Services har tilsvarende en lukket invite-only Preview per dags dato mens de også jobber med stabilitet og funksjonalitet.&lt;/p>
&lt;p>Både Azure og AWS uttrykker forventning om at deres Kubernetes tjenester skal være klare for produksjonsmiljø ila 2018.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Forberedelser">&lt;/a>&lt;/p>
&lt;h3 id="forberedelser">Forberedelser
&lt;/h3>&lt;p>Du behøver Azure-CLI (versjon 2.0.21 eller nyere) for å utføre kommandoene:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://aka.ms/InstallAzureCliWindows" target="_blank" rel="noopener"
>Last ned Azure-CLI her&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest" target="_blank" rel="noopener"
>Informasjon om Azure-CLI på MacOS og Linux finner du her&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Alle kommandoer gjøres i Windows PowerShell.&lt;/p>
&lt;p>&lt;a id="Azureinnlogging">&lt;/a>&lt;/p>
&lt;h3 id="azure-innlogging">Azure innlogging
&lt;/h3>&lt;p>Logg på Azure:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az login
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Du får en link som du åpner i din browser samt en autentiseringskode. Skriv koden på nettsiden og &lt;code>az login&lt;/code> lagrer påloggingsinformasjonen slik at du ikke behøver å autentisere igjen på samme maskin.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> Pålogingsinformasjonen lagres i &lt;code>C:\Users\Brukernavn\.azure\&lt;/code>. Du må selv passe på at ingen kopierer disse filene. Da får de full tilgang til din Azure konto.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="AktiverContainerService">&lt;/a>&lt;/p>
&lt;h3 id="aktiver-containerservice">Aktiver ContainerService
&lt;/h3>&lt;p>Siden AKS er i Preview/Beta må du eksplisitt aktivere det for å få tilgang til &lt;code>aks&lt;/code> kommandoene.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az provider register -n Microsoft.ContainerService
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">az provider show -n Microsoft.ContainerService
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="OpprettResourceGroup">&lt;/a>&lt;/p>
&lt;h3 id="opprett-en-resource-group">Opprett en resource group
&lt;/h3>&lt;p>Her oppretter vi en resource group med navn &amp;ldquo;min_aks_rg&amp;rdquo; i Azure region West Europe.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az group create --name min_aks_rg --location westeurope
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong>
For å se en liste over tilgjengelige Azure regioner, bruk kommandoen &lt;code>az account list-locations --output table&lt;/code>. &lt;strong>PS&lt;/strong> Det kan hende AKS ikke er tilgjengelig i alle regioner enda.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="OpprettK8sCluster">&lt;/a>&lt;/p>
&lt;h3 id="opprette-kubernetes-cluster">Opprette Kubernetes cluster
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks create --resource-group min_aks_rg --name mitt_cluster --node-count 3 --generate-ssh-keys --node-vm-size Standard_B2s --node-osdisk-size 256 --kubernetes-version 1.8.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;code>--node-count&lt;/code>
&lt;ul>
&lt;li>Antall vertsmaskiner tilgjengelig for å kjøre containers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--generate-ssh-keys&lt;/code>
&lt;ul>
&lt;li>Oppretter og outputter en SSH key som kan brukes for å SSHe direkte til vertsmaskinene.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--node-vm-size&lt;/code>
&lt;ul>
&lt;li>Hvilken type Azure VM clusteret skal bestå av. For å se tilgjengelige størrelser bruk &lt;code>az vm list-sizes -l westeurope --output table&lt;/code> og &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes" target="_blank" rel="noopener"
>Microsofts nettsider.&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--node-osdisk-size&lt;/code>
&lt;ul>
&lt;li>Disk størrelse på vertsmaskiner i GB. &lt;strong>PS&lt;/strong> Conteinere kan bli stoppet og flyttet til en annen host ved behov eller hvis en vertsmaskin forsvinner. Alle data lagret lokalt i conteineren blir da borte. Hvis en skal lagre ting permanent må en bruke PersistentVolumes og ikke lokal disk på vertsmaskin.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--kubernetes-version&lt;/code>
&lt;ul>
&lt;li>Hvilken Kubernetes versjon som skal installeres. Azure installerer IKKE den siste versjonen som standard, og per dags dato fungerer ikke &lt;code>az aks upgrade&lt;/code> tilstrekkelig. Siste tilgjengelige versjon per dags dato er 1.8.2. Det er en fordel å bruke siste versjon da det skjer store forbedringer i Kubernetes fra versjon til versjon. Dokumentasjon er også mye bedre for nyere versjoner.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Lagre teksten som kommandoen spytter ut i en fil på en trygg plass. Den inneholder nøkler som kan brukes for å kople til clusteret med SSH. Selv om det i teorien ikke skal være nødvendig.&lt;/p>
&lt;p>&lt;a id="InstallerKubectl">&lt;/a>&lt;/p>
&lt;h3 id="installer-kubectl">Installer kubectl
&lt;/h3>&lt;p>&lt;code>kubectl&lt;/code> er klienten som gjør alle operasjoner mot ditt Kubernetes cluster. Azure CLI kan installere &lt;code>kubectl&lt;/code> for deg:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks install-cli
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Etter &lt;code>kubectl&lt;/code> er installert behøver vi å få påloggingsinformasjon slik at &lt;code>kubectl&lt;/code> kan kommunisere med Kubernetes clusteret.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks get-credentials --resource-group min_aks_rg --name mitt_cluster
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Påloggingsinformasjonen lagres i &lt;code>C:\Users\Brukernavn\.kube\config&lt;/code>. Hold disse filene hemmelig også.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong> Når en har flere ulike Kubernetes clusters kan en bytte hvilken &lt;code>kubectl&lt;/code> skal snakke til med &lt;code>kubectl config get-contexts&lt;/code> og &lt;code>kubectl config set-context mitt_cluster&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="InspiserCluster">&lt;/a>&lt;/p>
&lt;h3 id="inspiser-cluster">Inspiser cluster
&lt;/h3>&lt;p>For å se at clusteret og &lt;code>kubectl&lt;/code> virker begynner vi med noen kommandoer.&lt;/p>
&lt;p>Se alle vertsmaskiner og status:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get nodes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME STATUS AGE VERSION
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-0 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-1 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-2 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Se alle tjenester, pods, deployments:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get all --all-namespaces
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system po/kubernetes-dashboard-6fc8cf9586-frpkn 1/1 Running 0 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system svc/kubernetes-dashboard 10.0.161.132 &amp;lt;none&amp;gt; 80/TCP 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system deploy/kubernetes-dashboard 1 1 1 1 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME DESIRED CURRENT READY AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system rs/kubernetes-dashboard-6fc8cf9586 1 1 1 3d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Jeg har bare tatt et lite utdrag fra denne kommandoen. Du behøver ikke å forstå hva alle ressursene i &lt;code>kube-system&lt;/code> namespacet gjør. Det er hensikten at du skal slippe det når Microsoft står for management av selve clusteret.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Namespaces&lt;/strong>
I Kubernetes er det noe som heter Namespaces. Ressurser i ett namespace har ikke automatisk tilgang til ressurser i et annet namespace. Tjenestene som Kubernetes selv benytter installeres i namespacet &lt;code>kube-system&lt;/code>. Kommandoen &lt;code>kubectl&lt;/code> viser deg vanligvis bare ressurser i &lt;code>default&lt;/code> namespace med mindre du spesifiserer &lt;code>--all-namespaces&lt;/code> eller &lt;code>--namespace=xx&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="StarteNginx">&lt;/a>&lt;/p>
&lt;h3 id="starte-noen-nginx-containere">Starte noen nginx containere
&lt;/h3>&lt;blockquote>
&lt;p>En instans av en kjørende container kalles i Kubernetes for en &lt;strong>Pod&lt;/strong>.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;code>nginx&lt;/code> er en rask og fleksibel webserver.&lt;/p>
&lt;/blockquote>
&lt;p>Nå som clusteret er oppe å kjøre kan vi begynne å rulle ut tjenster og deployments på det.&lt;/p>
&lt;p>Vi begynner med å lage en Deployment bestående av 3 containere som alle kjører &lt;code>nginx:mainline-alpine&lt;/code> imaget fra &lt;a class="link" href="https://hub.docker.com/r/_/nginx/" target="_blank" rel="noopener"
>Docker hub&lt;/a>.&lt;/p>
&lt;p>&lt;strong>nginx-dep.yaml&lt;/strong> ser slik ut:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">apiVersion: apps/v1beta2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kind: Deployment
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: nginx-deployment
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> replicas: 3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> selector:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> matchLabels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> template:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> containers:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - name: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> image: nginx:mainline-alpine
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ports:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - containerPort: 80
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Last denne inn på clusteret med &lt;code>kubectl create&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-dep.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Denne kommandoen oppretter ressursene beskrevet i filen. &lt;code>kubectl&lt;/code> kan lese filer enten lokalt fra din maskin eller fra en URL.&lt;/p>
&lt;blockquote>
&lt;p>Etter du har gjort endringer i en ressurs-definisjon (&lt;code>.yaml&lt;/code> fil) kan du oppdatere ressursene i clusteret med &lt;code>kubectl replace -f ressurs.yaml&lt;/code>&lt;/p>
&lt;/blockquote>
&lt;p>Vi kan verifisere at Deployment er klar:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get deploy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment 3 3 3 3 10m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Vi kan også hente de faktiske Pods som er startet:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get pods
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-dqwx5 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-xwzpw 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-z5tfk 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Logger&lt;/strong> Vi kan se logger fra én pod med &lt;code>kubectl logs nginx-deployment-569477d6d8-xwzpw&lt;/code>. Men siden vi i dette tilfellet ikke vet hvilken Pod som ender opp med å få innkommende forespørsler kan vi se logger fra alle Pods som har &lt;code>app=nginx&lt;/code> label: &lt;code>kubectl logs -lapp=nginx&lt;/code>. At vi her bruker &lt;code>app=nginx&lt;/code> har vi selv bestemt i &lt;code>nginx-dep.yaml&lt;/code> når vi satt &lt;code>spec.template.metadata.labels: app: nginx&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="NginxService">&lt;/a>&lt;/p>
&lt;h3 id="gjøre-nginx-tilgjengelig-med-en-tjeneste">Gjøre nginx tilgjengelig med en tjeneste
&lt;/h3>&lt;p>For å kommunisere med våre nye Pods behøver vi å opprette en tjeneste (&lt;strong>Service&lt;/strong>). En tjeneste består av en eller flere Pods som velges basert på ulike kriterier, blant annet hvilke labels de har og om Podene det gjelder er Running og Ready.&lt;/p>
&lt;p>Nå lager vi en tjeneste som ruter trafikk til alle Pods som har label &lt;code>app: nginx&lt;/code> og som lytter på port 80. I tillegg gjør vi tjenesten tilgjengelig via en LoadBalancer:&lt;/p>
&lt;p>&lt;strong>nginx-svc.yaml&lt;/strong> ser slik ut:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">apiVersion: v1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kind: Service
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> type: LoadBalancer
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ports:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - port: 80
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: http
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> targetPort: 80
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> selector:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Vi ber Kubernetes om å opprette tjeneten vår med &lt;code>kubectl create&lt;/code> som vanlig:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-svc.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Deretter kan vi se hvilken IP-adresse tjenesten vår har fått av Azure:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get svc -w
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx 10.0.24.11 13.95.173.255 80:31522/TCP 15m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> Det kan ta et par minutter for Azure å tildele tjenesten vår en Public IP, i mellomtiden vil det stå &lt;code>&amp;lt;pending&amp;gt;&lt;/code> under EXTERNAL-IP.&lt;/p>
&lt;/blockquote>
&lt;p>En enkel &lt;strong>Welcome to nginx&lt;/strong> webside skal nå være tilgjengelig på http://13.95.173.255 (&lt;em>husk å bytt ut med din egen External-IP&lt;/em>).&lt;/p>
&lt;p>&lt;strong>Vi har nå en lastbalansert &lt;code>nginx&lt;/code> tjeneste med 3 servere klar til å ta imot trafikk.&lt;/strong>&lt;/p>
&lt;p>For ordens skyld kan vi slette tjeneste og deployment etterpå:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl delete svc nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete deploy nginx-deployment
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="ScaleCluster">&lt;/a>&lt;/p>
&lt;h3 id="skalere-cluster">Skalere cluster
&lt;/h3>&lt;p>Hvis en ønsker å endre antall vertsmaskiner/noder som kjører Pods kan en gjøre det via Azure-CLI:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks scale --name mitt_cluster --resource-group min_aks_rg --node-count 5
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>For øyeblikket blir alle noder opprettet med samme størrelse som når clusteret ble opprettet. AKS vil antageligvis få støtte for &lt;a class="link" href="https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools" target="_blank" rel="noopener"
>&lt;strong>node-pools&lt;/strong>&lt;/a> i løpet av neste år. Da kan en opprette grupper av noder med forskjellig størrelse og operativsystem, både Linux og Windows.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="DeleteCluster">&lt;/a>&lt;/p>
&lt;h3 id="slette-cluster">Slette cluster
&lt;/h3>&lt;p>En kan slette hele clusteret slik:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks delete --name mitt_cluster --resource-group min_aks_rg --yes
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="Bonusmateriale">&lt;/a>&lt;/p>
&lt;h2 id="bonusmateriale">Bonusmateriale
&lt;/h2>&lt;p>Her er litt bonusmateriale dersom du ønsker å gå enda litt videre med Kubernetes.&lt;/p>
&lt;p>&lt;a id="HelmIntro">&lt;/a>&lt;/p>
&lt;h3 id="rulle-ut-tjenester-med-helm">Rulle ut tjenester med Helm
&lt;/h3>&lt;p>&lt;a class="link" href="https://helm.sh/" target="_blank" rel="noopener"
>Helm&lt;/a> er en pakke-behandler og et bibliotek av programvare som er klart for å rulles ut i et Kubernetes-cluster.&lt;/p>
&lt;p>Start med å laste ned &lt;a class="link" href="https://github.com/kubernetes/helm/releases" target="_blank" rel="noopener"
>Helm-klienten&lt;/a>. Den henter påloggingsinformasjon osv fra samme sted som &lt;code>kubectl&lt;/code> automatisk.&lt;/p>
&lt;p>Installer Helm-serveren (Tiller) på Kubernetes clusteret og oppdater pakke-biblioteket:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">helm init
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm repo update
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Se tilgjengelige pakker (&lt;strong>Charts&lt;/strong>) med: &lt;code>helm search&lt;/code>.&lt;/p>
&lt;p>&lt;a id="HelmMinecraft">&lt;/a>&lt;/p>
&lt;h4 id="rulle-ut-minecraft-med-helm">Rulle ut MineCraft med Helm
&lt;/h4>&lt;p>La oss rulle ut en MineCraft installasjon på clusteret vårt, fordi vi kan :-)&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">helm install --name stian-sin --set minecraftServer.eula=true stable/minecraft
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;code>--set&lt;/code> overstyrer en eller flere av standardverdiene som er satt i pakken. MineCraft pakken er laget slik at den ikke starter uten å ha sagt seg enig i brukervilkårene i variabelen &lt;code>minecraftServer.eula&lt;/code>. Alle variablene som kan overstyres i MineCraft pakken er &lt;a class="link" href="https://github.com/kubernetes/charts/blob/master/stable/minecraft/values.yaml" target="_blank" rel="noopener"
>dokumentert her&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>Så venter vi litt på at Azure skal tildele en Public IP:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get svc -w
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">stian-sin-minecraft 10.0.237.0 13.95.172.192 25565:30356/TCP 3m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Og vipps kan vi kople til Minecraft på &lt;code>13.95.172.192:25565&lt;/code>.&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/minecraft-k8s.png"
width="856"
height="507"
srcset="https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/minecraft-k8s_hu13197832490752396686.png 480w, https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/minecraft-k8s_hu16352153299487520650.png 1024w"
loading="lazy"
alt="Kubernetes in MineCraft on Kubernetes"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="405px"
>&lt;/p>
&lt;p>&lt;a id="KubernetesDashboard">&lt;/a>&lt;/p>
&lt;h3 id="kubernetes-dashboard">Kubernetes Dashboard
&lt;/h3>&lt;p>Kubernetes har også et grafisk web-grensesnitt som gjør det litt lettere å se hvilke ressurser som er i clusteret, se logger og åpne remote-shell inne i en kjørende Pod, blant annet.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl proxy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Starting to serve on 127.0.0.1:8001
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>kubectl&lt;/code> krypterer og tunnelerer trafikken inn til Kubernetes&amp;rsquo; API servere. Dashboardet er tilgjengelig på &lt;a class="link" href="http://127.0.0.1:8001/ui/" target="_blank" rel="noopener"
>http://127.0.0.1:8001/ui/&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/k8s-dash.png"
width="899"
height="588"
srcset="https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/k8s-dash_hu15091082541702636396.png 480w, https://blog.stian.omg.lol/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/k8s-dash_hu17318776918264020386.png 1024w"
loading="lazy"
alt="Kubernetes Dashboard"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>&lt;/p>
&lt;p>&lt;a id="Konklusjon">&lt;/a>&lt;/p>
&lt;h2 id="konklusjon">Konklusjon
&lt;/h2>&lt;p>Jeg håper du har fått mersmak for Kubernetes. Lærekurven kan være litt bratt i begynnelsen men det tar ikke så veldig lang tid før du er produktiv.&lt;/p>
&lt;p>Se på de &lt;a class="link" href="https://v1-8.docs.kubernetes.io/docs/tutorials/" target="_blank" rel="noopener"
>offisielle guidene på Kubernetes.io&lt;/a> for å lære mer om hvordan du definerer forskjellige typer ressurser og tjenester for å kjøre på Kubernetes. &lt;strong>PS: Det gjøres store endringer fra versjon til versjon så sørg for å bruke dokumentasjonen for riktig versjon!&lt;/strong>&lt;/p>
&lt;p>Kubernetes har også et veldig aktivt Slack-miljø på &lt;a class="link" href="http://slack.k8s.io/" target="_blank" rel="noopener"
>kubernetes.slack.com&lt;/a>. Der er det også en kanal for norske Kubernetes brukere; &lt;strong>#norw-users&lt;/strong>.&lt;/p></description></item></channel></rss>