[{"content":"Kubernetes Sidecar Config Drift Huge thanks to one of my favorite clients, Signicat, and especially Jon, for allowing me to share some of the nitty gritty details of a challenge that I believe is probably quite widespread, yet under-appreciated, in modern Kubernetes cloud environments.\nLast week, working on Signicat’s next generation cloud platform, I discovered that several individuals invented their own ways of mitigating what I now call Sidecar Configuration Drift.\nTo ease the pain I created k8s-sidecar-rollout to restart the required workloads and thereby updating their sidecar configurations.\nThis blog post is a bit of background information on what the causes of this problem is.\nWhat is Sidecar Config Drift? When using a Sidecar Injector (such as Istio), there is nothing that ensures that an update (potentially breaking) to a sidecar config template is applied/updated on Pods that have already been injected with a sidecar.\nThis means that after updating a sidecar config it may take a very long time until all Pods have the updated config. They may receive the updates at any undetermined time in the future. While the updates are pending things might not work as expected and things may not be compliant. When the update is finally applied to the Pod it may surface breaking changes.\nI call this phenomena Sidecar Configuration Drift.\nBackground Kubernetes - Declarative vs imperative What makes Kubernetes so powerful is also what can make it hard and confusing to work with until your mindset has shifted.\nThat is the philosophy of being declarative instead of imperative like most of us are used to for the past 20 years.\nDeclarative means you tell Kubernetes HOW you want things to look. You DON’T tell Kubernetes WHAT to do.\nFor example you do not tell Kubernetes to scale your Deployment to 5 instances. You tell Kubernetes that your Deployment should have 5 instances. The difference is subtle but extremely important. In a highly dynamic cloud environment your instances might disappear or crash for a multitude of reasons. In this declarative mindset you should not care about that since Kubernetes is tasked with ensuring 5 instances and will (try to) provision new ones when it’s needed.\nThis is the apparent magic which makes Kubernetes.\nThis magic is technically solved with what we call Controllers. A controller is responsible for constantly comparing the Actual state and Desired state. To scale your Deployment to 5 instances you set replicas: 5 on the Deployment resource. If needed a controller will create a completely new ReplicaSet resource with 5 instances. Another controller will then create 5 Pods. And the scheduler will finally try to place and start those Pods on actual nodes. The ReplicaSet controller will scale down the old once the new has reached it’s desired state.\nThis constant process is called a reconciliation loop and it’s a critical feature.\nSidecars Sidecar is a Kubernetes design pattern. A sidecar is simply a container that is living side-by-side with the main application container that can do tasks that are logically not part of the application. Examples of this can be log handling, monitoring agents, proxies. All containers in a Pod (including sidecars) share the same filesystem, kernel namespace, IP addresses etc.\nMore about sidecars: The Distributed System ToolKit: Patterns for Composite Containers\nSidecar injection Sidecar injection is when a sidecar container is added to a Pod even if the sidecar isn’t defined in any of the higher level primitives, such as Deployment or StatefulSet.\nWhen for example the ReplicaSet controller will Create a new Pod. If configured, the Kubernetes API will call one or more MutatingWebhooks. These webhooks can then change the Pod definition before they are saved (and picked up by the scheduler).\nThe Problem The problem is when updating a sidecar injection template there is no system that runs a reconciliation loop.\nThe webhook just updates the Pod template. It does not keep track of which Pods have gotten which template or check if any template change would result in a different sidecar configuration.\nThe controllers ALSO does not continuously monitor if and how re-creating the same Pod (without sidecars) would result in a different Pod once the sidecars have been injected.\nIn effect sidecar injection does not follow the expected declarative pattern that the rest of Kubernetes does.\nConsequences If a platform team changes the istio sidecar template it will not actually take effect on a Pod until that Pod for some reason is re-created.\nLet’s assume the istio-proxy sidecar template have been updated by the platform team. We roll it out and test it and it seems to work. But the change will break some applications running in the cluster.\nThat breakage will go un-noticed until:\nThe product team commits changes that triggers a re-deploy. The deployment will suddenly fail but it might not have anything to do with the actual changes the team did to the application. This is surely confusing! The platform team for example upgrades a pool of worker nodes causing all `Pods` to be re-created on new nodes. A Pod is re-created when a Kubernetes worker node crashes. In this scenario it appears the failure spawned into existence out of nowhere since neither the product team nor platform team actually “did” anything to trigger it. Also worth noting is that any attempts at Rolling back a Deployment now containing failing Pods will not actually fix anything.\nIt’s the sidecar templates that needs to be rolled back and Pods probably need to be re-created again.\nMitigations We can mitigate drift by:\nRe-starting all Pods in the cluster whenever we update sidecar injection templates. Sometimes we might forget to re-start so regularly re-start all Pods in the cluster anyway. k8s-sidecar-rollout To make these restarts easy and fast I’ve created https://github.com/StianOvrevage/k8s-sidecar-rollout .\nIt’s a tool that figures out (with your help) which workloads (Deployment, StatefulSet, DaemonSet) that needs to be rolled out again (re-started) and then rolls out for you. Head over to the GitHub repo for installation and complete usage instructions. Here is an example of how it can be used:\npython3 sidecar-rollout.py \\ --sidecar-container-name=istio-proxy \\ --include-daemonset=true \\ --annotation-prefix=myCompany \\ --parallel-rollouts 10 \\ --only-started-before=\u0026quot;2022-05-01 13:00\u0026quot; \\ --exclude-namespace=kube-system \\ --confirm=true This will gather all Pods with a container named istio-sidecar belonging to a Deployment or DaemonSet that was started before 2022-05-01 13:00 (which may be when we updated the istio sidecar config template) excluding the kube-system namespace. It will patch the workloads with two annotations with myCompany prefix and run 10 rollouts in parallel.\nThe script that now re-starts Pods adds two annotations indicating that a restart to update sidecars has occurred as well as the time:\n$ kubectl get pods -n product-team some-api-7cdc65482b-ged13 -o yaml | yq '.metadata.annotations' sidecarRollout.rollout.timestamp: 2022-05-03T18:05:31 sidecarRollout.rollout.reason: Update sidecars istio-proxy The idea is that if your Pods are suddenly failing, you can quickly check the annotations and see if it has anything to do with sidecar updates or not.\nThese annotations will of course disappear again when a Deployment is updated.\n","date":"2022-05-03T00:00:00Z","permalink":"https://demo.stack.jimmycai.com/p/kubernetes-sidecar-config-drift/","title":"Kubernetes Sidecar Config Drift"},{"content":"Yak shaving - Photo drips for my mom Update: Check out https://github.com/StianOvrevage/photo-drips for ugly but working code.\nUpdate May 2022: My mom told me this week that I must NEVER stop sending these daily photos \u0026lt;3\nBackground TL;DR: I finally organized my photo archive and in an evening created a service to e-mail my mom a photo from the last 20 years every morning.\nThis started out a few weeks ago when I decided to reinstall Windows on my laptop.\nThe first hurdle was that my 2-3-4 different cloud storage subscriptions were all overdue for a clean-up. The one best suited had me throttled to 1Mbit/s since I was storing 20TB+ of data.\nI’ve been taking a lot of photos and videos since I got my first digital camera 22 years ago. Even though I use Lightroom to keep some order there was some duplication and discontinuity. So (after upgrading my fibreoptic internet to 1Gbit, optimizing my home network and cleaning up space on my machine) I started to clean up and organize the various catalogues.\nLooking at memories from 20 years ago made me realize I’m better at taking photos than “utilizing” them afterwards. What really is the point, then? I took a picture of one on the screen with my phone and sent on snapchat to my mom, not thinking much about it. But she was really thrilled, which made me really happy as well.\nFor my current client I’m nearing the end of my contract and for the last two months I’ve mainly been maintaining, documenting and handing over and I really miss building things and solving problems.\nRecently a potential client asked about Python and AWS Lambda competence. It’s not something I work with daily and it’s not on my CV. But it made me think about all the various languages and tools I’ve used during the last decade.\nMy subconscious brain offers up an idea on how to a) build something b) brush up some Python and Lambda knowledge and c) brighten my moms day.\nConcept Every morning a photo from my archives is e-mailed to my mom, a photo drip.\nThere is also a gallery where she can look at the previous photo drips.\nProcess and goals The primary objective was to have a working prototype as quickly as possible. So no premature optimization, refactoring or anything.\nPhoto selection and preparation At around 3pm I started browsing photos from year 2000. It took me about 75 minutes to pick out about 300 photos from the first 20.000.\nTip: When browsing in Lightroom, press B to add to Quick Collection.\nExport the pictures in the quick collection with a custom filename format like this 0003-2000-05-14.jpg.\nThis format ensures filenames are ordered from oldest to newest, and the date can later be extracted directly from the filename without doing any EXIF stuff.\nPhotos resized to 1500x1500. Never enlarge. Sharpen for screen.\nPhoto storage It would have been quicker to set up a AWS EC2 virtual machine running all the components but that would be too easy.\nThe requirements for storage is: cheap, reliable, publicly available. So a standard AWS S3 bucket should do just fine.\nUsing the browser I upload all the photos in batch to a new bucket. In a sub-folder with a random name. That should provide an appropriate level of security and avoid strangers on the internet stumbling upon it. Since the bucket is publicly available.\nI verify that I can load the pictures in my browser with the public S3 URL. It took a few attempts at getting the permissions right, and I suspect adding this policy was required even though everything in the settings was set to “Full public access”.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;PublicRead\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:GetObject\u0026quot;, \u0026quot;s3:GetObjectVersion\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:s3:::memorydrops/*\u0026quot; ] } ] } Sending the e-mail Sending e-mails directly with SMTP is really not an option anymore because of all the spam blocking systems. I know we need to use a third party service or something.\nI first looked at AWS SES (simple-email-service). But nothing about it seemed simple. Then looked at a few of the stablished ones such as SendGrid and Mailgun. These tools have evolved a lot and now has a plethora of features aimed at marketing, transactional e-mails etc. Probably overkill and potentially time consuming while having no transferable knowledge or code when hitting a dead-end since they are all different APIs.\nWhat about just using my own Gmail SMTP to ship?\nThe first tutorials about that required turning on “Less secure app access” for my account.\nNot accepting that trade-off, I found https://levelup.gitconnected.com/an-alternative-way-to-send-emails-in-python-5630a7efbe84 where I learned that I can generate an “App password”, similar to an API Key for specific Google applications without disabling other security features. Jackpot!\nThe article also has working code that I shamelessly used as a starting point.\nFirst I attached the photo of the day, but I really want it embedded. Even though I don’t really like linking images on a public URL (S3 bucket) because of potential browser and client issues that’s what I ended up with. The alternative of base64 encoding the data inline seemed like a chore. Besides I know mom uses Gmail in Chrome anyway so if it worked for me it would probably work for her.\nI don’t want the system to be dependent on any state management, databases, etc.\nPicking the right photo each day is simply select file N, where N is the days since the first deployment.\nIdeally I would use AWS S3 API to list the contents of the bucket to get the available files. But to save some time the “photo index” is simply a text file listing the files:\nls \u0026gt; filelist.txt I create a new Lambda function and paste the python script and filelist.txt directly in the Lambda code editor and deploy and test it. Working!\nThe Lambda function is the configured with a EventBridge trigger with the schedule cron(0 5 * * ? *) that should trigger the function at 0500 UTC every day.\nGallery I also want to have a gallery where she can view the previous memory drops without having to shuffle through endless emails.\nStarted out by looking at VueJS for the frontend, which I have used before. But I have not used the new version yet and I suspected it might take a long time getting a project set up from scratch since unfortunately tutorials, tips, documentation in the frontend / JavaScript world has a tendency to be chronically outdated and unreliable.\nDropped that and opted for a very simple native JS photo gallery called zoomwall.js.\nOnly showing a selection of photos depending on the day however requires a bit more engineering. (Writing this I realize another acceptable approach would be to use native JS to manipulate the DOM directly.)\nI implemented this by inlining the CSS and JS into one template HTML file that is rendered on-demand in a Python Lambda function. Then using AWS API Gateway to expose it as a normal webserver.\nI wanted to use Jinja2 for templating instead of the built-in Python templating functions. Doing that causes some headache since the Lambda environment does not have Jinja2 installed.\nLuckily creating a custom deployment package (.zip) including dependencies is rather trivial.\nConclusion All in all this project was started with filtering photos at 3pm, having dinner from 5pm to 6pm and by 8.30pm everything was deployed. The following morning I had the memory drop in my inbox to great delight. An unexpected benefit of using my own Gmail for sending the e-mail is that mom can reply directly.\nExpected costs\nS3 offers 5GB of standard storage for free for 12 months. After that I expect the cost to be in the $0.x range per month. API Gateway offers 1 Million API calls per month for free for 12 months. After that I expect the cost to be in the $0.x range per month. Lambda offers 1 Million requests per month forever. Performance has been sacrificed for the gallery to make it simple. Server-side rendering is not going to be as fast as client-side. Python is not the fastest alternative. Using a FaaS such as Lambda also introduces penalties and unknowns (cold starts, etc). Yet the gallery HTML loads in ~200ms and and seems instant.\nImprovements After deciding to share the code I spent an hour or two writing this document as well as some necessary clean-up and changes from the prototype.\nThe filelist.txt is not bundled with the code. It’s hosted in the S3 bucket along with the photos. That means I can update and add photos later without touching code.\nThat requires the requests package, so now both modules are packaged with dependencies and uploaded via AWS CLI instead of browser.\nSome hard-coded URLs etc have been converted to environment variables.\nAdded the exif Python package to extract the original time and date of the photo to include in the e-mail.\nAdded an URL redirect from a prettier domain to the auto generated API Gateway hostname of the gallery. Did not bother with proper custom domain since that requires a lot of fiddling with SSL certificates.\nBugs and future improvements The JS gallery seems slightly broken. Might replace with a better one. The e-mail HTML is not pretty. Make sender and recipient e-mails environment variables (but recipient is currently a Python list, so). Make start date env var. Requires parsing date from user. The usual: Check that required env vars are set on startup. Improve error handling and logging. ","date":"2022-02-13T00:00:00Z","permalink":"https://demo.stack.jimmycai.com/p/yak-shaving-photo-drips-for-my-mom/","title":"Yak shaving - Photo drips for my mom"},{"content":"Quite often people ask me what I actually do. I have a hard time giving a short answer. Even to colleagues and friends in the industry.\nHere I will try to show and tell how I spent an evening digging around in a system I helped build for a client.\nTable of contents\nBackground The (initial) problem Fixing the (initial) problem Verifying the (initial) fix Baseline simple request - HTTP1 1 connections, 20000 requests Baseline complex request - HTTP1 1 connections, 20000 requests Verifying the fix for assumed workload Complex request - HTTP1 6 connections, 500 requests Complex request - HTTP2 500 \u0026ldquo;connections\u0026rdquo;, 500 requests Side quest: Database optimizations Determining the next bottleneck Side quest: Cluster resources and burstable VMs Conclusion Background I\u0026rsquo;m a consultant doing development, DevOps and cloud infrastructure.\nFor this specific client I mainly develop APIs using Golang to support new products and features as well as various exporting, importing and processing of data in the background.\nI\u0026rsquo;m also the \u0026ldquo;ops\u0026rdquo; guy handling everything in AWS, setting up and maintaing databases, making sure the \u0026ldquo;DevOps\u0026rdquo; works and the frontend and analytics people can do their work with little friction. 99% of the time things work just fine. No data is lost. The systems very rarely have unforeseen downtime and the users can access the data they want with acceptable latency rarely exceeding 500ms.\nA couple of times a year I assess the status of the architecture and set up new environments from scratch and update any documentation that has drifted. This is also a good time to do changes and add or remove constraints in anticipation of future business needs.\nIn short, the current tech stack that has evolved over a couple of years is:\nEverything hosted on Amazon Web Services (AWS). AWS managed Elastic Kubernetes Service (EKS) currently on K8s 1.18. GitHub Actions for building Docker images for frontends, backends and other systems. AWS Elastic Container Registry for storing Docker images. Deployment of each system defined as a Helm chart alongside source code. Actual environment configuration (Helm values) stored in repo along source code. Updated by GitHub Actions. ArgoCD in cluster to manage status of all environments and deployments. Development environments usually automatically deployed on change. Push a button to deploy to Production. Prometheus for storing metrics from the cluster and nodes itself as well as custom metrics for our own systems. Loki for storing logs. Makes it easier to retrieve logs from past Pods and aggregate across multiple Pods. Elastic APM server for tracing. Pyroscope for live CPU profiling/tracing of Go applications. Betteruptime.com for tracking uptime and hosting status pages. I might write up a longer post about the details if anyone is interested.\nThe (initial) problem A week ago I upgraded our API from version 1, that was deployed in January, to version 2 with new features and better architecture.\nOne of the endpoints of the API returns an analysis of an object we track. I have previously reduced the amount of database queries by 90% but it still requires about 50 database calls from three different databases. Getting and analyzing the data usually completes in about 3-400 milliseconds returning an 11.000 line JSON.\nIt\u0026rsquo;s also possible to just call /objects/analysis to get the analysis for all the 500 objects we are tracking. It takes 20 seconds but is meant for exports to other processes and not interactive use, so not a problem.\nSince the product is under very active development the frontend guys just download the whole analysis for an object to show certain relevant information to users. It\u0026rsquo;s too early to decide on which information is needed more often and how to optimize for that. Not a problem.\nSo we need an overview of some fields from multiple objects in a dashboard / list. We can easily pull analysis from 20 objects without any noticable delay.\nBut what if we just want to show more, 50? 200? 500? The frontend already have the IDs for all the objects and fetches them from /objects/id/analysis. So they loop over the IDs and fire of requests simultaneously.\nAnalyzing the network waterfall in Chrome DevTools indicated that the requests now took 20-30 seconds to complete! But looking closer most of the time they were actually queued up in the browser. This is because Chrome only allows 6 concurrent TCP connection to the same origin when using HTTP1 (https://developers.google.com/web/tools/chrome-devtools/network/understanding-resource-timing).\nFixing the (initial) problem HTTP2 should fix this problem easily. By default HTTP2 is disabled in nginx-ingress. I add a couple of lines enabling it and update the Helm deployment of the ingress controller.\nVerifying the (initial) fix Some common development tools doesn\u0026rsquo;t support HTTP2, such as Postman. So I found h2load which can both help me verify HTTP2 is working and I also get to measure the improvement, nice!\nNote that I\u0026rsquo;m not using the analysis endpoint since I want to measure the change from HTTP1 to HTTP2 and it will become apparent later that there are other bottlenecks preventing us from a linear performance increase when just changing from HTTP1 to HTTP2.\nAlso note that this is somewhat naive since it requests the same URL over and over which can give false results due to any caching. But fortunately we don\u0026rsquo;t do any caching yet.\nBaseline simple request - HTTP1 1 connections, 20000 requests Using 1 concurrent streams, 1 client and HTTP1 I get an estimate of performance pre-http2:\nh2load --h1 --requests=20000 --clients=1 --max-concurrent-streams=1 https://api.x.com/api/v1/objects/1 The results are as expected:\nfinished in 1138.99s, 17.56 req/s, 18.41KB/s requests: 20000 total, 20000 started, 20000 done, 19995 succeeded, 5 failed, 0 errored, 0 timeout For http2 we set max concurrent streams to the same as number of requests:\nh2load --requests=200 --clients=1 --max-concurrent-streams=200 https://api.x.com/api/v1/objects/1 Which results in almost half the latency:\nfinished in 1.23s, 162.65 req/s, 158.06KB/s requests: 200 total, 200 started, 200 done, 200 succeeded, 0 failed, 0 errored, 0 timeout So HTTP2 is working and providing significant latency improvements. Success!\nBaseline complex request - HTTP1 1 connections, 20000 requests We start by establishing a baseline with 1 connection querying over and over.\nh2load --h1 --requests=20000 --clients=1 --max-concurrent-streams=1 Verifying the fix for assumed workload So we verified that HTTP2 gives us a performance boost. But what happens when we fire away 500 requests to the much heavier /analysis endpoint?\nThese graphs are not as pretty since the ones above. This is mainly due to the sampling interval of the metrics and that we need several datapoints to accurately determine the rate() of a counter.\nComplex request - HTTP1 6 connections, 500 requests finished in 32.25s, 14.88 req/s, 2.29MB/s requests: 500 total, 500 started, 500 done, 500 succeeded, 0 failed, 0 errored, 0 timeout In summary it so far seems to scale linearly with load. Most of the time is spent fetching data from the database. Still very predictable low latency on database queries and the resulting HTTP response.\nComplex request - HTTP2 500 \u0026ldquo;connections\u0026rdquo;, 500 requests So now we unleash the beast. Firing all 500 requests at the same time.\nfinished in 16.66s, 30.02 req/s, 3.55MB/s requests: 500 total, 500 started, 500 done, 500 succeeded, 0 failed, 0 errored, 0 timeout Important about Kubernetes and CPU limits\nEven with CPU limits set to 1 (100% of one CPU), your container can still be throttled at much lower CPU usage. Check out this article for more information.\nIn an ideal world all 500 requests should start and complete in 2-300ms regardless. Since that is not happening it\u0026rsquo;s an indication that we are now hitting some other bottleneck.\nLooking at the graphs it seems we are starting to saturate the database. The latency for every request is now largely dependent on the slowest of the 10-12 database queries it depends on. And as we are stressing the database the probability of slow queries increase. The latency for the whole process of fetching 500 requests are again largely dependent on the slowest requests.\nSo this optimization gives on average better performance, but more variability of the individual requests, when the system is under heavy load.\nSide quest: Database optimizations It seems we are saturating the database. Before throwing more money at the problem (by increasing database size) I like to know what the bottlenecks are. Looking at the traces from APM I see one query that is consistently taking 10x longer than the rest. I also confirm this in the AWS RDS Performance Insights that show the top SQL queries by load.\nWhen designing the database schema I came up with the idea of having immutability for certain data types. So instead of overwriting row with ID 1, we add a row with ID 1 Revision 2. Now we have the history of who did what to the data and can easily track changes and roll back if needed. The most common use case is just fetching the last revision. So for simplicity I created a PostgreSQL view that only shows the last revision. That way clients don\u0026rsquo;t have to worry about the existense of revisions at all. That is now just an implementation detail.\nWhen it comes to performance that turns out to be an important implementation detail. The view is using SELECT DISTINCT ON (id) ... ORDER BY id, revision DESC. However many of the queries to the view is ordering the returned data by time, and expect the data returned from database to already be ordered chronologically. Using EXPLAIN ANALYZE on the queries this always results in a full table scan instead of using indexes, and is what\u0026rsquo;s causing this specific query to be slow. Without going into details it seems there is no simple and efficient way of having a view with the last revision and query that for a subset of rows ordered again by time.\nFor the forseable future this does not actually impact real world usage. It\u0026rsquo;s only apparent under artificially large loads under the worst conditions. But now we know where we need to refactor things if performance actually becomes a problem.\nDetermining the next bottleneck Whenever I fix one problem I like to know where, how and when the next problem or limit is likely to appear. When increasing the number of requests and streams I expected to see increasing latency. But instead I see errors appear like a cliff:\nfinished in 27.33s, 36.59 req/s, 5.64MB/s requests: 5000 total, 1002 started, 1002 done, 998 succeeded, 4002 failed, 4000 errored, 0 timeout Consulting the logs for both the nginx load balancer and the API there are no records of failing requests. Since nginx does not pass the HTTP2 connection directly to the API, but instead \u0026ldquo;unbundles\u0026rdquo; them into HTTP1 requests I suspect there might be issues with connection limits or even available ports from nginx to the API. But maybe it\u0026rsquo;s a configuration issue. By default nginx does not limit the number of connections to a backend (our API). . But, there is actually a default limit to the number of HTTP2 requests that can be served over a single connection - And it happens to be 1000.\nI leave it at that. It\u0026rsquo;s very unlikely we\u0026rsquo;ll be hitting these limits any time soon.\nSide quest: Cluster resources and burstable VMs When load testing the first time around sometimes Grafana would also become unresponsive. That\u0026rsquo;s usually a bad sign. It might indicate that the underlying infrastructure is also reaching saturation. That is not good since it can impact what should be independent services.\nOur Kubernetes cluster is composed of 2x t3a.medium on demand nodes and 2x t3a.medium spot nodes. These VM types are burstable. You can use 20% per vCPU sustained over time without problems. If you exceed those 20% you start consuming CPU credits faster than they are granted and once you run out of CPU credits processes will be forcibly throttled.\nOf course Kubernetes does not know about this and expects 1 CPU to actually be 1 CPU. In addition Kubernetes will decide where to place workloads based on their stated resource requirements and limits, and not their actual resource usage.\nWhen looking at the actual metrics two of our nodes are indeed out of CPU credits and being throttled. The sum of factors leading to this is:\nWe have not yet set resource requests and limits making it harder for Kubernetes to intelligently place workloads Using burstable nodes having some additional constraints not visible to Kubernetes Old deployments laying around consuming unnecessary resources Adding costly features without assessing the overall impact I have not touched on the last point yet. I started adding Pyroscope to our systems since I simply love monitoring All The Things. The documentation does not go into specifics but emphasizes that it\u0026rsquo;s \u0026ldquo;low overhead\u0026rdquo;. Remember that our budget for CPU usage is actually 40% per node, not 200%. The Pyroscope server itself consumes 10-15% CPU which seems fair. But investigating further the Pyroscope agent also consumes 5-6% CPU per instance. This graph shows the CPU usage of a single Pod before and after turning off Pyroscope profiling.\n5-6% CPU overhead on a highly utilized service is probably worth it. But when the baseline CPU usage is 0% CPU and we have multiple services and deployments in different environments we are suddenly using 40-60% CPU on profiling and less than 1% on actual work!\nThe outcome of this is that we need to separate burstable and stable load deployments. Monitoring and supporting systems are usually more stable resource wise while the actual business systems much more variable, and suitable for burst nodes. In practice we add a node pool of non-burst VMs and use NodeAffinity to stick Prometheus, Pyroscope etc to those nodes. Another benefit of this is that the supporting systems needed to troubleshoot problems are now less likely to be impacted by the problem itself, making troubleshooting much easier.\nConclusion This whole adventure only took a few hours but resulted in some specific and immediate performance gains. It also highlighted the weakest links in our application, database and infrastructure architecture.\n","date":"2021-03-06T00:00:00Z","permalink":"https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/","title":"A side quest in API development, observability, Kubernetes and cloud with a hint of database"},{"content":"There seems to exist a database for every niche, mood or emotion. And they seem to change just as fast.\nHow do you balance the urge for the new and shiny but without risking too much headache down the road?\nThis post is an attempt to lay out the rough landscape of databases that you might encounter or consider as of late 2020.\nThere will be broad generalizations for brevity.\nThe goal is not to be exhaustive or take all possible precautions. Consider it a starting point for further research and planning.\nTLDR: Scroll to the diagrams or view the big picture.\nTable of contents\nBackground Project phase overview Planning Database categories SQL NoSQL KeyValue Timeseries Graph Other nice things The Landscape SQL NoSQL KeyValue Timeseries Graph Further reading Conclusion Background I\u0026rsquo;m a consultant doing development, DevOps and cloud infrastructure. I also have the occasional side project trying out the Tech Flavor of the Month.\nProject phase overview The typical phases in projects I\u0026rsquo;m involved in follow no scientific or trademarked methodology, so YMMV:\nStarting out Get something working as fast as possible. Take all the shortcuts. Use some opinionated framework or platform.\nMoving from development to production People like it, people use it. Move the thing from a single \u0026ldquo;pet server\u0026rdquo; to a more robust cloud environment.\nScaling production Bottlenecks and scaling problems start to emerge. Refactor or replace some pieces to remove the bottlenecks.\nChallenges Moving between these phases might be a major PITA if the wrong shortcuts were taken in the previous phases.\nThis of course applies to all technology choices and not just databases. But we have to start somewhere, right?\nPlanning When starting out I try to envision all the phases of the project and which directions it may take in the future.\nFirst I want the technology or software I choose to be instantly usable. A Docker image. Great. An apt-get install. Sweet. npm install. Sure, why not. Downloading a tarball. Installing some C dependencies. Setting some flags. Compiling. Symlinking and fixing permissions. Creating some configuration from scratch. Making my own systemd service definitions. Going back and doing every step again because it failed. Mkay, no thanks, I\u0026rsquo;m out.\nAt least for me it\u0026rsquo;s a plus if it\u0026rsquo;s easy to deploy on Kubernetes since I use it for everything already. I always have a cluster or three laying around so I can get a prototype or five up and running quickly before later spending money for cloud hosting.\nDoes the thing have momentum and a community? If it does it probably has high quality tooling either by the vendor or the open source community (preferably both). It probably also has lots of common questions answered on blogs and StackOverflow and Github issues.\nSo we managed to build something and the audience likes it.\nHow easy is it to move it from a production environment into something stable and low-maintenance? For databases that would typically involve using a managed service for hosting it. You do not want to be responsible for operating your own databases. Is it common enough that there are competitors in the marketplace offering it as a managed service? If there is only a single option expect prices to be very steep. Preferably also a managed service by one of the big known cloud platforms. They are usually cheaper. They are less likely to vanish. It might make integration with other systems easier later.\nWe hit some problems either because of raw scale or some type of usage we did not anticipate in the beginning.\nAre there compatible implementations that might solve some common problems? Typically this is because an implementation has to make a decision about it\u0026rsquo;s trade-offs. For a database system this is usually around the CAP theorem. A database system (or anything that keeps state) can be:\nPartition Tolerant - The system still works if a node or the network between nodes fail. Available - All requests receive a response. Consistent - The data we read is the current data and not an earlier state. But, you can only have two at the same time. And distributed systems tends to need to be partition tolerant. So we are stuck between consistency and availability.\nIt might be a good to have an idea of the CAP tradeoffs an implementation has done, and whether there are compatible implementations with different tradeoffs that can be used if later we find out we need to tweak our trade-offs for speed and/or scale.\nMore information about CAP theorem here and here. Jepsen have also extensively tested many popular databases to see how they break and if they are true to their stated trade-offs.\nDatabase categories Databases can be roughly sorted into categories. I\u0026rsquo;ll keep it simple and use the everyday lingo and not go into details about semantics and definitions (forgive me).\nhttps://www.prisma.io/dataguide/intro/comparing-database-types\nSQL The oldest category is the relational database, also known as SQL based on the typical interface used to access these databases.\nIn general these databases have tables with names, a set of pre-defined columns and an arbitrary number of rows. You should have an idea of the data types to be stored in each column (such as text or numbers).\nThe downside of this is that you have to start with a rough model of the data you want to store and work with. The benefit of this is that later you know something about the model of the data you are working with. Most of the time I\u0026rsquo;ll happily do this in the database rather than handle all the potential inconsistencies in all systems that use that database.\nMain contenders: PostgreSQL. MySQL \u0026amp; MariaDB.\nNoSQL All the rage the last decade. You put data in you get data out. The data is structured but not necessarily predefined. Think JSON object with values, arrays and lists.\nThe benefit is productivity when developing. The drawback is that you may pay a price for those shortcuts later if you\u0026rsquo;re not careful.\nMain contender: MongoDB.\nKeyValue Technically a sub-category of NoSQL, and should probably be called caches. But I feel it deserves it\u0026rsquo;s own category.\nA hyper-fast hyper-simple type of database. It has two columns. A key (ID) and value. The value can be anything, a string, a number, an entire JSON object or a blob containing binary data.\nThese are typically used in combination with another type of database. Either by storing very commonly used data for even quicker access. Or for certain types of simple data that requires insane speed or throughput and you don\u0026rsquo;t want to overload the main database.\nMain contender: Redis.\nTimeseries A lesser known type of database optimized for storing a time series. A time series is a specific data type where the index is typically the time of a measurement. And the measurement is a number.\nA time series is almost never changed after the fact. So these databases can be optimized for writing huge amounts of new data and reading and calculating on existing data. At the cost of performance for deleting or updating old data which is sloooow. Since the values are always numbers that tend to change somewhat predictably compression and deduplication can save us massive amounts of storage.\nMain contenders: Prometheus, InfluxDB, TimescaleDB (plugin for PostgreSQL).\nGraph Graph databases are cool. In a graph database the relationship between objects is a primary feature. Whereas in SQL you need to join an element from one table with another object in another table with some kind of common identifier.\nFor most simple use cases a regular SQL database will do fine. But when the number of objects stored (rows) and the number of intermediary tables (joins) become large it gets slow, or expensive, or both.\nI don\u0026rsquo;t have much experience with graph databases but I suspect they are less suited to general tasks and should be reserved for solving specific problems.\nMain contenders: Neo4j. Redis + RedisGraph.\nPS: Graph databases and GraphQL are completely separate things.\nOther nice things When researching this post I\u0026rsquo;ve come across things that look promising but are hard to categorize or fall in their own very niche categories.\nDgraph - A GraphQL and backend in one. PrestoDB - An SQL interface on top of whatever database or storage you want to connect. RethinkDB - A NoSQL database focused on real-time streaming/updating clients. FoundationDB - A transactional key-value store by Apple. ClickHouse - An SQL database that stores data (on disk) in columns instead of rows. Makes for blazingly fast analytical and aggregation queries. Amazon Quantum Ledger Database - A managed distributed ledger database (aka blockchain). EDB Postgres Advanced Server - An Oracle compatible PostgreSQL variant. The Landscape How to use these maps:\nVersion compatibility are in parenthesis. I have not mapped every version and how much breaking they are compared to previous versions but included some notes where I know there might be issues.\nAPI/Protocol/Interface - This is decided by the framework, tool or driver you want to use. Sometimes it might be easier to choose the framework first and then a fitting database protocol. Or you might be lucky to choose the database features you need first and then select frameworks, tools and drivers that support it.\nI think interfaces are really important when creating and choosing technology. I had a presentation about it a while ago and I think it\u0026rsquo;s still relevant.\nEngine - Database implementations that are independent but try to be compatible. If there are alternatives to the \u0026ldquo;original\u0026rdquo; implementation they might have done different tradeoffs with regards to the CAP theorem or solve other specific problems.\nBig three managed - Available managed services by the big three clouds, Amazon (AWS), Google (GCP) or Microsoft (Azure). Having an option to host in the big three is most likely the cheapest method as well as having a variety of other managed services to build a complete system in a single cloud.\nVendor managed - If the database vendor or backing company offers an Official managed service. They are usually hosted on the big three. Potentially a large cost premium over the raw compute power.\nSelf-hosted - Implementations you can run on your own computer or server.\nLegend The checklist icon marks potential compatibility issues. For most use cases not a problem.\nPS: The absence of this icon does not automatically mean compatibility. I put the lightning icon on the self-hosted implementations that have what seems to be stable Kubernetes operators available. In short, a Kubernetes operator makes running a stateful system, such as a database, on Kubernetes much easier. It might allow for longer time before migrating to a managed service. SQL Compatibility:\nPostgreSQL - Yugabyte PostgreSQL - CockroachDB MySQL - MariaDB Kubernetes Operators:\nPostgreSQL (CrunchyData) PostgreSQL (Zalando) Yugabyte CockroachDB Percona PostgreSQL for MySQL \u0026amp; XtraDB NoSQL PS: There are some breaking changes from MongoDB 3.6 to 4 so make sure the tools you intend to use are compatible with the database version you intend on using.\nKubernetes Operators:\nMongoDB Percona Distribution for MongoDB ScyllaDB Elastic Stack KeyValue Kubernetes Operators:\nRedis (Spotahome) Timeseries Kubernetes Operators:\nPrometheus-Stack VictoriaMetrics Graph Kubernetes Operators:\nArangoDB Further reading Wikipedia on RDBMS DB-engines.com - Lots of statistics and comparisons between DB engines CNCF Landscape - What\u0026rsquo;s moving in the cloud native landscape, including databases. Conclusion Congratulations if you made it this far!\nI did this research primarily to reduce my own analysis paralysis on various projects so I can get-back-to-building. If you learned something as well, great stuff!\nAnd if you want my advice, just use PostgreSQL unless you really know about some special requirements that necessitates using something else :-)\n","date":"2020-11-27T00:00:00Z","image":"https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-nosql_hu4481999146992913726.png","permalink":"https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/","title":"End of 2020 rough database landscape"},{"content":"We discovered today that some implicit assumptions we had about AKS at smaller scales were incorrect.\nSuddenly new workloads and jobs in our Radix CI/CD could not start due to insufficient resources (CPU \u0026amp; memory).\nEven though it only caused problems in development environments with smaller node sizes it still surprised some of our developers, since we expected the size of development clusters to have enough resources.\nI thought it would be a good chance to go a bit deeper and verify some of our assumptions and also learn more about various components that usually \u0026ldquo;just works\u0026rdquo; and isn\u0026rsquo;t really given much thought.\nFirst I do a kubectl describe node \u0026lt;node\u0026gt; on 2-3 of the nodes to get an idea of how things are looking:\n1 2 3 4 Resource Requests Limits -------- -------- ------ cpu 930m (98%) 5500m (585%) memory 1659939584 (89%) 4250M (228%) So we are obviously hitting the roof when it comes to resources. But why?\nNode overhead We use Standard DS1 v2 instances as AKS nodes and they have 1 CPU core and 3.5 GiB memory.\nThe output of kubectl describe node also gives us info on the Capacity (total node size) and Allocatable (resources available to run Pods).\n1 2 3 4 5 6 Capacity: cpu: 1 memory: 3500452Ki Allocatable: cpu: 940m memory: 1814948Ki So we have lost 60 millicores / 6% of CPU and 1685Mi‬B / 48% of memory. The next question is if this increases linearly with node size (the percentage of resources lost is the same regardless of node size) or is fixed (always reserves 60 millicores and 1685Mi of memory), or a combination.\nI connect to another cluster that has double the node size (Standard DS2 v2) and compare:\n1 2 3 4 5 6 Capacity: cpu: 2 memory: 7113160Ki Allocatable: cpu: 1931m memory: 4667848Ki So for this the loss is 69 millicores / 3.5% of CPU and 2445MiB / 35% of memory.\nSo CPU reservations are close to fixed regardless of node size while memory reservations are influenced by node size but luckily not linearly.\nWhat causes this \u0026ldquo;waste\u0026rdquo;? Reading up on kubernetes.io gives a few clues. Kubelet will reserve CPU and memory resources for itself and other Kubernetes processes. It will also reserve a portion of memory to act as a buffer whenever a Pod is going beyond it\u0026rsquo;s memory limits to avoid risking System OOM, potentially making the whole node unstable.\nTo figure out what these are configured to we log in to an actual AKS node\u0026rsquo;s console and run ps ax|grep kube and the output looks like this:\n1 /usr/local/bin/kubelet --enable-server --node-labels=node-role.kubernetes.io/agent=,kubernetes.io/role=agent,agentpool=nodepool1,storageprofile=managed,storagetier=Premium_LRS,kubernetes.azure.com/cluster=MC_clusters_weekly-22_northeurope --v=2 --volume-plugin-dir=/etc/kubernetes/volumeplugins --address=0.0.0.0 --allow-privileged=true --anonymous-auth=false --authorization-mode=Webhook --azure-container-registry-config=/etc/kubernetes/azure.json --cgroups-per-qos=true --client-ca-file=/etc/kubernetes/certs/ca.crt --cloud-config=/etc/kubernetes/azure.json --cloud-provider=azure --cluster-dns=10.2.0.10 --cluster-domain=cluster.local --enforce-node-allocatable=pods --event-qps=0 --eviction-hard=memory.available\u0026lt;750Mi,nodefs.available\u0026lt;10%,nodefs.inodesFree\u0026lt;5% --feature-gates=PodPriority=true,RotateKubeletServerCertificate=true --image-gc-high-threshold=85 --image-gc-low-threshold=80 --image-pull-progress-deadline=30m --keep-terminated-pod-volumes=false --kube-reserved=cpu=60m,memory=896Mi --kubeconfig=/var/lib/kubelet/kubeconfig --max-pods=110 --network-plugin=cni --node-status-update-frequency=10s --non-masquerade-cidr=0.0.0.0/0 --pod-infra-container-image=k8s.gcr.io/pause-amd64:3.1 --pod-manifest-path=/etc/kubernetes/manifests --pod-max-pids=-1 --rotate-certificates=false --streaming-connection-idle-timeout=5m To log in to the console of a node, go to the MC_resourcegroup_clustername_region resource-group and select the VM. Then go to Boot diagnostics and enable it. Go to Reset password to create yourself a user and then Serial console to log in and execute commands.\nWe can see --kube-reserved=cpu=60m,memory=896Mi and --eviction-hard=memory.available\u0026lt;750Mi which adds up to 1646Mi which is pretty close to the 1685Mi that was the gap between Capacity and Allocatable.\nWe also do this on a Standard DS2 v2 node and get --kube-reserved=cpu=69m,memory=1638Mi and --eviction-hard=memory.available\u0026lt;750Mi.\nSo we can see that the memory of kube-reserved grows almost linearly and seems to always be about 20-25% while CPU reservations are almost the same. The memory eviction buffer is always fixed at 750Mi which would mean bigger resource waste as nodes decrease in size.\nCPU Standard DS1 v2 Standard DS2 v2 VM capacity 1.000m 2.000m kube-reserved -60m -69m Allocatable 940m 1.931m Allocatable % 94% 96.5% Memory Standard DS1 v2 Standard DS2 v2 VM capacity 3.500Mi 7.113Mi kube-reserved -896Mi -1.638Mi Eviction buf -750Mi -750Mi Allocatable 1.814Mi 4.667Mi Allocatable % 52% 65% Node pods (DaemonSets) We have some Pods that run on every node, and they are installed by default by AKS. We get the resource limits of these by describing either the pods or the daemonsets.\nCPU Standard DS1 v2 Standard DS2 v2 Allocatable 940m 1.931m kube-system/calico-node -250m -250m kube-system/kube-proxy -100m -100m kube-system/kube-svc-redirect -5m -5m Available 585m 1.576m Available % 58% 81% Memory Standard DS1 v2 Standard DS2 v2 Allocatable 1.814Mi 4.667Mi kube-system/kube-svc-redirect -32Mi -32Mi Available 1.782Mi 4.635Mi Available % 50% 61% So for Standard DS1 v2 nodes we have about 0.5 CPU and 1.7GiB memory per node for pods. And for Standard DS2 v2 nodes it\u0026rsquo;s about 1.5 CPU and 4.6GiB memory.\nkube-system pods Now lets add some standard Kubernetes pods we need to run. As far as I know these are pretty much fixed for a cluster and not related to node size or count.\nDeployment CPU Memory kube-system/kubernetes-dashboard 100m 50Mi kube-system/tunnelfront 10m 64Mi kube-system/coredns (x2) 200m 140Mi kube-system/coredns-autoscaler 20m 10Mi kube-system/heapster 130m 230Mi Sum 460m 494Mi Third party pods Deployment CPU Memory grafana 200m 500Mi prometheus-operator 500m 1.000Mi prometheus-alertmanager 100m 225Mi flux 50m 64Mi flux-helm-operator 50m 64Mi Sum 900m 1.853Mi Radix platform pods Deployment CPU Memory radix-api-prod/server (x2) 200m 400Mi radix-api-qa/server (x2) 100m 200Mi radix-canary-golang-dev/www 40m 500Mi radix-canary-golang-prod/www 40m 500Mi radix-platform-prod/public-site 5m 10Mi radix-web-console-prod/web 10m 42Mi radix-web-console-qa/web 5m 21Mi radix-github-webhook-prod/webhook 10m 30Mi radix-github-webhook-prod/webhook 5m 15Mi Sum 415m 1.718Mi If we add up the resource usage of these groups of workloads and see the total available resources on our 4 node Standard DS1 v2 clusters we are left with 0.56 CPU cores (14%) and 3GB of memory (22%):\nWorkload CPU Memory kube-system 460m 494Mi third-party 900m 1.853Mi radix-platform 415m 1.718Mi Sum 1.760m 4.020Mi Available on 4x DS1 2.340m 7.128Mi Available for workloads 565m 3.063Mi Though surprising that we lost this much resources before being able to deploy our actual customer applications, it should still be a bit of headroom.\nGoing further I checked the resource requests on 8 customer pods deployed in 4 environments (namespaces). Even though none of them had a resource configuration in their radixconfig.yaml files they still had resource requests and limits. Not surprising since we use LimitRange to set default resource requests and limits. The surprise was that half of them had 50Mi of memory and the other half 500Mi, seemingly at random.\nIt turns out that we did an update to the LimitRange values a few days ago but that only applies to new Pods, so depending on if the Pods got re-created for any reason they may or may not have the old request of 500Mi, which in our case of small clusters will quickly drain the available resources.\nRead more about LimitRange here: kubernetes.io , and here is the commit that eventually trickled down to reduce memory usage: github.com\nPod scheduling Depending on the weight between CPU and memory requests and how often things get destroyed and re-created you may find yourself in a situation where you have enough resources in your cluster but new workloads are still Pending. This can happen when one resource type (e.g. CPU) is filled before another (e.g. memory), leading one or more resources to be stranded and unlikely to be utilized.\nImagine for example a cluster that is already utilized like this:\nCPU Memory node0 94% 86% node1 80% 89% node2 98% 60% Scheduling a workload that requests 15% CPU and 20% memory cannot be scheduled since there are no nodes fulfilling both requirements. In theory there is probably a CPU intensive Pod on node2 that could be moved to node1 but Kubernetes does not do re-scheduling to optimize utilization. It can do re-scheduling based on Pod priority (medium.com) and there is an incubator project (akomljen.com) that can try to drain nodes with low utilization.\nSo for the foreseable future keeping in mind that resources can get stranded and that looking at the sum of cluster resources and sum of cluster resource demand might be misleading.\ncalico-node The biggest source of waste on our small clusters is calico-node which is installed on every node and requests 25% of a CPU core while only using 2.5-3% CPU:\nThe request is originally set here github.com but I have not got into why that number was choosen. Next steps would be to do some benchmarking of calico-node to smoke out it\u0026rsquo;s performance characteristics to see if it would be safe to lower the resource requests, but that is out of scope for now.\nConclusion By increasing node size from Standard DS1 v2 to Standard DS2 v2 we also increase the available CPU from 58% per node to 81% per node. Available memory increases from 50% to 61% per node. With a total platform requirement of 3-4GB of memory and 4.6GB available on Standard DS2 v2 we might have more resources for actual workloads on a 1-node Standard DS2 v2 cluster than a 3-node Standard DS1 v2 cluster! Beware of stranded resources limiting the utilization you can achieve across a cluster. ","date":"2019-06-04T00:00:00Z","image":"https://demo.stack.jimmycai.com/p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu_hu18202809688185937996.png","permalink":"https://demo.stack.jimmycai.com/p/mini-post-down-scaling-azure-kubernetes-service-aks/","title":"Mini-post: Down-scaling Azure Kubernetes Service (AKS)"},{"content":"Understanding the characteristics of disk performance of a platform might be more important than you think. If disk resources are not correctly matched to your workload, your performance will suffer and might lead you to incorrectly diagnose a problem as being related to CPU or memory.\nThe defaults might also not give you the performance you expect.\nIn this first post on troubleshooting some disk performance issues on Azure Kubernetes Service (AKS) we will benchmark Azure Premium SSD to find how workloads affect performance and which metrics to monitor to know when troubleshooting potential disk issues.\nTLDR:\nDisable Azure cache for workloads with high number of random writes Use a P15 (256GB) or larger Premium SSD even though you might only need a fraction of it. Table of contents\nBackground Metric Methodologies Storage Background What to measure? How to measure disk How to measure disk on Azure Kubernetes Service Test results Test 1 - Learning to dislike Azure Cache Test 2 - Disable Azure Cache - enable OS cache Test 3 - Disable OS cache Test 4 - Increase IO depth Test 5 - Larger block size, smaller IO depth Test 6 - Enable OS cache Test 7 - Random writes, small block size Test 8 - Large block size Conclusion Microsoft Azure If you don\u0026rsquo;t have a Azure subscription already you can try services for $200 for 30 days. The VM size Standard_B2s is Burstable, has 2vCPU, 4GB RAM, 8GB temp storage and costs roughly $38 / month. For $200 you can have a cluster of 3-4 B2s nodes plus traffic, loadbalancers and other additional costs.\nSee my blog post Managed Kubernetes on Microsoft Azure (English) for information on how to get up and running with Kubernetes on Azure.\nI have no affiliation with Microsoft Azure except using them through work.\nCorrections February 2020: Some of my previous knowledge and assumptions were not correct when applied to a cloud + Docker environment, as explained by AKS PM Jesse Noller on GitHub.\nOne of the issues is that even accessing a \u0026ldquo;data disk\u0026rdquo; will incur IOPS on the OS disk, and throttling of the OS disk will also constraint IOPS on the data disks.\nBackground I\u0026rsquo;m part of a team at Equinor building an internal PaaS based on Kubernetes running on AKS (Azure managed Kubernetes). We use Prometheus for monitoring each cluster as well as InfluxDB for collecting metrics from k6io which runs continous tests on our public endpoints.\nA couple of weeks ago we discovered some potential problems with both Prometheus and InfluxDB with memory usage and restarts. High CPU usage of type iowait suggested that there might be some disk issues contributing to the problems.\niowait: \u0026ldquo;Percentage of time that the CPU or CPUs were idle during which the system had an outstanding disk I/O request.\u0026rdquo; (hpe.com). You can see iowait on your Linux system by running top and looking at the wa percentage.\nPS: You can have a disk IO bottleneck even with low iowait, and a high iowait does not always indicate a disk IO bottleneck (ibm.com).\nFirst off we need to benchmark the underlying disk to get an understanding of it\u0026rsquo;s performance limits and characteristics. That is what we will cover in this post.\nMetric Methodologies There are two helpful methodologies when monitoring information systems. The first one is Utilization, Saturation and Errors (USE) from Brendan Gregg and the second one is Rate, Errors, Duration (RED) from Tom Wilkie. RED is best suited when observing workloads and transactions while USE is best suited for observing resources.\nI\u0026rsquo;ll be using the USE method here. USE can be summarised as:\nFor every resource, check utilization, saturation, and errors. resource: all physical server functional components (CPUs, disks, busses, \u0026hellip;) utilization: the average time that the resource was busy servicing work saturation: the degree to which the resource has extra work which it can\u0026rsquo;t service, often queued errors: the count of error events Storage Background Disk usage has two dimensions, throughput/bandwidth(BW) and operations per second (IOPS), and the underlying storage system will have upper limits of how much data it can receive (BW) and the number of operations it can perform per second (IOPS).\nBackground - harddrive types: harddrives come in two types, Solid State Disks (SSD) and spindle (HDD). A SSD disk is a microship capable of permanently storing data while a HDD uses spinning platters to store data. HDDs have a fixed rate of rotation (RPM), typically 5.400 and 7.200 RPM for lower cost drives for home use and higher cost 10.000 and 15.000 RPM drives for server use. Over the last 20 years of HDDs their storage density has increased, but the RPM has largely stayed the same. A disk with twice the density (500GB to 1TB for example) can read twice as much data on a single rotation and thus increase the bandwidth significantly. However, reading or writing a random block still requires waiting for the disk to spin enough to reach the relevant sector on the disk. So IOPS has not increased much for HDDs and is still a low 125-150 IOPS for a 10.000 RPM enterprise disk. A SSD does not have any moving parts so is able to reach MUCH higher IOPS. A low end Samsung 960 EVO with 500GB capacity costs $150 and can achieve a whopping 330.000 IOPS! (wikipedia.com)\nBackground - access patterns: The way a program uses storage also has a huge impact on the performance one can achieve. Sequential access is when we read or write a large file. When this happens the operating system and harddrive can optimize and \u0026ldquo;merge\u0026rdquo; operations so that we can read or write a much bigger chunk of data at a time. If we can read 1MB at a time 150 times per second we get 150MB/s of bandwidth. However, fully random access where the smallest chunk we read or write is a 4KB block the same 150 IOPS would only give a bandwidth of 0.6MB/s!\nBackground - cloud vs physical: Now we know what HDDs are limited to a low IOPS and low IOPS combined with a random access pattern gives us a low overall bandwidth. There is a huge gotcha here when it comes to cloud. On Azure when using Premium Managed SSD the IOPS you are given is a factor of the disk size you provision (microsoft.com). A 512GB disk is limited to 2.300 IOPS and 150MB/s. With 100% random access that only gives about 9MB/s of bandwidth!\nBackground - OS caching: To overcome some of the limitations of the underlying disk (mostly IOPS) there are potentially several layers of caching involved. Linux file systems can have writeback enabled which causes Linux to temporarily store data that is going to be written to disk in memory. This can give a big performance increase when there are sudden spikes of writes exceeding the performance of the underlying disk. It also increases the chance that operations can be merged where several write operations to areas of the disk that are nearby can be executed as one. This caching works best for sudden peaks and will not necessarily be enough if there is continous random writes to disk. This caching also means that even though an application thinks it has saved some data to disk it can be lost in the case of a power outage or other failure. Applications can also explicitly request direct access where every operation is persisted to disk before receiving a confirmation. This is a trade-off between performance and durability that needs to be decided based on the application itself and the environment.\nBackground - Azure caching: Azure also provides read and write cache for its disks which is enabled by default. As we will see soon for our use case it\u0026rsquo;s not a good idea to use.\nWhat to measure? These metrics are collected by the Prometheus node-exporter and follows it\u0026rsquo;s naming. I\u0026rsquo;ve also created a dashboard that is available on Grafana.com.\nWith the USE methodology as a guideline and the two separate but related \u0026ldquo;resources\u0026rdquo;, bandwidth and IOPS we can look for some useful metrics.\nUtilization:\nrate(node_disk_written_bytes_total) - Write bandwidth. The maximum is given by Azure and is 25MB/s for our disk size. rate(node_disk_writes_completed_total) - Write operations. The maximum is given by Azure and is 120 IOPS for our disk size. rate(node_disk_io_time_seconds_total) - Disk active time in percent. The time the disk was busy servicing requests. 100% means fully utilized. Saturation:\nrate(node_cpu_seconds_total{mode=\u0026quot;iowait\u0026quot;} - CPU iowait. The percentage of time a CPU core is blocked from doing useful work because it\u0026rsquo;s waiting for an IO operation to complete (typically disk, but can also be network). Useful calculated metrics:\nrate(node_disk_write_time_seconds_total) / rate(node_disk_writes_completed_total) - Write latency. How long from a write is requested until it\u0026rsquo;s completed. rate(node_disk_written_bytes_total) / rate(node_disk_writes_completed_total) - Write size. How big the average write operation is. 4KB is minimum and indicates 100% random access while 512KB is maximum and indicates sequential access. How to measure disk The best tool for measuring disk performance is fio, even though it might seem a bit intimidating at first due to it\u0026rsquo;s insane number of options.\nInstalling fio on Ubuntu:\napt-get install fio\rfio executes jobs described in a file. Here is the top of our jobs file:\n[global]\rioengine=libaio # sync|libaio|mmap\rgroup_reporting\rthread\rsize=10g # Size of test file\rcpus_allowed=1 # Only use this CPU core\rruntime=300s # Run test for 5 minutes\r[test1]\rfilename=/tmp/fio-test-file\rdirect=1 # If value is true, use non-buffered I/O. Non-buffered I/O usually means O_DIRECT\rreadwrite=write # read|write|randread|randwrite|readwrite|randrw\riodepth=1 # How many operations to queue to the disk\rblocksize=4k\rThe fields we will be changing for the various tests are direct, readwrite, iodepth and blocksize. Save the contents in a file named jobs.fio and we run a test with fio --sector test1 jobs.fio and wait until the test completes.\nPS: To run these tests on higher performance hardware and better caching you might want to set runtime to 0 to have the test run continously and monitor the metrics until performance reaches a steady-state.\nHow to measure disk on Azure Kubernetes Service For this testing we use a standard Prometheus installation collecting data from node-exporter and visualizing data in Grafana. The dashboard I created for the testing can be found here: https://grafana.com/dashboards/9852.\nBy default Kubernetes will schedule a Pod to any node that has enough memory and CPU for our workload. Since one of the tests we are going to run are on the OS disk we do not want the Pod to run on the same node as any other disk-intensive application, such as Prometheus.\nLook at which Pods are running with kubectl get pods -o wide and look for a node that does not have any disk-intensive application.\nThen we tag that node with kubectl label nodes aks-nodepool1-37707184-2 tag=disktest. This allows us later to specify that we want to run our testing Pod on that specific node.\nA StorageClass in Kubernetes is a specification of a underlying disk that Pods can request usage of through volumeClaimTemplates. AKS comes with a default StorageClass managed-premium that has caching enabled. Most of these tests require the Azure cache disabled so create a new StorageClass managed-premium-retain-nocache:\nkind: StorageClass\rapiVersion: storage.k8s.io/v1\rmetadata:\rname: managed-premium-retain-nocache\rprovisioner: kubernetes.io/azure-disk\rreclaimPolicy: Retain\rparameters:\rstorageaccounttype: Premium_LRS\rkind: Managed\rcachingmode: None\rYou can add it to your cluster with:\nkubectl apply -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2019-02-23-disk-performance-on-aks-part-1/storageclass.yaml\rNext we create a StatefulSet that uses a volumeClaimTemplate to request a 250GB Azure disk. This provisions a P15 Azure Premium SSD with 125MB/s bandwidth and 1100 IOPS:\nkubectl apply -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2019-02-23-disk-performance-on-aks-part-1/ubuntu-statefulset.yaml\rFollow the progress of the Pod creation with kubectl get pods -w and wait until it is Running.\nWhen the Pod is Running we can start a shell on it with kubectl exec -it disk-test-0 bash\nOnce inside bash on the Pod, we install fio:\napt-get update \u0026amp;\u0026amp; apt-get install -y fio wget\rAnd save the contents of in the Pod:\nwget https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2019-02-23-disk-performance-on-aks-part-1/jobs.fio\rNow we can run the different test sections one by one. PS: If you don\u0026rsquo;t specify a section fio will run all the tests simultaneously, which is not what we want.\nfio --section=test1 jobs.fio\rfio --section=test2 jobs.fio\rfio --section=test3 jobs.fio\rfio --section=test4 jobs.fio\rfio --section=test5 jobs.fio\rfio --section=test6 jobs.fio\rfio --section=test7 jobs.fio\rfio --section=test8 jobs.fio\rfio --section=test9 jobs.fio\rTest results Test 1 - Learning to dislike Azure Cache Sequential write, 4K block size, Azure Cache enabled, OS cache disabled. See full fio test results.\nI run the first tests on the OS disk of a Kubernetes node. The OS disks have Azure caching enabled.\nThe first 1-2 minutes of the test I get very good performance of 45MB/s and ~11.500 IOPS but that drops to 0 very quickly as the cache is full and busy writing things to the underlying disk. When that happens everything freezes and I cannot even execute shell commands. After stopping the test the system still hangs for a bit while the cache empties.\nThe maximum latency measured by fio was 108751k usec. Or about 108 seconds!\nFor the first try of these tests a 20-30 second period of very fast writes (250MB/s) caused a 7-8 minutes hang while the cache emptied. Trying again caused another pattern of lower peak performance with shorter hangs in between. Very unpredictable. I\u0026rsquo;m not sure what to make of this. It\u0026rsquo;s not acceptable that a Kubernetes node becomes unresponsive for many minutes following a short burst of writing. There are scattered recommendations online of disabling caching for write-heavy applications. Since I have not found any way to measure the Azure cache itself, the results are unpredictable and potentially very impactful as well as making it very hard to use the metrics we do have to evaluate application and storage behaviour I\u0026rsquo;ve concluded that it\u0026rsquo;s best to use data disks with caching disabled for our workloads (you cannot disable caching on an AKS node OS disk).\nTest 2 - Disable Azure Cache - enable OS cache Sequential write, 4K block size. Change: Azure cache disabled, OS caching enabled. See full fio test results.\nIf we swap the Azure cache for the Linux OS cache we see that iowait increases while the writing occurs. The application sees high write performance until the number of Dirty bytes reaches a threshold of about 3.7GB of memory. The performance of the underlying disk is 125MB/s and 250 IOPS. Here we are throttled by the 125MB/s limit of the Azure P15 Premium SSD.\nAlso notice that on sequential writes of 4K with OS caching the actual blocks written to disk is 512K which saves us a lot of IOPS. This will become important later.\nTest 3 - Disable OS cache Sequential write, 4K block size. Change: OS caching disabled. See full fio test results.\nBy disabling the OS cache (direct=1) the results are consistent and predictable. There is no iowait since the application does not have multiple writes pending at the same time. Because of the 2-3ms latency of the disks we are not able to get more than about 400 IOPS. This gives us a meager 1.5MB/s even though the disk is limited to 1100 IOPS and 125MB/s. To reach that we need multiple simultaneous writes or a bigger IO depth (queue). Disk active time is also 0% which indicates that the disk is not saturated.\nTest 4 - Increase IO depth Sequential write, 4K block size, OS caching disabled. Change: IO depth 16. See full fio test results.\nFor this test we only increase the IO depth from 1 to 16. IO depth is the number of write operations fio will execute simultaneously. Since we are using direct these will be queued by the OS for writing. We are now able to hit the performance limit of 1100 IOPS. Disk active time is now steady at 100% indicating that we have saturated the disk.\nTest 5 - Larger block size, smaller IO depth Sequential write, OS caching disabled. Change: 128K block size, IO depth 1. See full fio test results.\nWe increase the block size to 128KB and reduce the IO depth to 1 again. The write latency for larger blocks increase to ~5ms which gives us 200 IOPS and 28MB/s. The disk is not saturated.\nTest 6 - Enable OS cache Sequential write, 256K block size, IO depth 1. Change: OS caching enabled. See full fio test results.\nWe have now enabled the OS cache/buffer (direct=0). We can see that the writes hitting the disk are now merged to 512KB blocks. We are hitting the 125MB/s limit with about 250 IOPS. Enabling the cache also has other effects: CPU suddenly shows significant IO wait. The write latency shoots through the roof. Also note that the writing continued for 30-40 seconds after the test was done. This also means that the bandwidth and IOPS that fio sees and reports is higher than what is actually hitting the disk.\nTest 7 - Random writes, small block size IO depth 1, OS caching enabled. Change: Random write, 4K block size. See full fio test results.\nHere we go from sequential writes to random writes. We are limited by IOPS. The average size of the blocks actually written to disks, and the IOPS required to hit the bandwidth limit is actually varying a bit throughout the test. The time taken to empty the cache is about as long as I ran the test (4-5 minutes).\nTest 8 - Large block size Random write, OS caching enabled. Change: 256K block size, IO depth 16. See full fio test results.\nIncreasing the block size to 256K makes us bandwidth limited to 125MB/s.\nConclusion Access patterns and block sizes have a tremendous impact on the amount of data we are able to write to disk.\n","date":"2019-02-23T00:00:00Z","image":"https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1_hu4302020548573248974.png","permalink":"https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/","title":"Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking"},{"content":"A few days ago I wrote a walkthrough of setting up Azure Container Service (AKS) in Norwegian. Someone asked me for an English version of that, and here it is.\nKubernetes(K8s) is becoming the de-facto standard for deploying container-based applications and workloads. Microsoft is currently in preview of their managed Kubernetes offering (Azure Kubernetes Service, AKS) which makes it easy to create a Kubernetes cluster and deploy workloads without the skill and time required to manage day-to-day operations of a Kubernetes-cluster, which today can be complex and time consuming.\nIn this post we will set up a Kubernetes cluster from scratch using Azure CLI.\nTable of contents\nBackground Docker containers Container orchestration Getting started with Azure Kubernetes - AKS Caveats Preparations Azure login Activate ContainerService Create a resource group Create a Kubernetes cluster Install kubectl Inspect cluster Start some nginx containere Making nginx available with a service Scale cluster Delete cluster Bonus material Deploying services with Helm Deploy MineCraft with Helm Kubernetes Dashboard Conclusion Microsoft Azure If you don\u0026rsquo;t have a Azure subscription already you can try services for $200 for 30 days. The VM size Standard_B2s is Burstable, has 2vCPU, 4GB RAM, 8GB temp storage and costs roughly $38 / month. For $200 you can have a cluster of 3-4 B2s nodes plus traffic, loadbalancers and other additional costs.\nWe have no affiliation with Microsoft Azure except their sponsorship of our startup DataDynamics with cloud services for 24 months in their BizSpark program.\nBackground Docker containers We will not do a deep dive on Docker containers in this post, but here is a summary for those who are not familiar with it.\nDocker is a way to package software so that it can run on the most popular platforms without worrying about installation, dependencies and to a certain degree, configuration.\nIn addition, a Docker container uses the operating system of the host machine when it runs. Because of this it\u0026rsquo;s possible to run many more containers on the same host machine compared to running virtual machines.\nHere is a incomplete and rough comparison between a Docker container and a virtual machine:\nVirtual machine Docker container Image size from 200MB to many GB from 10MB to 3-400MB Startup time 60 seconds + 1-10 seconds Memory usage 256MB-512MB-1GB + 2MB + Security Good isolation between VMs Not as good isolation between containers Building image Minutes Seconds PS The numbers for virtual machines is taken from memory. I tried starting a MySQL virtual appliance on my laptop but VMware Player refuses to run because of Windows Hyper-V incompatibility. VMware Workstation refuses to run because of license issues and Oracle VirtualBox repeatedly gives me a nasty bluescreen. Hooray!\nProtip The smallest and fastest Docker images are built on Alpine Linux. For the webserver Nginx the Alpine-based image is 15MB compared to 108MB for the normal Debian-based image. PostgreSQL:Alpine is 38MB compared to 287MB with \u0026ldquo;full\u0026rdquo; OS. Last version of MySQL is 343MB but will in version 8 support Alpine Linux as well.\nTo recap, some of the advantages of Docker containers are:\nCompatibility across platforms, Linux, Windows, MacOS. 10-100x smaller size. Faster to download, build and upload. Memory usage only for application and not base OS. Advantage when developing. Ability to run 10-20-30 containers on a development laptop. Advantage in production. Can reduce hardware/cloud costs considerably. Near instant startup. Makes dynamic scaling of applications easier. Download Docker for Windows here.\nTo start a MySQL database container from Windows CMD or Powershell:\n1 docker run --name mysql -p 3306:3306 -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql Stop the container with:\n1 docker kill mysql You can search for already built Docker images on Docker Hub. It\u0026rsquo;s also possible to create private Docker repositories for your own software that you don\u0026rsquo;t want to be publicly available.\nContainer orchestration Now that Docker container images has become the preferred way to package and distribute software on the Linux platform, there has emerged a need for systems to coordinate running and deploying these containers. Similar to the ecosystem of products VMware has built up around development and operation of virtual machines.\nContainer orchestration systems have the responsibility for:\nLoad balancing. Service discovery. Health checks. Automatic scaling and restarting of host nodes and containers. Zero downtime upgrades (rolling deploys). Until recently the ecosystem around container orchestration has been fragmented, and the most popular alternatives have been:\nKubernetes (Originaly from Google, now managed by CNCF, the Cloud Native Computing Foundation) Swarm (From the maker of Docker) Mesos (From Apache Software Foundation) Fleet (From CoreOS) But the last year there has been a convergence towards Kubernetes as the preferred solution.\n7 February CoreOS announces that they are removing Fleet from Container Linux and recommends Kubernetes 27 July Microsoft joins the CNCF 9 August Amazon Web Services join the CNCF 29 August VMware and Pivotal joins the CNCF 17 September Oracle joins the CNCF 17 October Docker announces native support for Kubernetes in addition to it\u0026rsquo;s own Swarm product 24 October Microsoft Azure announces the managed Kubernetes service AKS 29 November Amazon Web Services announces the managed Kubernetes service EKS Especially the last two news items are important. Deploying and running your own Kubernetes-installation requires time and skills (Read how Stripe used 5 months to trust running Kubernetes in production, just for batch jobs.)\nUntil now the choice has been running your own Kubernetes cluster or using Google Container Engine which has been using Kubernetes since 2014. Many of us feel a certain discomfort by locking ourselves to one provider. But this is now changing when you can develop infrastructure on Kubernetes and choose between the 3 large cloud providers in addition to running your own cluster if wanted. *\n* Kubernetes is a fast moving project, and features might be available on the different platforms on different timelines.\nGetting started with Azure Kubernetes - AKS Caveats This guide is based on the documentation on Microsoft.com. Setting up a Azure Kubernetes cluster did not work in the beginning of December, but today, 23. December, it seems to work fairly well. But, upgrading the cluster from Kubernetes 1.7 to 1.8 for example does NOT work.\nAKS is in Preview and Azure are working continuously to make AKS stable and to support as many Kubernetes-features as possible. Amazon Web Services has a similar closed invite-only Preview currently while working on stability and features.\nBoth Azure and AWS expresses expectations about their Kubernetes offerings will be ready for production in 2018.\nPreparations You need Azure-CLI (version 2.0.21 or newer) to execute the az commands:\nDownload Azure-CLI here Information about Azure-CLI on MacOS and Linux here All commands executed in Windows PowerShell.\nAzure login Log on to Azure:\n1 az login You will get a link to open in your browser together with an authentication code. Enter the code on the webpage and az login will save the login information so that you will not have to authenticate again on the same machine.\nPS The login information gets saved in C:\\Users\\Username\\.azure\\. You have to make sure nobody can access these files. They will then have full access to your Azure account.\nActivate ContainerService Since AKS is in Preview/Beta, you explicitly have to activate it in your subscription to get access to the aks subcommands.\n1 2 az provider register -n Microsoft.ContainerService az provider show -n Microsoft.ContainerService Create a resource group Here we create a resource group named \u0026ldquo;my_aks_rg\u0026rdquo; in Azure region West Europe.\n1 az group create --name my_aks_rg --location westeurope Protip To see a list of all available Azure regions, use the command az account list-locations --output table. PS AKS might not be available in all regions yet!\nCreate Kubernetes cluster 1 az aks create --resource-group my_aks_rg --name my_cluster --node-count 3 --generate-ssh-keys --node-vm-size Standard_B2s --node-osdisk-size 128 --kubernetes-version 1.8.2 --node-count Number of agent(host) nodes available to run containers --generate-ssh-keys Creates and prints a SSH key which can be used for SSHing directly to the agent nodes. --node-vm-size Which size Azure VMs the agent nodes should be created as. To see available sizes use az vm list-sizes -l westeurope --output table and Microsofts webpages. --node-osdisk-size Disk size of the agent nodes in GB. PS Containers can be stopped and moved to another host if Kubernetes finds it necessary or if a agent node disappears. All data saved locally in the container will be gone. If saving data permanently use Kubernetes PersistentVolumes and not the local agent node or container disks. --kubernetes-version Which Kubernetes version to install. Azure does NOT necessarily install the last version by default, and currently upgrading with az aks upgrade does not work. Latest version available right now is 1.8.2. It\u0026rsquo;s recommended to use the latest available version since there is a lot of changes from version to version. The documentation is also much better for newer versions. Save the output of the command in a file in a secure location. It contains keys that can be used to connect to the cluster with SSH. Even though that should not in theory be necessary.\nInstall kubectl kubectl is the client which performs all operations against your Kubernetes cluster. Azure CLI can install kubectl for you:\n1 az aks install-cli After kubectl is installed we need to get login information so that kubectl can communicate with the Kubernetes cluster.\n1 az aks get-credentials --resource-group my_aks_rg --name my_cluster The login information is saved in C:\\Users\\Username\\.kube\\config. Keep these files secure as well.\nProtip When you have several Kubernetes clusters you can change which one kubectl talks to with kubectl config get-contexts and kubectl config set-context my_cluster.\nInspect cluster To check that the cluster and kubectl works we start with a couple of commands.\nSee all agent nodes and status:\n1 2 3 4 5 \u0026gt; kubectl get nodes NAME STATUS AGE VERSION aks-nodepool1-16970026-0 Ready 15m v1.8.2 aks-nodepool1-16970026-1 Ready 15m v1.8.2 aks-nodepool1-16970026-2 Ready 15m v1.8.2 See all services, pods and deployments:\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system po/kubernetes-dashboard-6fc8cf9586-frpkn 1/1 Running 0 3d NAMESPACE NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-system svc/kubernetes-dashboard 10.0.161.132 \u0026lt;none\u0026gt; 80/TCP 3d NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-system deploy/kubernetes-dashboard 1 1 1 1 3d NAMESPACE NAME DESIRED CURRENT READY AGE kube-system rs/kubernetes-dashboard-6fc8cf9586 1 1 1 3d This is just some of the output from this command. You do not have to know what the resources in the kube-system namespace does. That is part of the intention when Microsoft is managing our cluster for us.\nNamespaces In Kubernetes there is something called Namespaces. Resources in one namespace does not have automatic access to resources in another namespace. The services that runs Kubernetes itself use the namespace kube-system. The kubectl command by default only shows you resources in the default namespace, unless you specify --all-namespaces or --namespace=xx.\nStart some nginx containers An instance of a running container in Kubernetes is called a Pod.\nnginx is a fast and flexible web server.\nNow that the clsuter is up we can start rolling out services and deployments on it.\nLets start with creating a Deployment consiting of 3 containers all running the nginx:mainline-alpine image from Docker hub.\nnginx-dep.yaml looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1beta2 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:mainline-alpine ports: - containerPort: 80 Load this into the cluster with kubectl create:\n1 kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-dep.yaml This command creates the resources described in the file. kubectl can read files either from your local disk or from a web URL.\nAfter making changes to a resource definition (.yaml file), you can update the resources in the cluster with kubetl replace -f resource.yaml.\nWe can verify that the Deployment is ready:\n1 2 3 \u0026gt; kubectl get deploy NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 10m We can also get the actual Pods that are running:\n1 2 3 4 5 \u0026gt; kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-569477d6d8-dqwx5 1/1 Running 0 10m nginx-deployment-569477d6d8-xwzpw 1/1 Running 0 10m nginx-deployment-569477d6d8-z5tfk 1/1 Running 0 10m Logger We can view logs from one pod with kubectl logs nginx-deployment-569477d6d8-xwzpw. But since we in this case don\u0026rsquo;t know which Pod ends up getting an incomming request we can view logs from all the Pods which have app=nginx label: kubectl logs -lapp=nginx. The use of app=nginx is our choice in nginx-dep.yaml when we configured spec.template.metadata.labels: app: nginx.\nMaking nginx available with a service To send traffic to our new Pods we need to create a Service. A service consists of one or more Pods which are chosen based on different criteria, for example which labels they have and whether the Pods are Running and Ready.\nLets create a service which forwards traffic to all Pods with label app: nginx and are listening to port 80. In addition we make the service available via a LoadBalancer:\nnginx-svc.yaml looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: type: LoadBalancer ports: - port: 80 name: http targetPort: 80 selector: app: nginx We tell Kubernetes to create our service with kubectl create as usual:\n1 kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-svc.yaml We can then wait and see which IP-address Azure assigns our service:\n1 2 3 \u0026gt; kubectl get svc -w NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx 10.0.24.11 13.95.173.255 80:31522/TCP 15m PS It can take a few minutes for Azure to allocate and assign a Public IP for us. In the mean time \u0026lt;pending\u0026gt; will appear under EXTERNAL-IP.\nA simple Welcome to nginx webpage should now be available on http://13.95.173.255 (remember to replace with your own External-IP).\nWe can also delete the service and deployment afterwards:\n1 2 kubectl delete svc nginx kubectl delete deploy nginx-deployment Scaling the cluster If we want to change the number of agent nodes running Pods we can do that via Azure-CLI:\n1 az aks scale --name my_cluster --resource-group my_aks_rg --node-count 5 Currently all nodes will be created with the same size as when we created the cluster. AKS will probably get support for node-pools next year. That will allow for creating different groups of nodes with different size and operating systems, both Linux and Windows.\nDelete cluster You can delete the whole cluster like this:\n1 az aks delete --name my_cluster --resource-group my_aks_rg --yes Bonus material Here is some bonus material if you want to go a bit further with Kubernetes.\nDeploying services with Helm Helm is a package manager and library of software that is ready to be deployed on a Kubernetes cluster.\nStart by downloading the Helm-client. It will read login information etc. from the same location as kubectl automatically.\nInstall the Helm-server (Tiller) on the Kubernetes cluster and update the package library:\n1 2 helm init helm repo update See available packages (Charts) with helm search.\nDeploy MineCraft with Helm Lets deploy a MineCraft server installation on our cluster, just because we can :-)\n1 helm install --name stians --set minecraftServer.eula=true stable/minecraft --set overrides one or more of the standard values configured in the package. The MineCraft package is made in a way where it does not start without accepting the user license agreement by setting the variable minecraftServer.eula. All the variables that can be set in the MineCraft package are documented here.\nThen we wait for Azure to assign us a Public IP:\n1 2 \u0026gt; kubectl get svc -w stians-minecraft 10.0.237.0 13.95.172.192 25565:30356/TCP 3m Now we can connect to our MineCraft server on 13.95.172.192:25565!\nKubernetes Dashboard Kubernetes also has a graphic web user-interface which makes it a bit easier to see which resources are in the cluster, view logs and even open a remote shell inside a running Pod, among other things.\n1 2 \u0026gt; kubectl proxy Starting to serve on 127.0.0.1:8001 kubectl encrypts and tunnels the traffic to the Kubernetes API servers. The dashboard is available on http://127.0.0.1:8001/ui/.\nConclusion I hope you enjoy Kubernetes as much as I have. The learning curve can be a bit steep in the beginning, but it does not take long before you are productive.\nLook at the official guides on Kubernetes.io to learn more about defining different types of resources and services to run on Kubernetes. PS: There are big changes from version to version so make sure you use the documentation for the correct version!\nKubernetes also have a very active Slack-community on kubernetes.slack.com that is worthwhile to check out.\n","date":"2017-12-29T00:00:00Z","image":"https://demo.stack.jimmycai.com/p/managed-kubernetes-on-microsoft-azure-english/minecraft-k8s_hu15082107401327841977.png","permalink":"https://demo.stack.jimmycai.com/p/managed-kubernetes-on-microsoft-azure-english/","title":"Managed Kubernetes on Microsoft Azure (English)"},{"content":"Update 29. Dec: There is an English version of this post here.\nKubernetes (K8s) er i ferd med å bli de-facto standard for deployments av kontainer-baserte applikasjoner. Microsoft har nå preview av deres managed Kubernetes tjeneste (Azure Kubernetes Service, AKS) som gjør det enkelt å opprette et Kubernetes cluster og rulle ut tjenester uten å måtte ha kompetanse og tid til den daglige driften av selve Kubernetes-clusteret, som per i dag kan være relativt komplisert og tidkrevende.\nI denne posten setter vi opp et Kubernetes cluster fra scratch ved bruk av Azure CLI.\nTable of contents\nBakgrunn Docker containers Container orchestration Kom i gang med Azure Kubernetes - AKS Forbehold Forberedelser Azure innlogging Aktiver ContainerService Opprett en resource group Opprette Kubernetes cluster Installer kubectl Inspiser cluster Starte noen nginx containere Gjøre nginx tilgjengelig med en tjeneste Skalere cluster Slette cluster Bonusmateriale Rulle ut tjenester med Helm pakker MineCraft server med Helm Kubernetes Dashboard Konklusjon Microsoft Azure Hvis du ikke har Azure fra før kan du prøve tjenester for $200 i 30 dager. VM typen Standard_B2s er Burstable, har 2vCPU, 4GB RAM, 8GB temp storage og koster ~$38 / mnd. For $200 kan du ha et cluster på 3-4 B2s noder plus trafikkostnad, lastbalanserere og andre nødvendige tjenester.\nVi har ingen tilknytning til Microsoft bortsett fra at de sponser vår startup DataDynamics med cloud-tjenester i 24 mnd i deres BizSpark program.\nBakgrunn Docker containers Vi tar ikke for oss Docker containers i dybden i denne posten, men her er en kort oppsummering for de som ikke er kjent med teknologien.\nDocker er en måte å pakketere programvare slik at det kan kjøres på samtlige populære platformer uten å måtte bruke mye tid på dependencies, oppsett og konfigurasjon.\nI tillegg bruker en Docker container operativsystemet på vertsmaskinen når den kjører. Dette gjør at en kan kjøre mange flere containere på samme vertsmaskin sammenlignet med virtuelle maskiner.\nHer er en ufullstendig og grov sammenligning mellom en Docker container og en virtuell maskin:\nVirtuel maskin Docker container Image størrelse fra 200MB til mange GB fra 10MB til 3-400MB Oppstartstid 60 sekunder + 1-10 sekunder Minnebruk 256MB-512MB-1GB + 2MB + Sikkerhet God isolasjon mellom VM Dårligere isolasjon mellom containere Bygge image Minutter Sekunder PS Tallene for virtuelle maskiner er tatt fra hukommelsen. Jeg forsøkte å starte en MySQL virtuell appliance på min laptop men VMware Player nekter å kjøre pga inkompatibilitet med Windows Hyper-V. VMware Workstation nekter å kjøre pga utgått lisens og Oracle VirtualBox gir en nasty bluescreen gang på gang. Hooray!\nProtip De minste og raskeste Docker imagene er bygget på Alpine Linux. For webserveren Nginx er det Alpine-baserte imaget 15MB mot det Debian-baserte imaget på 108MB. PostgreSQL:Alpine er 38MB mot 287MB. Siste versjon av MySQL er 343MB men vil i versjon 8 støtte Alpine Linux også.\nNoen av fordelene med Docker containers er altså:\nKompatibilitet på tvers av platformer, Linux, Windows og MacOS. 10-100x mindre størrelse. Raskere å laste ned, raskere å bygge, raskere å laste opp. Minnebruk kun for applikasjon og ikke eget OS. Fordel under utvikling, kan kjøre 10-20-30 Docker containere samtidig på en laptop. Fordel i produksjon, kan redusere hardware utgifter betraktelig. Oppstart på få sekunder. Gjør dynamisk skalering av applikasjoner mye enklere. Last ned Docker for Windows her.\nOg start en MySQL database fra Windows CMD eller Powershell:\n1 docker run --name mysql -p 3306:3306 -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql Stop containeren med:\n1 docker kill mysql En kan søke etter ferdige Docker images på Docker Hub. Det er også mulig å lage private Docker repositories for egen programvare som ikke skal være tilgjengelig for omverden.\nContainer orchestration Etter som Docker containers har blitt den foretrukne måten å pakke og distribuere programvare på Linux platformen de siste par årene har det vokst frem et behov for systemer som kan samkjøre drift og utrulling av disse containerene. Ikke ulikt det økosystemet av produkter VMware har bygget opp rundt utvikling og drift av virtuelle maskiner.\nContainer orchestration systemene har som oppgave å sørge for:\nLastbalansering. Service discovery. Health checks. Automatisk skalering og restarting av vertsmaskiner og containere. Oppgraderinger uten nedetid (rolling deploy). Frem til nylig har økosystemet rundt container orchestration vært fragmentert og de mest populære alternativene har vært:\nKubernetes (Opprinnelig fra Google, nå styrt av CNCF, Cloud Native Computing Foundation) Swarm (Fra produsenten bak Docker) Mesos (Fra Apache Software Foundation) Fleet (Fra CoreOS) Men det siste året har det vært en konvergens mot Kubernetes som foretrukket løsning.\n7 februar CoreOS annonserer at de fjerner Fleet fra Container Linux og anbefaler Kubernetes 27 juli Microsoft slutter seg til CNCF 9 august Amazon Web Services slutter seg til CNCF 29 august VMware og Pivotal slutter seg til CNCF 17 september Oracle slutter seg til CNCF 17 oktober Docker annonserer native støtte for Kubernetes i tillegg til sitt eget Swarm produkt 24 oktober Microsoft Azure annonserer managed Kubernetes med tjenesten AKS 29 november Amazon Web Services annonserer managed Kubernetes med tjenesten EKS De to siste nyhetene er spesielt viktige. Å drifte sin egen Kubernetes-installasjon krever tid og kompetanse. (Les hvordan Stripe brukte 5 måneder på å bli fortrolig med å drifte sitt eget Kubernetes cluster, bare for batch jobs.)\nFrem til nå har valget vært mellom å drifte sitt eget Kubernetes cluster eller bruke Google Container Engine som har brukt Kubernetes siden 2014. Mange av oss føler et visst ubehag ved å låse oss til én tilbyder. Men dette er nå anderledes når en kan utvikle infrastruktur på Kubernetes, og velge tilnærmet fritt * mellom de 3 store cloud-tilbyderene i tillegg til å drifte selv om ønskelig.\n* Kubernetes utvikles raskt, og funksjonalitet blir ofte ikke tilgjengelig på de ulike platformene samtidig.\nOpprette Azure Kubernetes Cluster Forbehold Denne gjennomgangen tar utgangspunkt i dokumentasjonen på Microsoft.com. Å sette opp et Azure Kubernetes cluster fungerte ikke i starten av desember, men per dags dato, 23. desember, ser det ut til å fungere relativt bra. Men, oppgradering av cluster fra Kubernetes 1.7 til 1.8 fungerer for eksempel IKKE.\nAKS er i Preview og Azure jobber kontinuerlig med å gjøre AKS stabilt og støtte så mange Kubernetes-funksjoner som mulig. Amazon Web Services har tilsvarende en lukket invite-only Preview per dags dato mens de også jobber med stabilitet og funksjonalitet.\nBåde Azure og AWS uttrykker forventning om at deres Kubernetes tjenester skal være klare for produksjonsmiljø ila 2018.\nForberedelser Du behøver Azure-CLI (versjon 2.0.21 eller nyere) for å utføre kommandoene:\nLast ned Azure-CLI her Informasjon om Azure-CLI på MacOS og Linux finner du her Alle kommandoer gjøres i Windows PowerShell.\nAzure innlogging Logg på Azure:\n1 az login Du får en link som du åpner i din browser samt en autentiseringskode. Skriv koden på nettsiden og az login lagrer påloggingsinformasjonen slik at du ikke behøver å autentisere igjen på samme maskin.\nPS Pålogingsinformasjonen lagres i C:\\Users\\Brukernavn\\.azure\\. Du må selv passe på at ingen kopierer disse filene. Da får de full tilgang til din Azure konto.\nAktiver ContainerService Siden AKS er i Preview/Beta må du eksplisitt aktivere det for å få tilgang til aks kommandoene.\n1 2 az provider register -n Microsoft.ContainerService az provider show -n Microsoft.ContainerService Opprett en resource group Her oppretter vi en resource group med navn \u0026ldquo;min_aks_rg\u0026rdquo; i Azure region West Europe.\n1 az group create --name min_aks_rg --location westeurope Protip For å se en liste over tilgjengelige Azure regioner, bruk kommandoen az account list-locations --output table. PS Det kan hende AKS ikke er tilgjengelig i alle regioner enda.\nOpprette Kubernetes cluster 1 az aks create --resource-group min_aks_rg --name mitt_cluster --node-count 3 --generate-ssh-keys --node-vm-size Standard_B2s --node-osdisk-size 256 --kubernetes-version 1.8.2 --node-count Antall vertsmaskiner tilgjengelig for å kjøre containers --generate-ssh-keys Oppretter og outputter en SSH key som kan brukes for å SSHe direkte til vertsmaskinene. --node-vm-size Hvilken type Azure VM clusteret skal bestå av. For å se tilgjengelige størrelser bruk az vm list-sizes -l westeurope --output table og Microsofts nettsider. --node-osdisk-size Disk størrelse på vertsmaskiner i GB. PS Conteinere kan bli stoppet og flyttet til en annen host ved behov eller hvis en vertsmaskin forsvinner. Alle data lagret lokalt i conteineren blir da borte. Hvis en skal lagre ting permanent må en bruke PersistentVolumes og ikke lokal disk på vertsmaskin. --kubernetes-version Hvilken Kubernetes versjon som skal installeres. Azure installerer IKKE den siste versjonen som standard, og per dags dato fungerer ikke az aks upgrade tilstrekkelig. Siste tilgjengelige versjon per dags dato er 1.8.2. Det er en fordel å bruke siste versjon da det skjer store forbedringer i Kubernetes fra versjon til versjon. Dokumentasjon er også mye bedre for nyere versjoner. Lagre teksten som kommandoen spytter ut i en fil på en trygg plass. Den inneholder nøkler som kan brukes for å kople til clusteret med SSH. Selv om det i teorien ikke skal være nødvendig.\nInstaller kubectl kubectl er klienten som gjør alle operasjoner mot ditt Kubernetes cluster. Azure CLI kan installere kubectl for deg:\n1 az aks install-cli Etter kubectl er installert behøver vi å få påloggingsinformasjon slik at kubectl kan kommunisere med Kubernetes clusteret.\n1 az aks get-credentials --resource-group min_aks_rg --name mitt_cluster Påloggingsinformasjonen lagres i C:\\Users\\Brukernavn\\.kube\\config. Hold disse filene hemmelig også.\nProtip Når en har flere ulike Kubernetes clusters kan en bytte hvilken kubectl skal snakke til med kubectl config get-contexts og kubectl config set-context mitt_cluster.\nInspiser cluster For å se at clusteret og kubectl virker begynner vi med noen kommandoer.\nSe alle vertsmaskiner og status:\n1 2 3 4 5 \u0026gt; kubectl get nodes NAME STATUS AGE VERSION aks-nodepool1-16970026-0 Ready 15m v1.8.2 aks-nodepool1-16970026-1 Ready 15m v1.8.2 aks-nodepool1-16970026-2 Ready 15m v1.8.2 Se alle tjenester, pods, deployments:\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system po/kubernetes-dashboard-6fc8cf9586-frpkn 1/1 Running 0 3d NAMESPACE NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-system svc/kubernetes-dashboard 10.0.161.132 \u0026lt;none\u0026gt; 80/TCP 3d NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-system deploy/kubernetes-dashboard 1 1 1 1 3d NAMESPACE NAME DESIRED CURRENT READY AGE kube-system rs/kubernetes-dashboard-6fc8cf9586 1 1 1 3d Jeg har bare tatt et lite utdrag fra denne kommandoen. Du behøver ikke å forstå hva alle ressursene i kube-system namespacet gjør. Det er hensikten at du skal slippe det når Microsoft står for management av selve clusteret.\nNamespaces I Kubernetes er det noe som heter Namespaces. Ressurser i ett namespace har ikke automatisk tilgang til ressurser i et annet namespace. Tjenestene som Kubernetes selv benytter installeres i namespacet kube-system. Kommandoen kubectl viser deg vanligvis bare ressurser i default namespace med mindre du spesifiserer --all-namespaces eller --namespace=xx.\nStarte noen nginx containere En instans av en kjørende container kalles i Kubernetes for en Pod.\nnginx er en rask og fleksibel webserver.\nNå som clusteret er oppe å kjøre kan vi begynne å rulle ut tjenster og deployments på det.\nVi begynner med å lage en Deployment bestående av 3 containere som alle kjører nginx:mainline-alpine imaget fra Docker hub.\nnginx-dep.yaml ser slik ut:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1beta2 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:mainline-alpine ports: - containerPort: 80 Last denne inn på clusteret med kubectl create:\n1 kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-dep.yaml Denne kommandoen oppretter ressursene beskrevet i filen. kubectl kan lese filer enten lokalt fra din maskin eller fra en URL.\nEtter du har gjort endringer i en ressurs-definisjon (.yaml fil) kan du oppdatere ressursene i clusteret med kubectl replace -f ressurs.yaml\nVi kan verifisere at Deployment er klar:\n1 2 3 \u0026gt; kubectl get deploy NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 10m Vi kan også hente de faktiske Pods som er startet:\n1 2 3 4 5 \u0026gt; kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-569477d6d8-dqwx5 1/1 Running 0 10m nginx-deployment-569477d6d8-xwzpw 1/1 Running 0 10m nginx-deployment-569477d6d8-z5tfk 1/1 Running 0 10m Logger Vi kan se logger fra én pod med kubectl logs nginx-deployment-569477d6d8-xwzpw. Men siden vi i dette tilfellet ikke vet hvilken Pod som ender opp med å få innkommende forespørsler kan vi se logger fra alle Pods som har app=nginx label: kubectl logs -lapp=nginx. At vi her bruker app=nginx har vi selv bestemt i nginx-dep.yaml når vi satt spec.template.metadata.labels: app: nginx.\nGjøre nginx tilgjengelig med en tjeneste For å kommunisere med våre nye Pods behøver vi å opprette en tjeneste (Service). En tjeneste består av en eller flere Pods som velges basert på ulike kriterier, blant annet hvilke labels de har og om Podene det gjelder er Running og Ready.\nNå lager vi en tjeneste som ruter trafikk til alle Pods som har label app: nginx og som lytter på port 80. I tillegg gjør vi tjenesten tilgjengelig via en LoadBalancer:\nnginx-svc.yaml ser slik ut:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: type: LoadBalancer ports: - port: 80 name: http targetPort: 80 selector: app: nginx Vi ber Kubernetes om å opprette tjeneten vår med kubectl create som vanlig:\n1 kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-svc.yaml Deretter kan vi se hvilken IP-adresse tjenesten vår har fått av Azure:\n1 2 3 \u0026gt; kubectl get svc -w NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx 10.0.24.11 13.95.173.255 80:31522/TCP 15m PS Det kan ta et par minutter for Azure å tildele tjenesten vår en Public IP, i mellomtiden vil det stå \u0026lt;pending\u0026gt; under EXTERNAL-IP.\nEn enkel Welcome to nginx webside skal nå være tilgjengelig på http://13.95.173.255 (husk å bytt ut med din egen External-IP).\nVi har nå en lastbalansert nginx tjeneste med 3 servere klar til å ta imot trafikk.\nFor ordens skyld kan vi slette tjeneste og deployment etterpå:\n1 2 kubectl delete svc nginx kubectl delete deploy nginx-deployment Skalere cluster Hvis en ønsker å endre antall vertsmaskiner/noder som kjører Pods kan en gjøre det via Azure-CLI:\n1 az aks scale --name mitt_cluster --resource-group min_aks_rg --node-count 5 For øyeblikket blir alle noder opprettet med samme størrelse som når clusteret ble opprettet. AKS vil antageligvis få støtte for node-pools i løpet av neste år. Da kan en opprette grupper av noder med forskjellig størrelse og operativsystem, både Linux og Windows.\nSlette cluster En kan slette hele clusteret slik:\n1 az aks delete --name mitt_cluster --resource-group min_aks_rg --yes Bonusmateriale Her er litt bonusmateriale dersom du ønsker å gå enda litt videre med Kubernetes.\nRulle ut tjenester med Helm Helm er en pakke-behandler og et bibliotek av programvare som er klart for å rulles ut i et Kubernetes-cluster.\nStart med å laste ned Helm-klienten. Den henter påloggingsinformasjon osv fra samme sted som kubectl automatisk.\nInstaller Helm-serveren (Tiller) på Kubernetes clusteret og oppdater pakke-biblioteket:\n1 2 helm init helm repo update Se tilgjengelige pakker (Charts) med: helm search.\nRulle ut MineCraft med Helm La oss rulle ut en MineCraft installasjon på clusteret vårt, fordi vi kan :-)\n1 helm install --name stian-sin --set minecraftServer.eula=true stable/minecraft --set overstyrer en eller flere av standardverdiene som er satt i pakken. MineCraft pakken er laget slik at den ikke starter uten å ha sagt seg enig i brukervilkårene i variabelen minecraftServer.eula. Alle variablene som kan overstyres i MineCraft pakken er dokumentert her.\nSå venter vi litt på at Azure skal tildele en Public IP:\n1 2 \u0026gt; kubectl get svc -w stian-sin-minecraft 10.0.237.0 13.95.172.192 25565:30356/TCP 3m Og vipps kan vi kople til Minecraft på 13.95.172.192:25565.\nKubernetes Dashboard Kubernetes har også et grafisk web-grensesnitt som gjør det litt lettere å se hvilke ressurser som er i clusteret, se logger og åpne remote-shell inne i en kjørende Pod, blant annet.\n1 2 \u0026gt; kubectl proxy Starting to serve on 127.0.0.1:8001 kubectl krypterer og tunnelerer trafikken inn til Kubernetes\u0026rsquo; API servere. Dashboardet er tilgjengelig på http://127.0.0.1:8001/ui/.\nKonklusjon Jeg håper du har fått mersmak for Kubernetes. Lærekurven kan være litt bratt i begynnelsen men det tar ikke så veldig lang tid før du er produktiv.\nSe på de offisielle guidene på Kubernetes.io for å lære mer om hvordan du definerer forskjellige typer ressurser og tjenester for å kjøre på Kubernetes. PS: Det gjøres store endringer fra versjon til versjon så sørg for å bruke dokumentasjonen for riktig versjon!\nKubernetes har også et veldig aktivt Slack-miljø på kubernetes.slack.com. Der er det også en kanal for norske Kubernetes brukere; #norw-users.\n","date":"2017-12-25T00:00:00Z","image":"https://demo.stack.jimmycai.com/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/minecraft-k8s_hu15082107401327841977.png","permalink":"https://demo.stack.jimmycai.com/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/","title":"Managed Kubernetes på Microsoft Azure (Norwegian)"},{"content":"2021 Update: The specific tools discussed in this blog post should be considered obsolete by todays standards. You should investigate Prometheus, InfluxDB and TimescaleDB for your monitoring needs. In this paper we will provide a step by step guide on how to install a single-instance of OpenTSDB using the latest versions of the underlying technology, Hadoop and HBase. We will also provide some background on the state of existing monitoring solutions.\nTable of contents\nAbstract Background Performance problems - Welcome to I/O-hell Scaling problems Loss of detail Lack of flexibility The monitoring revolution Setting up a single node OpenTSDB instance on Debian 7 Wheezy Hardware requirements Operating system requirements Pre-setup preparations Installing java from packages Installing HBase Install snappy Building native libhadoop and libsnappy Configuring HBase Testing HBase and compression Starting HBase Installing OpenTSDB Configuring OpenTSDB Creating HBase tables Starting OpenTSDB Feeding data into OpenTSDB tcollector peritus-tc-tools collectd-opentsdb Monitoring OpenTSDB Performance comparison Collection Storage Conclusion Background Since its inception in 1999 rrdtool (the underlying storage mechanism of once universal MRTG) has been the base of many popular monitoring solutions; Cacti, collectd, Ganglia, Munin, Observium, OpenNMS and Zenoss, to name a few.\nThere are a number of problems with the current approach and we will highlight some of these here.\nPlease note that this includes Graphite and its backend Whisper, which is based on the same basic design as rrdtool and has some of the same limitations.\nPerformance problems - Welcome to I/O-hell When MRTG and rrdtool was created the preservation of disk space was more important than preservation of disk operations and the default collection interval was 5 minutes (which many are still using). The way rrdtool is designed it requires quite a few random reads and writes per datapoint. It also re-reads, computes the average, and writes old data again according to the RRA rules defined which causes additional I/O load. In 2014 memory is cheap, disk storage is cheap and CPU is fairly cheap. Disk I/O operations (IOPS) however are still very expensive in terms of hardware. The recent maturing of SSD provides extreme amounts of IOPS for a reasonable price, but the drive sizes are fractional. The result is that in order to scale IOPS-wise you currently need many low-space SSDs to get the required space, or many low-IOPS spindle drives to get the required IOPS:\nSamsung EVO 840 1TB SSD - 98.000 IOPS - 470 USD\nSeagate Barracuda 3TB - 240 IOPS - 110 USD\nYou would need $44.880 (408 drives) worth of spindle drives in order to match a single SSD drive in terms of I/O-performance. On the other hand a $2.000 array of spindle drives would get you a net ~54 TB of space. The cost of SSD to reach the same volume would be $25.380. Not to mention the cost of servers, power, provisioning, etc.\nNote: This is the cheapest available bulk consumer drives and comparable OEM drives (SSD, spindle) for a HP server will be 6 to 30 times more expensive.\nIn rrdtool version 1.4, released in 2009, rrdcached was introduced as a caching daemon for buffering multiple data updates and reducing the number of random I/O operations by writing several related datapoints in sequence. It took a couple of years before this new feature was implemented in most of the common open source monitoring solutions.\nFor a good introduction into the internals of rrdtool/rrdcached updates and the problems with I/O scaling look at presentation by Sebastian Harl, How to Escape the I/O Hell\nScaling problems Most of today\u0026rsquo;s monitoring systems do not easily scale-out. Scale-out, or scaling horizontally, is when you can add new nodes in response to increased load. Scaling up by replacing existing hardware with state-of-the-art hardware is both expensive and only buys you limited time before the next even more expensive necessary hardware upgrade. Many systems offer distributed polling but none offer the option of spreading out the disk load. For example; you can scale Zenozz for High Availability but not performance.\nLoss of detail Current RRD based systems will aggregate old data into averages in order to save storage space. Most technicians do not have the in depth knowledge in order to tune the rules for aggregation and will leave the default values as is. Using cacti as an example and looking at the cacti documentation we see that in a very short time, 2 months, data is averaged to a single data point PER DAY. For systems such as Internet backbones where traffic vary a lot from bottom (30% utilization for example) to peak (90% utilization for example) during a day only the average of 60% is shown in the graphs. This in turn makes troubleshooting by comparing old data difficult. It makes trending based on peaks/bottoms impossible and it may also lead to wrong or delayed strategic decisions on where to invest in added capacity.\nLack of flexibility In order to collect, store and graph new kinds of metrics an operator would need a certain level of programming skills and experience with the internals of the monitoring system. Adding new metrics to the systems would range from hours to weeks depending on the skill and experience of the operator. Creating new graphs based on existing metrics is also very difficult on most systems. And not within reach for the average operator.\nThe monitoring revolution We are currently at the beginning of a monitoring revolution. The advent of cloud computing and big data has created a need for measuring lots of metrics for thousands of machines at small intervals. This has sparked the creation of completely new monitoring components. One of the components where we now have improved alternatives is for efficient metric storage.\nThe first is OpenTSDB, a \u0026ldquo;Scalable, Distributed, Time Series Database\u0026rdquo; that begun development at StumbleUpon in 2011 and aimed at solving some of the problems with existing monitoring systems. OpenTSDB is built in top of Apache HBase which is a scalable and performant database that builds on top of Apache Hadoop. Hadoop is a series of tools for building large and scalable distributed systems. Back in 2010 Facebook already had 2000 machines in a Hadoop cluster with 21PB (that is 21.000.000 GB) of combined storage.\nThe second is an interesting newcommer, InfluxDB, that began development in 2013 and has the goal of offering scalability and performance without the requirements of HBase/Hadoop.\nIn addition to advances in performance these alternatives also decouple storage of metrics and display of graphs and abstract the interaction in simple and well-defined APIs. This makes it easy for developers to create improved frontends rapidly and this has already resulted in several very attractive open-source frontends such as Metrilyx (OpenTSDB), Grafana (InfluxDB, Graphite, soon OpenTSDB), StatusWolf (OpenTSDB), Influga (InfluxDB).\nSetting up a single node OpenTSDB instance on Debian 7 Wheezy In the rest of this paper we will set up a single node OpenTSDB instance. OpenTSDB builds on top of HBase and Hadoop and scales to very large setups easily. But it also delivers substantial performance on a single node which is deployed in less than an hour. There are plenty of guides on installing a Hadoop cluster but here we will focus on the natural first step of getting a single node running using recent releases of the relevant software:\nOpenTSDB 2.0.0 - Released 2014-05-05 HBase 0.98.2 - Released 2014-05-01 Hadoop 2.4.0 - Released 2014-04-07 If you later require to deploy a larger cluster consider using a framework such as Cloudera CDH or Hortonworks HDP which are open-source platforms which package Apache Hadoop components and provides a fully tested environment and easy-to-use graphical frontends for configuration and management. It is recommended to have at least 5 machines in a HBase cluster supporting OpenTSDB.\nThis guide assumes you are somewhat familiar with using a Linux shell/command prompt.\nHardware requirements CPU cores: Max (Limit to 50% of your available CPU resources) RAM: Min 16 GB Disk 1 - OS: 10 GB - Thin provisioned Disk 2 - Data: 100 GB - Thin provisioned Operating system requirements This guide is based on a recently installed Debian 7 Wheezy 64bit installed without any extra packages. See the official documentation for more information.\nAll commands are entered as root user unless otherwise noted.\nPre-setup preparations We start by installing a few tools that we will need later.\napt-get install wget make gcc g++ cmake maven\rCreate a new ext3 partition on the data disk /dev/sdb:\n(echo \u0026quot;n\u0026quot;; echo \u0026quot;p\u0026quot;; echo \u0026quot;\u0026quot;; echo \u0026quot;\u0026quot;; echo \u0026quot;\u0026quot;; echo \u0026quot;t\u0026quot;; echo \u0026quot;83\u0026quot;; echo \u0026quot;w\u0026quot;) | fdisk /dev/sdb\rmkfs.ext3 /dev/sdb1\rext3 is the recommended filesystem for Hadoop.\nCreate a mountpoint /mnt/data1 and add it to the file system table and mount the disk:\nmkdir /mnt/data1\recho \u0026quot;/dev/sdb1 /mnt/data1 ext3 auto,noexec,noatime,nodiratime 0 1\u0026quot; | tee -a /etc/fstab\rmount /mnt/data1\rUsing noexec for the data partition will increase security as nothing on the data partition will be allowed to ever execute. Using noatime and nodiratime increases performance since the read access timestamps are not updated on every file access.\nInstalling java from packages Installing java on Linux can be quite challenging due to licensing issues, but thanks to the guys over at Launchpad.net who are providing a repository with a custom java package this can now be done quite easy.\nWe start by adding the launchpad java repository to our /etc/apt/sources.list file:\necho \u0026quot;deb http://ppa.launchpad.net/webupd8team/java/ubuntu precise main\u0026quot; | tee -a /etc/apt/sources.list\recho \u0026quot;deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu precise main\u0026quot; | tee -a /etc/apt/sources.list\rAdd the signing key and download information from the new repository:\napt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys EEA14886\rapt-get update\rRun the java installer:\napt-get install oracle-java7-installer\rFollow the instructions on screen to complete the Java 7 installation.\nInstalling HBase OpenTSDB has its own HBase installation tutorial here. It is very brief and does not use the latest versions or snappy compression.\nDownload and unpack HBase:\ncd /opt\rwget http://apache.vianett.no/hbase/hbase-0.98.2/hbase-0.98.2-hadoop2-bin.tar.gz\rtar xvfz hbase-0.98.2-hadoop2-bin.tar.gz\rexport HBASEDIR=`pwd`/hbase-0.98.2-hadoop2/\rIncrease the system-wide limitations of open files and processes from the default of 1000 to 32000 by adding a few lines to /etc/security/limits.conf:\necho \u0026quot;root - nofile 32768\u0026quot; | tee -a /etc/security/limits.conf\recho \u0026quot;root soft/hard nproc 32000\u0026quot; | tee -a /etc/security/limits.conf\recho \u0026quot;* - nofile 32768\u0026quot; | tee -a /etc/security/limits.conf\recho \u0026quot;* soft/hard nproc 32000\u0026quot; | tee -a /etc/security/limits.conf\rThe settings above will only take effect if we also add a line to /etc/pam.d/common-session:\necho \u0026quot;session required pam_limits.so\u0026quot; | tee -a /etc/pam.d/common-session\rInstall snappy Snappy is a compression algorithm that values speed over compression ratio and this makes it a good choice for high throughput applications such as Hadoop/HBase. Due to licensing issues Snappy does not ship with HBase and need to be installed on top.\nThe installation process is a bit complicated and has caused headache for many people (me included). Here we will show a method of installing snappy and getting it to work with the latest version of HBase and Hadoop.\nCompression algorithms in HBase Compression is the method of reducing the size of a file or text without losing any of the contents. There are many compression algorithms available and some focus on being able to create the smallest compressed file at the cost of time and CPU usage while other achieve reasonable compression ratio while being very fast. Out of the box HBase supports gz(gzip/zlib), snappy and lzo. Only gz is included due to licensing issues. Unfortunately gz is a slow and costly algorithm compared to snappy and lzo. In a test performed by Yahoo (see slides here, page 8) gz achieves 64% compression in 32 seconds. lzo 47% in 4.8 seconds and snappy 42% in 4.0 seconds. lz4 is another protocol considered for inclusion that is even faster (2.4 seconds) but requires much more memory. For more information look at the Apache HBase Handbook - Appendix C - Compression\nBuilding native libhadoop and libsnappy In order to use compression we need the common Hadoop library, libhadoop.so, and the snappy library, libsnappy.so. HBase ships without libhadoop.so and the libhadoop.so that ships in the Hadoop Package is only for 32 bit OS. So we need to compile these files ourself.\nStart by downloading and installing ProtoBuf. Hadoop requres version 2.5+ which is not available as a Debian package unfortunately.\nwget --no-check-certificate https://protobuf.googlecode.com/files/protobuf-2.5.0.tar.gz\rtar zxvf protobuf-2.5.0.tar.gz\rcd protobuf-2.5.0\r./configure; make; make install\rexport LD_LIBRARY_PATH=/usr/local/lib/\rDownload and compile Hadoop:\napt-get install zlib1g-dev\rwget http://apache.uib.no/hadoop/common/hadoop-2.4.0/hadoop-2.4.0-src.tar.gz\rtar zxvf hadoop-2.4.0-src.tar.gz\rcd hadoop-2.4.0-src/hadoop-common-project/\rmvn package -Pdist,native -Dskiptests -Dtar -Drequire.snappy -DskipTests\rCopy the newly compiled native libhadoop library into /usr/local/lib, then create the folder in which HBase looks for it and create a shortcut from there to /usr/local/lib/libhadoop.so:\ncp hadoop-common/target/native/target/usr/local/lib/libhadoop.* /usr/local/lib\rmkdir -p $HBASEDIR/lib/native/Linux-amd64-64/\rcd $HBASEDIR/lib/native/Linux-amd64-64/\rln -s /usr/local/lib/libhadoop.so* .\rInstall snappy from Debian packages:\napt-get install libsnappy-dev\rConfiguring HBase Now we need to do some basic configuration before we can start HBase. The configuration files are in $HBASEDIR/conf/.\nconf/hbase-env.sh A shell script setting various environment variables related to how HBase and Java should behave. The file contains a lot of options and they are all documented by comments so feel free to look around in it.\nStart by setting the JAVA_HOME, which points to where Java is installed:\nexport JAVA_HOME=/usr/lib/jvm/java-7-oracle/\rThen increase the size of the Java Heap from the default of 1000 which is a bit low:\nexport HBASE_HEAPSIZE=8000\rconf/hbase-site.xml An XML file containing HBase specific configuration parameters.\n\u0026lt;configuration\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hbase.rootdir\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;/mnt/data1/hbase\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;property\u0026gt;\r\u0026lt;name\u0026gt;hbase.zookeeper.property.dataDir\u0026lt;/name\u0026gt;\r\u0026lt;value\u0026gt;/mnt/data1/zookeeper\u0026lt;/value\u0026gt;\r\u0026lt;/property\u0026gt;\r\u0026lt;/configuration\u0026gt;\rTesting HBase and compression Now that we have installed snappy and configured HBase we can verify that HBase is working and that the compression is loaded by doing:\n$HBASEDIR/bin/hbase org.apache.hadoop.hbase.util.CompressionTest /tmp/test.txt snappy\rThis should output some lines with information and end with SUCCESS.\nStarting HBase HBase ships with scripts for starting and stopping it, namely start-hbase.sh and stop-hbase.sh. You start HBase with\n$HBASEDIR/bin/start-hbase.sh\rThen look at the log to ensure it has started without any serious errors:\ntail -fn100 $HBASEDIR/bin/../logs/hbase-root-master-opentsdb.log\rIf you want HBase to start automatically on boot you can use a process management tool such as Monit or simply put it in /etc/rc.local:\n/opt/hbase-0.98.2-hadoop2/bin/start-hbase.sh\rInstalling OpenTSDB Start by installing gnuplot, which is used by the native webui to draw graphs:\napt-get install gnuplot\rThen download and install OpenTSDB:\nwget https://github.com/OpenTSDB/opentsdb/releases/download/v2.0.0/opentsdb-2.0.0_all.deb\rdpkg -i opentsdb-2.0.0_all.deb\rConfiguring OpenTSDB The configuration file is /etc/opentsdb/opentsdb.conf. It has some of the basic configuration parameters but not nearly all of them. Here is the official documentation with all configuration parameters.\nThe defaults are reasonable but we need to make a few tweaks, the first is to add this:\ntsd.core.auto_create_metrics = true\rThis will make OpenTSDB accept previously unseen metrics and add them to the database. This is very useful in the beginning when feeding data into OpenTSDB. Without this you will have to use the command mkmetric for each metric you will store and get errors that might be hard to trace if the metric you create do not match what is actually sent.\nThen we will add support for chunked requests via the HTTP API:\ntsd.http.request.enable_chunked = true\rtsd.http.request.max_chunk = 16000\rSome tools and plugins (such as our own improved collectd to OpenTSDB plugin) send multiple data points in a single HTTP request for increased efficiency and requires this setting to be enabled.\nCreating HBase tables Before we start OpenTSDB we need to create the necessary tables in HBase:\nenv COMPRESSION=SNAPPY HBASE_HOME=$HBASEDIR /usr/share/opentsdb/tools/create_table.sh\rStarting OpenTSDB Since version 2.0.0 OpenTSDB ships as a Debian package and includes SysV init scripts. To start OpenTSDB as a daemon running in the background we run:\nservice opentsdb start\rAnd then check the logs for any errors or other relevant information:\ntail -f /var/log/opentsdb/opentsdb.log\rIf the server is started successfully the last line of the log should say:\n13:42:30.900 INFO [TSDMain.main] - Ready to serve on /0.0.0.0:4242\rAnd you can now browse to your new OpenTSDB in a browser using http://hostname:4242 !\nFeeding data into OpenTSDB It is not within the scope of this paper to go into details about how to feed data into OpenTSDB but we will give a quick introduction here to get you started.\nA note on metric naming in OpenTSDB Each datapoint has a metric name such as df.bytes.free and one or more tags such as host=server1 and mount=/mnt/data1. This is closer to the proposed Metrics 2.0 standard for naming metrics than the traditional naming of df.bytes.free.server1.mnt-data. This makes it possible to create aggregates across tags and combine data easily using the tags. OpenTSDB stores each datapoint with a given metric and tags in one HBase row per hour. But due to a HBase issue it still has to scan every row that matches the metric, ignoring the tags. Even though it will only return the data also matching the tags. This results in very much data being read and it will be very slow to read if there is a large number of data points for a given metric. The default for the collectd-opentsdb plugin is to use the read plugin name as metric, and other values as tags. In my case this results in 72.000.000 datapoints per hour for this metric. When generating a graph all of this data has to be read and evaluated before drawing a graph. 24 hours of data is over 1.7 billion datapoints for this single metric and results in a read performance of 5-15 minutes for a simple graph. A solution to this is to use shift-to-metric, as mentioned in the OpenTSDB user guide. Shift-to-metric is simply moving one or more data identifiers from tags to the metric in order to reduce the cardinality (number of values) for a metric, and hence the time required to read out the data we want. We have modified the collectd-opentsdb java plugin in order to shift the tags to metrics, and this increases read-performance by ~1000x down to 10-100ms. Read the section about collectd below for more information on our modified plugin.\ntcollector tcollector is the default agent for collecting and sending data from a Linux server to a OpenTSDB server. It is based on Python and plugins / addons can be written in any language. It ships with the most common plugins to collect information about disk usage and performance, cpu and memory statistics and also for some specific systems such as mysql, mongodb, riak, varnish, postgresql and others. tcollector is very lightweight and features advanced de-duplication in order to reduce unneeded network traffic.\nThe commands for installing dependencies and downloading tcollector are\naptitude install git python\rcd /opt\rgit clone git://github.com/OpenTSDB/tcollector.git\rConfiguration is in the startup script tcollector/startstop, you will need to uncomment and set the value of TSD_HOST to point to your OpenTSDB server.\nTo start it run\n/opt/tcollector/startstop start\rThis is also the command you want to add to /etc/rc.local in order to have the agent automatically start at boot. Logfiles are saved in /var/log/tcollector.log and they are rotated automatically.\nperitus-tc-tools We have developed a set of tcollector plugins for collecting statistics from\nISC DHCPd server, about number of DHCP events and DHCP pool sizes OpenSIPS, total number of subscribers and registered user agents Atmail, number of users, admins, sent and received emails, logins and errors As well as a high performance replacement for smokeping called tc-ping.\nThese plugins are available for download from our GitHub page.\ncollectd-opentsdb collectd is the system statistics collection daemon and is a widely used system for collecting metrics from various sources. There are several options for sending data from collectd to OpenTSDB but one way that works well is to use the collectd-opentsdb java write plugin.\nSince collectd is a generic metric collection tool the original collectd-opentsdb plugin will use the plugin name (such as snmp) as the metric, and use tags such as host=servername, plugin_instance=ifHcInOctets and type_instance=FastEthernet0/1.\nAs mentioned in the note on metric naming in OpenTSDB this can be very inefficient when data needs to be read again resulting in read performance potentially thousands of times slower than optimal (\u0026lt;100ms). To alleviate this we have modified the original collectd-opentsdb plugin to store all metadata as part of the metric. This gives metric names such as ifHCInBroadcastPkts.sw01.GigabitEthernet0 and very good read performance.\nThe modified collectd-opentsdb plugin can be downloaded from our GitHub repository.\nMonitoring OpenTSDB To monitor OpenTSDB itself install tcollector as described above on the OpenTSDB server and set TSD_HOST to localhost in /opt/tcollector/startstop.\nYou can then go to http://opentsdb-server:4242/#start=1h-ago\u0026amp;end=1s-ago\u0026amp;m=sum:rate:tsd.rpc.received%7Btype=*%7D\u0026amp;o=\u0026amp;yrange=%5B0:%5D\u0026amp;wxh=1200x600 to view a graph of amount of data received in the last hour.\nPerformance comparison Lastly we include a little performance comparison between the latest version of OpenTSDB+HBase+Hadoop, a previous version of OpenTSDB+HBase+Hadoop that we have used for a while as well as rrdcached which ran in production for 4 years at a client.\nThe workload is gathering and storing metrics from 150 Cisco switches with 8200 ports/interfaces every 5 seconds. This equals about 15.000 points per second.\nCollection Even though it is not the primary focus, we include some data about collection performance for completeness. Collection is done using the latest version of collectd and the builtin SNMP plugin.\nNB #1: There is a memory leak in the way collectd\u0026rsquo;s SNMP plugin uses the underlying libsnmp library and you might need to schedule a restart of the collectd service as a workaround for that if handling large workloads.\nNB #2: Due to limitations in the libnetsnmp library you will run into problems if polling many (1000+) devices with a single collectd instance. A workaround is to run multiple collectd instances with fewer hosts. memory leak\nFigure 2 shows that collection through SNMP polling consumes about 2200Mhz. We optimized some of the data types and definitions in collectd when moving to OpenTSDB and achieved a 20% performance increase in the polling as seen in Figure 3.\nWriting to the native rrdcached write plugin consumes 1300Mhz while our modified collectd-opentsdb plugin consumes 1450Mhz. It is probably possible to create a much more efficient write plugin with more advanced knowledge of concurrency and using a lower level language such as C.\nStorage When considering storage performance we will look at CPU usage and disk IOPS since these are the primary drivers of cost in today\u0026rsquo;s datacenters.\ncollectd + rrdcached CPU usage - 1300Mhz, see Figure 2 above.\nOpenTSDB + Hbase 0.96 + Hadoop 1 OpenTSDB + HBase 0.98 + Hadoop 2 Conclusion Even without tuning, a single instance OpenTSDB installation is able to handle significant amounts of data before running into IO problems. This comes at a cost of CPU, currently OpenTSDB will consume \u0026gt; 300% the amount of CPU cycles compared to rrdcached for storage. But this is offset by a 85-95% reduction in disk load. In absolute terms for our particular set up (one 2 year old HP DL360p Gen8 running VMware vSphere 5.5) CPU usage increased from 15% to 25% while reducing IOPS load from 70% to \u0026lt; 10%.\nFine tuning of parameters (such as Java GC) as well as detailed analysis of memory usage is outside the scope of this brief paper and detailed information may be found elsewhere (51,52,53) for those interested.\n","date":"2014-06-02T19:56:40Z","image":"https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure1_hu9703766401419295099.png","permalink":"https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/","title":"Next generation monitoring with OpenTSDB"}]