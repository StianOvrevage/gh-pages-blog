<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Technology on blog.stian.omg.lol</title><link>https://demo.stack.jimmycai.com/categories/technology/</link><description>Recent content in Technology on blog.stian.omg.lol</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 03 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://demo.stack.jimmycai.com/categories/technology/index.xml" rel="self" type="application/rss+xml"/><item><title>Kubernetes Sidecar Config Drift</title><link>https://demo.stack.jimmycai.com/p/kubernetes-sidecar-config-drift/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://demo.stack.jimmycai.com/p/kubernetes-sidecar-config-drift/</guid><description>&lt;h1 id="kubernetes-sidecar-config-drift">Kubernetes Sidecar Config Drift
&lt;/h1>&lt;p>Huge thanks to one of my favorite clients, &lt;a class="link" href="https://www.signicat.com/" target="_blank" rel="noopener"
>Signicat&lt;/a>, and especially &lt;a class="link" href="https://www.linkedin.com/in/jon-skarpeteig/" target="_blank" rel="noopener"
>Jon&lt;/a>, for allowing me to share some of the nitty gritty details of a challenge that I believe is probably quite widespread, yet under-appreciated, in modern Kubernetes cloud environments.&lt;/p>
&lt;p>Last week, working on Signicat’s next generation cloud platform, I discovered that several individuals invented their own ways of mitigating what I now call Sidecar Configuration Drift.&lt;/p>
&lt;p>To ease the pain I created &lt;a class="link" href="https://github.com/StianOvrevage/k8s-sidecar-rollout" target="_blank" rel="noopener"
>k8s-sidecar-rollout&lt;/a> to restart the required workloads and thereby updating their sidecar configurations.&lt;/p>
&lt;p>This blog post is a bit of background information on what the causes of this problem is.&lt;/p>
&lt;h2 id="what-is-sidecar-config-drift">What is Sidecar Config Drift?
&lt;/h2>&lt;p>When using a Sidecar Injector (such as Istio), there is nothing that ensures that an update (potentially breaking) to a sidecar config template is applied/updated on Pods that have already been injected with a sidecar.&lt;/p>
&lt;p>This means that after updating a sidecar config it may take a very long time until all Pods have the updated config. They may receive the updates at any undetermined time in the future. While the updates are pending things might not work as expected and things may not be compliant. When the update is finally applied to the Pod it may surface breaking changes.&lt;/p>
&lt;p>I call this phenomena &lt;em>Sidecar Configuration Drift&lt;/em>.&lt;/p>
&lt;h2 id="background">Background
&lt;/h2>&lt;h3 id="kubernetes---declarative-vs-imperative">Kubernetes - Declarative vs imperative
&lt;/h3>&lt;p>What makes Kubernetes so powerful is also what can make it hard and confusing to work with until your mindset has shifted.&lt;/p>
&lt;p>That is the philosophy of being declarative instead of imperative like most of us are used to for the past 20 years.&lt;/p>
&lt;p>&lt;strong>Declarative means you tell Kubernetes HOW you want things to look. You DON’T tell Kubernetes WHAT to do.&lt;/strong>&lt;/p>
&lt;p>For example you do not tell Kubernetes to scale your Deployment to 5 instances. You tell Kubernetes that your Deployment should have 5 instances. The difference is subtle but extremely important. In a highly dynamic cloud environment your instances might disappear or crash for a multitude of reasons. In this declarative mindset you should not care about that since Kubernetes is tasked with ensuring 5 instances and will (try to) provision new ones when it’s needed.&lt;/p>
&lt;p>This is the apparent magic which makes Kubernetes.&lt;/p>
&lt;p>This magic is technically solved with what we call &lt;strong>Controllers&lt;/strong>. A controller is responsible for constantly comparing the &lt;strong>Actual state&lt;/strong> and &lt;strong>Desired state&lt;/strong>. To scale your Deployment to 5 instances you set &lt;code>replicas: 5&lt;/code> on the &lt;code>Deployment&lt;/code> resource. If needed a controller will create a completely new &lt;code>ReplicaSet&lt;/code> resource with 5 instances. Another controller will then create 5 &lt;code>Pods&lt;/code>. And the scheduler will finally try to place and start those &lt;code>Pods&lt;/code> on actual nodes. The &lt;code>ReplicaSet&lt;/code> controller will scale down the old once the new has reached it’s desired state.&lt;/p>
&lt;p>This constant process is called a &lt;strong>reconciliation loop&lt;/strong> and it’s a critical feature.&lt;/p>
&lt;h3 id="sidecars">Sidecars
&lt;/h3>&lt;p>Sidecar is a Kubernetes design pattern. A sidecar is simply a container that is living side-by-side with the main application container that can do tasks that are logically not part of the application. Examples of this can be log handling, monitoring agents, proxies. &lt;strong>All containers in a Pod (including sidecars) share the same filesystem, kernel namespace, IP addresses etc.&lt;/strong>&lt;/p>
&lt;p>More about sidecars: &lt;a class="link" href="https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns/" target="_blank" rel="noopener"
>The Distributed System ToolKit: Patterns for Composite Containers&lt;/a>&lt;/p>
&lt;h3 id="sidecar-injection">Sidecar injection
&lt;/h3>&lt;p>Sidecar injection is when a sidecar container is added to a Pod &lt;strong>even if the sidecar isn’t defined in any of the higher level primitives, such as &lt;code>Deployment&lt;/code> or &lt;code>StatefulSet&lt;/code>.&lt;/strong>&lt;/p>
&lt;p>When for example the &lt;code>ReplicaSet&lt;/code> controller will &lt;code>Create&lt;/code> a new &lt;code>Pod&lt;/code>. If configured, the Kubernetes API will call one or more &lt;code>MutatingWebhooks&lt;/code>. These webhooks can then change the &lt;code>Pod&lt;/code> definition before they are saved (and picked up by the scheduler).&lt;/p>
&lt;h3 id="the-problem">The Problem
&lt;/h3>&lt;p>The problem is when updating a sidecar injection template there is no system that runs a reconciliation loop.&lt;/p>
&lt;p>The webhook just updates the &lt;code>Pod&lt;/code> template. It does not keep track of which &lt;code>Pods&lt;/code> have gotten which template or check if any template change would result in a different sidecar configuration.&lt;/p>
&lt;p>The controllers ALSO does not continuously monitor if and how re-creating the same &lt;code>Pod&lt;/code> (without sidecars) would result in a different Pod once the sidecars have been injected.&lt;/p>
&lt;p>In effect sidecar injection does not follow the expected declarative pattern that the rest of Kubernetes does.&lt;/p>
&lt;h3 id="consequences">Consequences
&lt;/h3>&lt;p>If a platform team changes the istio sidecar template it will not actually take effect on a &lt;code>Pod&lt;/code> until that &lt;code>Pod&lt;/code> for some reason is re-created.&lt;/p>
&lt;p>Let’s assume the &lt;code>istio-proxy&lt;/code> sidecar template have been updated by the platform team. We roll it out and test it and it seems to work. But the change will break some applications running in the cluster.&lt;/p>
&lt;p>That breakage will go un-noticed until:&lt;/p>
&lt;pre>&lt;code>The product team commits changes that triggers a re-deploy. The deployment will suddenly fail but it might not have anything to do with the actual changes the team did to the application. This is surely confusing!
The platform team for example upgrades a pool of worker nodes causing all `Pods` to be re-created on new nodes.
A Pod is re-created when a Kubernetes worker node crashes. In this scenario it appears the failure spawned into existence out of nowhere since neither the product team nor platform team actually “did” anything to trigger it.
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>Also worth noting is that any attempts at Rolling back a Deployment now containing failing Pods will not actually fix anything.&lt;/strong>&lt;/p>
&lt;p>It’s the sidecar templates that needs to be rolled back and Pods probably need to be re-created again.&lt;/p>
&lt;h3 id="mitigations">Mitigations
&lt;/h3>&lt;p>We can mitigate drift by:&lt;/p>
&lt;pre>&lt;code>Re-starting all Pods in the cluster whenever we update sidecar injection templates.
Sometimes we might forget to re-start so regularly re-start all Pods in the cluster anyway.
&lt;/code>&lt;/pre>
&lt;h2 id="k8s-sidecar-rollout">k8s-sidecar-rollout
&lt;/h2>&lt;p>To make these restarts easy and fast I’ve created &lt;a class="link" href="https://github.com/StianOvrevage/k8s-sidecar-rollout" target="_blank" rel="noopener"
>https://github.com/StianOvrevage/k8s-sidecar-rollout&lt;/a> .&lt;/p>
&lt;p>It’s a tool that figures out (with your help) which workloads (Deployment, StatefulSet, DaemonSet) that needs to be rolled out again (re-started) and then rolls out for you. Head over to the GitHub repo for installation and complete usage instructions. Here is an example of how it can be used:&lt;/p>
&lt;pre>&lt;code>python3 sidecar-rollout.py \
--sidecar-container-name=istio-proxy \
--include-daemonset=true \
--annotation-prefix=myCompany \
--parallel-rollouts 10 \
--only-started-before=&amp;quot;2022-05-01 13:00&amp;quot; \
--exclude-namespace=kube-system \
--confirm=true
&lt;/code>&lt;/pre>
&lt;p>This will gather all Pods with a container named &lt;code>istio-sidecar&lt;/code> belonging to a Deployment or DaemonSet that was started before 2022-05-01 13:00 (which may be when we updated the istio sidecar config template) excluding the &lt;code>kube-system&lt;/code> namespace. It will patch the workloads with two annotations with &lt;code>myCompany&lt;/code> prefix and run 10 rollouts in parallel.&lt;/p>
&lt;p>The script that now re-starts Pods adds two annotations indicating that a restart to update sidecars has occurred as well as the time:&lt;/p>
&lt;pre>&lt;code>$ kubectl get pods -n product-team some-api-7cdc65482b-ged13 -o yaml | yq '.metadata.annotations'
sidecarRollout.rollout.timestamp: 2022-05-03T18:05:31
sidecarRollout.rollout.reason: Update sidecars istio-proxy
&lt;/code>&lt;/pre>
&lt;p>The idea is that if your Pods are suddenly failing, you can quickly check the annotations and see if it has anything to do with sidecar updates or not.&lt;/p>
&lt;p>These annotations will of course disappear again when a Deployment is updated.&lt;/p></description></item><item><title>Yak shaving - Photo drips for my mom</title><link>https://demo.stack.jimmycai.com/p/yak-shaving-photo-drips-for-my-mom/</link><pubDate>Sun, 13 Feb 2022 00:00:00 +0000</pubDate><guid>https://demo.stack.jimmycai.com/p/yak-shaving-photo-drips-for-my-mom/</guid><description>&lt;h1 id="yak-shaving---photo-drips-for-my-mom">Yak shaving - Photo drips for my mom
&lt;/h1>&lt;p>Update: Check out &lt;a class="link" href="https://github.com/StianOvrevage/photo-drips" target="_blank" rel="noopener"
>https://github.com/StianOvrevage/photo-drips&lt;/a> for ugly but working code.&lt;/p>
&lt;blockquote>
&lt;p>Update May 2022: My mom told me this week that I must NEVER stop sending these daily photos &amp;lt;3&lt;/p>
&lt;/blockquote>
&lt;h2 id="background">Background
&lt;/h2>&lt;p>&lt;strong>TL;DR: I finally organized my photo archive and in an evening created a service to e-mail my mom a photo from the last 20 years every morning.&lt;/strong>&lt;/p>
&lt;p>This started out a few weeks ago when I decided to reinstall Windows on my laptop.&lt;/p>
&lt;p>The first hurdle was that my 2-3-4 different cloud storage subscriptions were all overdue for a clean-up. The one best suited had me throttled to 1Mbit/s since I was storing 20TB+ of data.&lt;/p>
&lt;p>I’ve been taking a lot of photos and videos since I got my first digital camera 22 years ago. Even though I use Lightroom to keep some order there was some duplication and discontinuity. So (after upgrading my fibreoptic internet to 1Gbit, optimizing my home network and cleaning up space on my machine) I started to clean up and organize the various catalogues.&lt;/p>
&lt;p>Looking at memories from 20 years ago made me realize I’m better at taking photos than “utilizing” them afterwards. What really is the point, then? I took a picture of one on the screen with my phone and sent on snapchat to my mom, not thinking much about it. But she was really thrilled, which made me really happy as well.&lt;/p>
&lt;p>For my current client I’m nearing the end of my contract and for the last two months I’ve mainly been maintaining, documenting and handing over and I really miss building things and solving problems.&lt;/p>
&lt;p>Recently a potential client asked about Python and AWS Lambda competence. It’s not something I work with daily and it’s not on my CV. But it made me think about all the various languages and tools I’ve used during the last decade.&lt;/p>
&lt;p>My subconscious brain offers up an idea on how to a) build something b) brush up some Python and Lambda knowledge and c) brighten my moms day.&lt;/p>
&lt;h3 id="concept">Concept
&lt;/h3>&lt;p>Every morning a photo from my archives is e-mailed to my mom, a photo drip.&lt;/p>
&lt;p>There is also a gallery where she can look at the previous photo drips.&lt;/p>
&lt;h3 id="process-and-goals">Process and goals
&lt;/h3>&lt;p>The primary objective was to have a working prototype as quickly as possible. So no premature optimization, refactoring or anything.&lt;/p>
&lt;h2 id="photo-selection-and-preparation">Photo selection and preparation
&lt;/h2>&lt;p>At around 3pm I started browsing photos from year 2000. It took me about 75 minutes to pick out about 300 photos from the first 20.000.&lt;/p>
&lt;p>Tip: When browsing in Lightroom, press B to add to Quick Collection.&lt;/p>
&lt;p>Export the pictures in the quick collection with a custom filename format like this &lt;code>0003-2000-05-14.jpg&lt;/code>.&lt;/p>
&lt;p>This format ensures filenames are ordered from oldest to newest, and the date can later be extracted directly from the filename without doing any EXIF stuff.&lt;/p>
&lt;p>Photos resized to 1500x1500. Never enlarge. Sharpen for screen.&lt;/p>
&lt;h2 id="photo-storage">Photo storage
&lt;/h2>&lt;p>It would have been quicker to set up a AWS EC2 virtual machine running all the components but that would be too easy.&lt;/p>
&lt;p>The requirements for storage is: cheap, reliable, publicly available. So a standard AWS S3 bucket should do just fine.&lt;/p>
&lt;p>Using the browser I upload all the photos in batch to a new bucket. In a sub-folder with a random name. That should provide an appropriate level of security and avoid strangers on the internet stumbling upon it. Since the bucket is publicly available.&lt;/p>
&lt;p>I verify that I can load the pictures in my browser with the public S3 URL. It took a few attempts at getting the permissions right, and I suspect adding this policy was required even though everything in the settings was set to “Full public access”.&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;,
&amp;quot;Statement&amp;quot;: [
{
&amp;quot;Sid&amp;quot;: &amp;quot;PublicRead&amp;quot;,
&amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;,
&amp;quot;Principal&amp;quot;: &amp;quot;*&amp;quot;,
&amp;quot;Action&amp;quot;: [
&amp;quot;s3:GetObject&amp;quot;,
&amp;quot;s3:GetObjectVersion&amp;quot;
],
&amp;quot;Resource&amp;quot;: [
&amp;quot;arn:aws:s3:::memorydrops/*&amp;quot;
]
}
]
}
&lt;/code>&lt;/pre>
&lt;h2 id="sending-the-e-mail">Sending the e-mail
&lt;/h2>&lt;p>Sending e-mails directly with SMTP is really not an option anymore because of all the spam blocking systems. I know we need to use a third party service or something.&lt;/p>
&lt;p>I first looked at AWS SES (simple-email-service). But nothing about it seemed simple. Then looked at a few of the stablished ones such as SendGrid and Mailgun. These tools have evolved a lot and now has a plethora of features aimed at marketing, transactional e-mails etc. Probably overkill and potentially time consuming while having no transferable knowledge or code when hitting a dead-end since they are all different APIs.&lt;/p>
&lt;p>What about just using my own Gmail SMTP to ship?&lt;/p>
&lt;p>The first tutorials about that required turning on “Less secure app access” for my account.&lt;/p>
&lt;p>Not accepting that trade-off, I found &lt;a class="link" href="https://levelup.gitconnected.com/an-alternative-way-to-send-emails-in-python-5630a7efbe84" target="_blank" rel="noopener"
>https://levelup.gitconnected.com/an-alternative-way-to-send-emails-in-python-5630a7efbe84&lt;/a> where I learned that I can generate an “App password”, similar to an API Key for specific Google applications without disabling other security features. Jackpot!&lt;/p>
&lt;p>The article also has working code that I shamelessly used as a starting point.&lt;/p>
&lt;p>First I attached the photo of the day, but I really want it embedded. Even though I don’t really like linking images on a public URL (S3 bucket) because of potential browser and client issues that’s what I ended up with. The alternative of base64 encoding the data inline seemed like a chore. Besides I know mom uses Gmail in Chrome anyway so if it worked for me it would probably work for her.&lt;/p>
&lt;p>I don’t want the system to be dependent on any state management, databases, etc.&lt;/p>
&lt;p>Picking the right photo each day is simply select file N, where N is the days since the first deployment.&lt;/p>
&lt;p>Ideally I would use AWS S3 API to list the contents of the bucket to get the available files. But to save some time the “photo index” is simply a text file listing the files:&lt;/p>
&lt;pre>&lt;code>ls &amp;gt; filelist.txt
&lt;/code>&lt;/pre>
&lt;p>I create a new Lambda function and paste the python script and filelist.txt directly in the Lambda code editor and deploy and test it. Working!&lt;/p>
&lt;p>The Lambda function is the configured with a EventBridge trigger with the schedule &lt;code>cron(0 5 * * ? *)&lt;/code> that should trigger the function at 0500 UTC every day.&lt;/p>
&lt;h2 id="gallery">Gallery
&lt;/h2>&lt;p>I also want to have a gallery where she can view the previous memory drops without having to shuffle through endless emails.&lt;/p>
&lt;p>Started out by looking at VueJS for the frontend, which I have used before. But I have not used the new version yet and I suspected it might take a long time getting a project set up from scratch since unfortunately tutorials, tips, documentation in the frontend / JavaScript world has a tendency to be chronically outdated and unreliable.&lt;/p>
&lt;p>Dropped that and opted for a very simple native JS photo gallery called &lt;a class="link" href="https://github.com/ericleong/zoomwall.js/" target="_blank" rel="noopener"
>zoomwall.js&lt;/a>.&lt;/p>
&lt;p>Only showing a selection of photos depending on the day however requires a bit more engineering. (Writing this I realize another acceptable approach would be to use native JS to manipulate the DOM directly.)&lt;/p>
&lt;p>I implemented this by inlining the CSS and JS into one template HTML file that is rendered on-demand in a Python Lambda function. Then using AWS API Gateway to expose it as a normal webserver.&lt;/p>
&lt;p>I wanted to use Jinja2 for templating instead of the built-in Python templating functions. Doing that causes some headache since the Lambda environment does not have Jinja2 installed.&lt;/p>
&lt;p>Luckily creating a custom deployment package (.zip) including dependencies is rather trivial.&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>All in all this project was started with filtering photos at 3pm, having dinner from 5pm to 6pm and by 8.30pm everything was deployed. The following morning I had the memory drop in my inbox to great delight. An unexpected benefit of using my own Gmail for sending the e-mail is that mom can reply directly.&lt;/p>
&lt;p>Expected costs&lt;/p>
&lt;pre>&lt;code>S3 offers 5GB of standard storage for free for 12 months. After that I expect the cost to be in the $0.x range per month.
API Gateway offers 1 Million API calls per month for free for 12 months. After that I expect the cost to be in the $0.x range per month.
Lambda offers 1 Million requests per month forever.
&lt;/code>&lt;/pre>
&lt;p>Performance has been sacrificed for the gallery to make it simple. Server-side rendering is not going to be as fast as client-side. Python is not the fastest alternative. Using a FaaS such as Lambda also introduces penalties and unknowns (cold starts, etc). Yet the gallery HTML loads in ~200ms and and seems instant.&lt;/p>
&lt;h2 id="improvements">Improvements
&lt;/h2>&lt;p>After deciding to share the code I spent an hour or two writing this document as well as some necessary clean-up and changes from the prototype.&lt;/p>
&lt;p>The &lt;code>filelist.txt&lt;/code> is not bundled with the code. It’s hosted in the S3 bucket along with the photos. That means I can update and add photos later without touching code.&lt;/p>
&lt;p>That requires the &lt;code>requests&lt;/code> package, so now both modules are packaged with dependencies and uploaded via AWS CLI instead of browser.&lt;/p>
&lt;p>Some hard-coded URLs etc have been converted to environment variables.&lt;/p>
&lt;p>Added the &lt;code>exif&lt;/code> Python package to extract the original time and date of the photo to include in the e-mail.&lt;/p>
&lt;p>Added an URL redirect from a prettier domain to the auto generated API Gateway hostname of the gallery. Did not bother with proper custom domain since that requires a lot of fiddling with SSL certificates.&lt;/p>
&lt;h2 id="bugs-and-future-improvements">Bugs and future improvements
&lt;/h2>&lt;ul>
&lt;li>The JS gallery seems slightly broken. Might replace with a better one.&lt;/li>
&lt;li>The e-mail HTML is not pretty.&lt;/li>
&lt;li>Make sender and recipient e-mails environment variables (but recipient is currently a Python list, so).&lt;/li>
&lt;li>Make start date env var. Requires parsing date from user.&lt;/li>
&lt;li>The usual: Check that required env vars are set on startup. Improve error handling and logging.&lt;/li>
&lt;/ul></description></item><item><title>A side quest in API development, observability, Kubernetes and cloud with a hint of database</title><link>https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/</link><pubDate>Sat, 06 Mar 2021 00:00:00 +0000</pubDate><guid>https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/</guid><description>&lt;p>Quite often people ask me what I actually do. I have a hard time giving a short answer. Even to colleagues and friends in the industry.&lt;/p>
&lt;p>Here I will try to show and tell how I spent an evening digging around in a system I helped build for a client.&lt;/p>
&lt;br>
&lt;hr>
&lt;br>
&lt;p>&lt;strong>Table of contents&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="#Background" >Background&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#TheProblem" >The (initial) problem&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#FurtherReading" >Fixing the (initial) problem&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Verifying the (initial) fix&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#FurtherReading" >Baseline simple request - HTTP1 1 connections, 20000 requests&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Baseline complex request - HTTP1 1 connections, 20000 requests&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Verifying the fix for assumed workload&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#FurtherReading" >Complex request - HTTP1 6 connections, 500 requests&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Complex request - HTTP2 500 &amp;ldquo;connections&amp;rdquo;, 500 requests&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Side quest: Database optimizations&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Determining the next bottleneck&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Side quest: Cluster resources and burstable VMs&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Conclusion" >Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="Background">&lt;/a>&lt;/p>
&lt;h1 id="background">Background
&lt;/h1>&lt;p>I&amp;rsquo;m a consultant doing development, DevOps and cloud infrastructure.&lt;/p>
&lt;p>For this specific client I mainly develop APIs using Golang to support new products and features as well as various exporting, importing and processing of data in the background.&lt;/p>
&lt;p>I&amp;rsquo;m also the &amp;ldquo;ops&amp;rdquo; guy handling everything in AWS, setting up and maintaing databases, making sure the &amp;ldquo;DevOps&amp;rdquo; works and the frontend and analytics people can do their work with little friction.
99% of the time things work just fine. No data is lost. The systems very rarely have unforeseen downtime and the users can access the data they want with acceptable latency rarely exceeding 500ms.&lt;/p>
&lt;p>A couple of times a year I assess the status of the architecture and set up new environments from scratch and update any documentation that has drifted. This is also a good time to do changes and add or remove constraints in anticipation of future business needs.&lt;/p>
&lt;p>In short, the current tech stack that has evolved over a couple of years is:&lt;/p>
&lt;ul>
&lt;li>Everything hosted on Amazon Web Services (AWS).&lt;/li>
&lt;li>AWS managed Elastic Kubernetes Service (EKS) currently on K8s 1.18.&lt;/li>
&lt;li>GitHub Actions for building Docker images for frontends, backends and other systems.&lt;/li>
&lt;li>AWS Elastic Container Registry for storing Docker images.&lt;/li>
&lt;li>Deployment of each system defined as a Helm chart alongside source code.&lt;/li>
&lt;li>Actual environment configuration (Helm values) stored in repo along source code. Updated by GitHub Actions.&lt;/li>
&lt;li>ArgoCD in cluster to manage status of all environments and deployments. Development environments usually automatically deployed on change. Push a button to deploy to Production.&lt;/li>
&lt;li>Prometheus for storing metrics from the cluster and nodes itself as well as custom metrics for our own systems.&lt;/li>
&lt;li>Loki for storing logs. Makes it easier to retrieve logs from past Pods and aggregate across multiple Pods.&lt;/li>
&lt;li>Elastic APM server for tracing.&lt;/li>
&lt;li>Pyroscope for live CPU profiling/tracing of Go applications.&lt;/li>
&lt;li>Betteruptime.com for tracking uptime and hosting status pages.&lt;/li>
&lt;/ul>
&lt;p>I might write up a longer post about the details if anyone is interested.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h1 id="the-initial-problem">The (initial) problem
&lt;/h1>&lt;p>A week ago I upgraded our API from version 1, that was deployed in January, to version 2 with new features and better architecture.&lt;/p>
&lt;p>One of the endpoints of the API returns an analysis of an object we track. I have previously reduced the amount of database queries by 90% but it still requires about 50 database calls from three different databases.
Getting and analyzing the data usually completes in about 3-400 milliseconds returning an 11.000 line JSON.&lt;/p>
&lt;p>It&amp;rsquo;s also possible to just call &lt;code>/objects/analysis&lt;/code> to get the analysis for all the 500 objects we are tracking. It takes 20 seconds but is meant for exports to other processes and not interactive use, so not a problem.&lt;/p>
&lt;p>Since the product is under very active development the frontend guys just download the whole analysis for an object to show certain relevant information to users. It&amp;rsquo;s too early to decide on which information is needed more often and how to optimize for that. Not a problem.&lt;/p>
&lt;p>So we need an overview of some fields from multiple objects in a dashboard / list. We can easily pull analysis from 20 objects without any noticable delay.&lt;/p>
&lt;p>But what if we just want to show more, 50? 200? 500? The frontend already have the IDs for all the objects and fetches them from &lt;code>/objects/id/analysis&lt;/code>. So they loop over the IDs and fire of requests simultaneously.&lt;/p>
&lt;p>Analyzing the network waterfall in Chrome DevTools indicated that the requests now took 20-30 seconds to complete! But looking closer most of the time they were actually queued up in the browser. This is because
Chrome only allows 6 concurrent TCP connection to the same origin when using HTTP1 (&lt;a class="link" href="https://developers.google.com/web/tools/chrome-devtools/network/understanding-resource-timing%29" target="_blank" rel="noopener"
>https://developers.google.com/web/tools/chrome-devtools/network/understanding-resource-timing)&lt;/a>.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h2 id="fixing-the-initial-problem">Fixing the (initial) problem
&lt;/h2>&lt;p>HTTP2 should fix this problem easily. By default HTTP2 is disabled in nginx-ingress. I add a couple of lines enabling it and update the Helm deployment of the ingress controller.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h2 id="verifying-the-initial-fix">Verifying the (initial) fix
&lt;/h2>&lt;p>Some common development tools doesn&amp;rsquo;t support HTTP2, such as Postman. So I found &lt;code>h2load&lt;/code> which can both help me verify HTTP2 is working and I also get to measure the improvement, nice!&lt;/p>
&lt;blockquote>
&lt;p>Note that I&amp;rsquo;m not using the analysis endpoint since I want to measure the change from HTTP1 to HTTP2 and it will become apparent later that there are other bottlenecks preventing us from a linear performance increase when just changing from HTTP1 to HTTP2.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Also note that this is somewhat naive since it requests the same URL over and over which can give false results due to any caching. But fortunately we don&amp;rsquo;t do any caching yet.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h3 id="baseline-simple-request---http1-1-connections-20000-requests">Baseline simple request - HTTP1 1 connections, 20000 requests
&lt;/h3>&lt;p>Using 1 concurrent streams, 1 client and HTTP1 I get an estimate of performance pre-http2:&lt;/p>
&lt;pre>&lt;code>h2load --h1 --requests=20000 --clients=1 --max-concurrent-streams=1 https://api.x.com/api/v1/objects/1
&lt;/code>&lt;/pre>
&lt;p>The results are as expected:&lt;/p>
&lt;pre>&lt;code>finished in 1138.99s, 17.56 req/s, 18.41KB/s
requests: 20000 total, 20000 started, 20000 done, 19995 succeeded, 5 failed, 0 errored, 0 timeout
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-apm.png"
width="1816"
height="705"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-apm_hu17076544040877865054.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-apm_hu9690961613720477067.png 1024w"
loading="lazy"
alt="Overview from Elastic APM. Duration is very acceptable at around 20ms. No errors. And about 25% of the time spent doing database queries."
class="gallery-image"
data-flex-grow="257"
data-flex-basis="618px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-cpu.png"
width="1034"
height="314"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-cpu_hu2013984859425220242.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-cpu_hu13236457479005124721.png 1024w"
loading="lazy"
alt="Container CPU usage. Nothing special."
class="gallery-image"
data-flex-grow="329"
data-flex-basis="790px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-latency.png"
width="861"
height="356"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-latency_hu1867474644236441644.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-latency_hu15523458481306920119.png 1024w"
loading="lazy"
alt="Database query latency. The vast majority under 5ms. Acceptable."
class="gallery-image"
data-flex-grow="241"
data-flex-basis="580px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-queries.png"
width="780"
height="348"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-queries_hu17361066750422420350.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-db-queries_hu10420784785634132053.png 1024w"
loading="lazy"
alt="Number of DB queries per second."
class="gallery-image"
data-flex-grow="224"
data-flex-basis="537px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-latency.png"
width="863"
height="316"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-latency_hu6805455395055610702.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-latency_hu3298924550317512719.png 1024w"
loading="lazy"
alt="HTTP response latency."
class="gallery-image"
data-flex-grow="273"
data-flex-basis="655px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-requests.png"
width="778"
height="217"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-requests_hu5945712598158928612.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/0-baseline-http1-1-concurrent-http-requests_hu419352892190289981.png 1024w"
loading="lazy"
alt="Number of HTTP requests per second. Unsurprisingly the number of database queries are identical to the number of HTTP requests. Latency of HTTP requests also tracks the latency of the (single) database query."
class="gallery-image"
data-flex-grow="358"
data-flex-basis="860px"
>&lt;/p>
&lt;p>For http2 we set max concurrent streams to the same as number of requests:&lt;/p>
&lt;pre>&lt;code>h2load --requests=200 --clients=1 --max-concurrent-streams=200 https://api.x.com/api/v1/objects/1
&lt;/code>&lt;/pre>
&lt;p>Which results in almost half the latency:&lt;/p>
&lt;pre>&lt;code>finished in 1.23s, 162.65 req/s, 158.06KB/s
requests: 200 total, 200 started, 200 done, 200 succeeded, 0 failed, 0 errored, 0 timeout
&lt;/code>&lt;/pre>
&lt;p>So HTTP2 is working and providing significant latency improvements. Success!&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h3 id="baseline-complex-request---http1-1-connections-20000-requests">Baseline complex request - HTTP1 1 connections, 20000 requests
&lt;/h3>&lt;p>We start by establishing a baseline with 1 connection querying over and over.&lt;/p>
&lt;pre>&lt;code>h2load --h1 --requests=20000 --clients=1 --max-concurrent-streams=1
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-apm.png"
width="2087"
height="707"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-apm_hu13565166093859492250.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-apm_hu14225806095910349646.png 1024w"
loading="lazy"
alt="Latency increases as much more computation is done and data is returned. But the latency is consistent which is good. We also see that the database is becomming the bottleneck for where most time is spent."
class="gallery-image"
data-flex-grow="295"
data-flex-basis="708px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-cpu.png"
width="1200"
height="308"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-cpu_hu4067009256511003722.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-cpu_hu17950687200232907900.png 1024w"
loading="lazy"
alt="CPU usage increased to 15%. Lower increase than expected considering the complexity involved in serving the requests."
class="gallery-image"
data-flex-grow="389"
data-flex-basis="935px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-latency.png"
width="985"
height="344"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-latency_hu7089278039337102831.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-latency_hu17575998733902921949.png 1024w"
loading="lazy"
alt="Database query latency still mostly under 5ms."
class="gallery-image"
data-flex-grow="286"
data-flex-basis="687px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-queries.png"
width="894"
height="351"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-queries_hu6509951597429203579.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-db-queries_hu14549008744426915283.png 1024w"
loading="lazy"
alt="Number of database queries increases by a factor of 10 compared to HTTP requests."
class="gallery-image"
data-flex-grow="254"
data-flex-basis="611px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-latency.png"
width="987"
height="313"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-latency_hu565398038381364000.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-latency_hu7243746795286387547.png 1024w"
loading="lazy"
alt="HTTP latency."
class="gallery-image"
data-flex-grow="315"
data-flex-basis="756px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-requests.png"
width="895"
height="219"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-requests_hu9040739607883373017.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/1-baseline-http1-1-concurrent-analysis-http-requests_hu3790465645525383070.png 1024w"
loading="lazy"
alt="HTTP requests per second."
class="gallery-image"
data-flex-grow="408"
data-flex-basis="980px"
>&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h2 id="verifying-the-fix-for-assumed-workload">Verifying the fix for assumed workload
&lt;/h2>&lt;p>So we verified that HTTP2 gives us a performance boost. But what happens when we fire away 500 requests to the much heavier &lt;code>/analysis&lt;/code> endpoint?&lt;/p>
&lt;blockquote>
&lt;p>These graphs are not as pretty since the ones above. This is mainly due to the sampling interval of the metrics and that we need several datapoints to accurately determine the rate() of a counter.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h3 id="complex-request---http1-6-connections-500-requests">Complex request - HTTP1 6 connections, 500 requests
&lt;/h3>&lt;pre>&lt;code>finished in 32.25s, 14.88 req/s, 2.29MB/s
requests: 500 total, 500 started, 500 done, 500 succeeded, 0 failed, 0 errored, 0 timeout
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-apm.png"
width="1484"
height="705"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-apm_hu2573287072721533229.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-apm_hu10620414983979950514.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-apm"
class="gallery-image"
data-flex-grow="210"
data-flex-basis="505px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-cpu.png"
width="847"
height="299"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-cpu_hu1135877464954207689.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-cpu_hu17008451797633252465.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-cpu"
class="gallery-image"
data-flex-grow="283"
data-flex-basis="679px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-latency.png"
width="706"
height="359"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-latency_hu9358974391483687749.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-latency_hu16352819740947538611.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-db-latency"
class="gallery-image"
data-flex-grow="196"
data-flex-basis="471px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-queries.png"
width="637"
height="352"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-queries_hu17797633323803646239.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-db-queries_hu2207066698060964218.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-db-queries"
class="gallery-image"
data-flex-grow="180"
data-flex-basis="434px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-latency.png"
width="700"
height="321"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-latency_hu13253021454442065078.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-latency_hu13649080090034874975.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-http-latency"
class="gallery-image"
data-flex-grow="218"
data-flex-basis="523px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-requests.png"
width="638"
height="213"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-requests_hu12544065771888019895.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/2-burst-http1-6-concurrent-analysis-http-requests_hu1798439057131787658.png 1024w"
loading="lazy"
alt="2-burst-http1-6-concurrent-analysis-http-requests"
class="gallery-image"
data-flex-grow="299"
data-flex-basis="718px"
>&lt;/p>
&lt;p>In summary it so far seems to scale linearly with load. Most of the time is spent fetching data from the database. Still very predictable low latency on database queries and the resulting HTTP response.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h3 id="complex-request---http2-500-connections-500-requests">Complex request - HTTP2 500 &amp;ldquo;connections&amp;rdquo;, 500 requests
&lt;/h3>&lt;p>&lt;em>So now we unleash the beast. Firing all 500 requests at the same time.&lt;/em>&lt;/p>
&lt;pre>&lt;code>finished in 16.66s, 30.02 req/s, 3.55MB/s
requests: 500 total, 500 started, 500 done, 500 succeeded, 0 failed, 0 errored, 0 timeout
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-cpu.png"
width="939"
height="307"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-cpu_hu5828488102210360607.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-cpu_hu3343570902582545481.png 1024w"
loading="lazy"
alt="CPU on API still doing good. A slight hint of CPU throttling due to CFS, which is used when you set CPU limits in Kubernetes."
class="gallery-image"
data-flex-grow="305"
data-flex-basis="734px"
>&lt;/p>
&lt;blockquote>
&lt;p>Important about Kubernetes and CPU limits&lt;br />
Even with CPU limits set to 1 (100% of one CPU), your container can still be throttled at much lower CPU usage. Check out &lt;a class="link" href="https://medium.com/omio-engineering/cpu-limits-and-aggressive-throttling-in-kubernetes-c5b20bd8a718" target="_blank" rel="noopener"
>this article&lt;/a> for more information.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-latency.png"
width="782"
height="346"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-latency_hu16596262103242219393.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-latency_hu6743393326456624241.png 1024w"
loading="lazy"
alt="Whopsie. The average database query latency has increased drastically, and we have a long tail of very slow queries. Looks like we are starting to see signs of bottlenecks on the database. This might also be affected by our maximum of 60 concurrent connections to the database, resulting in queries having to wait their turn before executing."
class="gallery-image"
data-flex-grow="226"
data-flex-basis="542px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-queries.png"
width="705"
height="346"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-queries_hu17270843372303526721.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-db-queries_hu14745640312772792195.png 1024w"
loading="lazy"
alt="Its hard to judge the peak rate of database queries due to limited sampling of the metrics."
class="gallery-image"
data-flex-grow="203"
data-flex-basis="489px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-http-latency.png"
width="783"
height="309"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-http-latency_hu9429010041105922198.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-http-latency_hu14677623071583589615.png 1024w"
loading="lazy"
alt="Now individual HTTP requests are much slower due to waiting for the database."
class="gallery-image"
data-flex-grow="253"
data-flex-basis="608px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-apm-trace.png"
width="1150"
height="1192"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-apm-trace_hu12116834356231894546.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/3-burst-http2-500-concurrent-analysis-apm-trace_hu17401424198592652531.png 1024w"
loading="lazy"
alt="Here is just a random trace from Elastic APM to see if the increased database latency is concentrated to specific queries or tables or just general saturation. Indeed there is a single query responsible for half the time taken for the entire query! We better get back to that in a bit and dig further."
class="gallery-image"
data-flex-grow="96"
data-flex-basis="231px"
>&lt;/p>
&lt;p>In an ideal world all 500 requests should start and complete in 2-300ms regardless. Since that is not happening it&amp;rsquo;s an indication that we are now hitting some other bottleneck.&lt;/p>
&lt;p>Looking at the graphs it seems we are starting to saturate the database. The latency for every request is now largely dependent on the slowest of the 10-12 database queries it depends on. And as we are stressing the database the probability of slow queries increase. The latency for the whole process of fetching 500 requests are again largely dependent on the slowest requests.&lt;/p>
&lt;p>So this optimization gives on average better performance, but more variability of the individual requests, when the system is under heavy load.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h1 id="side-quest-database-optimizations">Side quest: Database optimizations
&lt;/h1>&lt;p>It seems we are saturating the database. Before throwing more money at the problem (by increasing database size) I like to know what the bottlenecks are. Looking at the traces from APM
I see one query that is consistently taking 10x longer than the rest. I also confirm this in the AWS RDS Performance Insights that show the top SQL queries by load.&lt;/p>
&lt;p>When designing the database schema I came up with the idea of having immutability for certain data types. So instead of overwriting row with ID 1, we add a row with ID 1 Revision 2. Now we have the history of who did what to the data and can easily track changes and roll back if needed. The most common use case is just fetching the last revision. So for simplicity I created a PostgreSQL view that only shows the last revision. That way clients don&amp;rsquo;t have to worry about the existense of revisions at all. That is now just an implementation detail.&lt;/p>
&lt;p>When it comes to performance that turns out to be an important implementation detail. The view is using &lt;code>SELECT DISTINCT ON (id) ... ORDER BY id, revision DESC&lt;/code>. However many of the queries to the view is ordering the returned data by time, and expect the data returned from database to already be ordered chronologically. Using &lt;code>EXPLAIN ANALYZE&lt;/code> on the queries this always results in a full table scan instead of using indexes, and is what&amp;rsquo;s causing this specific query to be slow. Without going into details it seems there is no simple and efficient way of having a view with the last revision and query that for a subset of rows ordered again by time.&lt;/p>
&lt;p>For the forseable future this does not actually impact real world usage. It&amp;rsquo;s only apparent under artificially large loads under the worst conditions. But now we know where we need to refactor things if performance actually becomes a problem.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h1 id="determining-the-next-bottleneck">Determining the next bottleneck
&lt;/h1>&lt;p>Whenever I fix one problem I like to know where, how and when the next problem or limit is likely to appear. When increasing the number of requests and streams I expected to see increasing latency. But instead I see errors appear like a cliff:&lt;/p>
&lt;pre>&lt;code>finished in 27.33s, 36.59 req/s, 5.64MB/s
requests: 5000 total, 1002 started, 1002 done, 998 succeeded, 4002 failed, 4000 errored, 0 timeout
&lt;/code>&lt;/pre>
&lt;p>Consulting the logs for both the nginx load balancer and the API there are no records of failing requests. Since nginx does not pass the HTTP2 connection directly to the API, but instead &amp;ldquo;unbundles&amp;rdquo; them into HTTP1 requests I suspect there might be issues with connection limits or even available ports from nginx to the API. But maybe it&amp;rsquo;s a configuration issue. By default nginx does &lt;a class="link" href="http://nginx.org/en/docs/http/ngx_http_upstream_module.html#server" target="_blank" rel="noopener"
>not limit the number of connections to a backend&lt;/a> (our API). . But, there is actually a &lt;a class="link" href="https://nginx.org/en/docs/http/ngx_http_v2_module.html#http2_max_requests" target="_blank" rel="noopener"
>default limit to the number of HTTP2 requests that can be served over a single connection&lt;/a> - And it happens to be 1000.&lt;/p>
&lt;p>I leave it at that. It&amp;rsquo;s very unlikely we&amp;rsquo;ll be hitting these limits any time soon.&lt;/p>
&lt;p>&lt;a id="TheProblem">&lt;/a>&lt;/p>
&lt;h1 id="side-quest-cluster-resources-and-burstable-vms">Side quest: Cluster resources and burstable VMs
&lt;/h1>&lt;p>When load testing the first time around sometimes Grafana would also become unresponsive. That&amp;rsquo;s usually a bad sign. It might indicate that the underlying infrastructure is also reaching saturation. That is not good since it can impact what should be independent services.&lt;/p>
&lt;p>Our Kubernetes cluster is composed of 2x t3a.medium on demand nodes and 2x t3a.medium spot nodes. These VM types are burstable. You can use 20% per vCPU sustained over time without problems. If you exceed those 20% you start consuming CPU credits faster than they are granted and once you run out of CPU credits processes will be forcibly throttled.&lt;/p>
&lt;p>Of course Kubernetes does not know about this and expects 1 CPU to actually be 1 CPU. In addition Kubernetes will decide where to place workloads based on their stated resource requirements and limits, and not their actual resource usage.&lt;/p>
&lt;p>When looking at the actual metrics two of our nodes are indeed out of CPU credits and being throttled. The sum of factors leading to this is:&lt;/p>
&lt;ul>
&lt;li>We have not yet set resource requests and limits making it harder for Kubernetes to intelligently place workloads&lt;/li>
&lt;li>Using burstable nodes having some additional constraints not visible to Kubernetes&lt;/li>
&lt;li>Old deployments laying around consuming unnecessary resources&lt;/li>
&lt;li>Adding costly features without assessing the overall impact&lt;/li>
&lt;/ul>
&lt;p>I have not touched on the last point yet. I started adding &lt;a class="link" href="https://pyroscope.io/" target="_blank" rel="noopener"
>Pyroscope&lt;/a> to our systems since I simply love monitoring All The Things. The documentation does not go into specifics but emphasizes that it&amp;rsquo;s &amp;ldquo;low overhead&amp;rdquo;. Remember that our budget for CPU usage is actually 40% per node, not 200%. The Pyroscope server itself consumes 10-15% CPU which seems fair. But investigating further the Pyroscope agent also consumes 5-6% CPU per instance. This graph shows the CPU usage of a single Pod before and after turning off Pyroscope profiling.&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/pyroscope-agent-cpu.png"
width="1029"
height="271"
srcset="https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/pyroscope-agent-cpu_hu15074112692027608241.png 480w, https://demo.stack.jimmycai.com/p/a-side-quest-in-api-development-observability-kubernetes-and-cloud-with-a-hint-of-database/pyroscope-agent-cpu_hu2516833612973000297.png 1024w"
loading="lazy"
alt="pyroscope-agent-cpu"
class="gallery-image"
data-flex-grow="379"
data-flex-basis="911px"
>&lt;/p>
&lt;p>5-6% CPU overhead on a highly utilized service is probably worth it. But when the baseline CPU usage is 0% CPU and we have multiple services and deployments in different environments we are suddenly using 40-60% CPU on profiling and less than 1% on actual work!&lt;/p>
&lt;p>The outcome of this is that we need to separate burstable and stable load deployments. Monitoring and supporting systems are usually more stable resource wise while the actual business systems much more variable, and suitable for burst nodes. In practice we add a node pool of non-burst VMs and use NodeAffinity to stick Prometheus, Pyroscope etc to those nodes. Another benefit of this is that the supporting systems needed to troubleshoot problems are now less likely to be impacted by the problem itself, making troubleshooting much easier.&lt;/p>
&lt;p>&lt;a id="Conclusion">&lt;/a>&lt;/p>
&lt;h1 id="conclusion">Conclusion
&lt;/h1>&lt;p>This whole adventure only took a few hours but resulted in some specific and immediate performance gains. It also highlighted the weakest links in our application, database and infrastructure architecture.&lt;/p></description></item><item><title>End of 2020 rough database landscape</title><link>https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/</link><pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate><guid>https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/</guid><description>&lt;img src="https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-nosql.png" alt="Featured image of post End of 2020 rough database landscape" />&lt;p>There seems to exist a database for every niche, mood or emotion. And they seem to change just as fast.&lt;/p>
&lt;p>How do you balance the urge for the new and shiny but without risking too much headache down the road?&lt;/p>
&lt;p>This post is an attempt to lay out the rough landscape of databases that you might encounter or consider as of late 2020.&lt;/p>
&lt;p>There will be broad generalizations for brevity.&lt;/p>
&lt;p>The goal is not to be exhaustive or take all possible precautions. Consider it a starting point for further research and planning.&lt;/p>
&lt;hr>
&lt;p>TLDR: Scroll to the &lt;a class="link" href="#Landscape" >diagrams&lt;/a> or view the &lt;a class="link" href="https://demo.stack.jimmycai.com/attachments/2020-11-27-end-of-2020-rough-database-landscape/map-complete.png" >big picture&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Table of contents&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="#Background" >Background&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#ProjectPhase" >Project phase overview&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Planning" >Planning&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#DatabaseCategories" >Database categories&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#SQL" >SQL&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#NoSQL" >NoSQL&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#KeyValue" >KeyValue&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Timeseries" >Timeseries&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Graph" >Graph&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#OtherNiceThings" >Other nice things&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Landscape" >The Landscape&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#SQLMap" >SQL&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#NoSQLMap" >NoSQL&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#KeyValueMap" >KeyValue&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#TimeseriesMap" >Timeseries&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#GraphMap" >Graph&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#FurtherReading" >Further reading&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Conclusion" >Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="Background">&lt;/a>&lt;/p>
&lt;h1 id="background">Background
&lt;/h1>&lt;p>I&amp;rsquo;m a consultant doing development, DevOps and cloud infrastructure. I also have the occasional side project trying out the Tech Flavor of the Month.&lt;/p>
&lt;p>&lt;a id="ProjectPhase">&lt;/a>&lt;/p>
&lt;h2 id="project-phase-overview">Project phase overview
&lt;/h2>&lt;p>The typical phases in projects I&amp;rsquo;m involved in follow no scientific or trademarked methodology, so YMMV:&lt;/p>
&lt;h3 id="starting-out">Starting out
&lt;/h3>&lt;p>Get something working as fast as possible. Take all the shortcuts. Use some opinionated framework or platform.&lt;/p>
&lt;h3 id="moving-from-development-to-production">Moving from development to production
&lt;/h3>&lt;p>People like it, people use it. Move the thing from a single &amp;ldquo;pet server&amp;rdquo; to a more robust cloud environment.&lt;/p>
&lt;h3 id="scaling-production">Scaling production
&lt;/h3>&lt;p>Bottlenecks and scaling problems start to emerge. Refactor or replace some pieces to remove the bottlenecks.&lt;/p>
&lt;h3 id="challenges">Challenges
&lt;/h3>&lt;p>Moving between these phases might be a major PITA if the wrong shortcuts were taken in the previous phases.&lt;/p>
&lt;p>&lt;em>This of course applies to all technology choices and not just databases. But we have to start somewhere, right?&lt;/em>&lt;/p>
&lt;p>&lt;a id="Planning">&lt;/a>&lt;/p>
&lt;h1 id="planning">Planning
&lt;/h1>&lt;p>When starting out I try to envision all the phases of the project and which directions it may take in the future.&lt;/p>
&lt;p>First I want the technology or software I choose to be instantly usable. A Docker image. Great. An &lt;code>apt-get install&lt;/code>. Sweet. &lt;code>npm install&lt;/code>. Sure, why not. Downloading a tarball. Installing some C dependencies. Setting some flags. Compiling. Symlinking and fixing permissions. Creating some configuration from scratch. Making my own systemd service definitions. Going back and doing every step again because it failed. &lt;em>Mkay, no thanks, I&amp;rsquo;m out.&lt;/em>&lt;/p>
&lt;p>At least for me it&amp;rsquo;s a plus if it&amp;rsquo;s easy to deploy on Kubernetes since I use it for everything already. I always have a cluster or three laying around so I can get a prototype or five up and running quickly before later spending money for cloud hosting.&lt;/p>
&lt;p>Does the thing have momentum and a community? If it does it probably has high quality tooling either by the vendor or the open source community (preferably both). It probably also has lots of common questions answered on blogs and StackOverflow and Github issues.&lt;/p>
&lt;p>&lt;strong>So we managed to build something and the audience likes it.&lt;/strong>&lt;/p>
&lt;p>How easy is it to move it from a production environment into something stable and low-maintenance? For databases that would typically involve using a managed service for hosting it. You do not want to be responsible for operating your own databases. Is it common enough that there are competitors in the marketplace offering it as a managed service? If there is only a single option expect prices to be very steep. Preferably also a managed service by one of the big known cloud platforms. They are usually cheaper. They are less likely to vanish. It might make integration with other systems easier later.&lt;/p>
&lt;p>&lt;strong>We hit some problems either because of raw scale or some type of usage we did not anticipate in the beginning.&lt;/strong>&lt;/p>
&lt;p>Are there compatible implementations that might solve some common problems? Typically this is because an implementation has to make a decision about it&amp;rsquo;s trade-offs. For a database system this is usually around the CAP theorem. A database system (or anything that keeps state) can be:&lt;/p>
&lt;ul>
&lt;li>&lt;em>Partition Tolerant&lt;/em> - The system still works if a node or the network between nodes fail.&lt;/li>
&lt;li>&lt;em>Available&lt;/em> - All requests receive a response.&lt;/li>
&lt;li>&lt;em>Consistent&lt;/em> - The data we read is the current data and not an earlier state.&lt;/li>
&lt;/ul>
&lt;p>But, you can only have two at the same time. And distributed systems tends to need to be partition tolerant. So we are stuck between consistency and availability.&lt;/p>
&lt;p>It might be a good to have an idea of the CAP tradeoffs an implementation has done, and whether there are compatible implementations with different tradeoffs that can be used if later we find out we need to tweak our trade-offs for speed and/or scale.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>More information about CAP theorem &lt;a class="link" href="https://en.wikipedia.org/wiki/CAP_theorem" target="_blank" rel="noopener"
>here&lt;/a> and &lt;a class="link" href="https://towardsdatascience.com/cap-theorem-and-distributed-database-management-systems-5c2be977950e" target="_blank" rel="noopener"
>here&lt;/a>. Jepsen have also &lt;a class="link" href="https://jepsen.io/analyses" target="_blank" rel="noopener"
>extensively tested&lt;/a> many popular databases to see how they break and if they are true to their stated trade-offs.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="DatabaseCategories">&lt;/a>&lt;/p>
&lt;h2 id="database-categories">Database categories
&lt;/h2>&lt;p>Databases can be roughly sorted into categories. I&amp;rsquo;ll keep it simple and use the everyday lingo and not go into details about semantics and definitions (forgive me).&lt;/p>
&lt;p>&lt;a class="link" href="https://www.prisma.io/dataguide/intro/comparing-database-types" target="_blank" rel="noopener"
>https://www.prisma.io/dataguide/intro/comparing-database-types&lt;/a>&lt;/p>
&lt;p>&lt;a id="Planning">&lt;/a>&lt;/p>
&lt;h3 id="sql">SQL
&lt;/h3>&lt;p>The oldest category is the relational database, also known as SQL based on the typical interface used to access these databases.&lt;/p>
&lt;p>In general these databases have tables with names, a set of pre-defined columns and an arbitrary number of rows. You should have an idea of the data types to be stored in each column (such as text or numbers).&lt;/p>
&lt;p>The downside of this is that you have to start with a rough model of the data you want to store and work with. The benefit of this is that later you know something about the model of the data you are working with. Most of the time I&amp;rsquo;ll happily do this in the database rather than handle all the potential inconsistencies in all systems that use that database.&lt;/p>
&lt;p>&lt;em>Main contenders: PostgreSQL. MySQL &amp;amp; MariaDB.&lt;/em>&lt;/p>
&lt;p>&lt;a id="NoSQL">&lt;/a>&lt;/p>
&lt;h3 id="nosql">NoSQL
&lt;/h3>&lt;p>All the rage the last decade. You put data in you get data out. The data is structured but not necessarily predefined. Think JSON object with values, arrays and lists.&lt;/p>
&lt;p>The benefit is productivity when developing. The drawback is that you may pay a price for those shortcuts later if you&amp;rsquo;re not careful.&lt;/p>
&lt;p>&lt;em>Main contender: MongoDB.&lt;/em>&lt;/p>
&lt;p>&lt;a id="KeyValue">&lt;/a>&lt;/p>
&lt;h3 id="keyvalue">KeyValue
&lt;/h3>&lt;p>Technically a sub-category of NoSQL, and should probably be called caches. But I feel it deserves it&amp;rsquo;s own category.&lt;/p>
&lt;p>A hyper-fast hyper-simple type of database. It has two columns. A key (ID) and value. The value can be anything, a string, a number, an entire JSON object or a blob containing binary data.&lt;/p>
&lt;p>These are typically used in combination with another type of database. Either by storing very commonly used data for even quicker access. Or for certain types of simple data that requires insane speed or throughput and you don&amp;rsquo;t want to overload the main database.&lt;/p>
&lt;p>&lt;em>Main contender: Redis.&lt;/em>&lt;/p>
&lt;p>&lt;a id="Timeseries">&lt;/a>&lt;/p>
&lt;h3 id="timeseries">Timeseries
&lt;/h3>&lt;p>A lesser known type of database optimized for storing a time series. A time series is a specific data type where the index is typically the time of a measurement. And the measurement is a number.&lt;/p>
&lt;p>A time series is almost never changed after the fact. So these databases can be optimized for writing huge amounts of new data and reading and calculating on existing data. At the cost of performance for deleting or updating old data which is sloooow. Since the values are always numbers that tend to change somewhat predictably compression and deduplication can save us massive amounts of storage.&lt;/p>
&lt;p>&lt;em>Main contenders: Prometheus, InfluxDB, TimescaleDB (plugin for PostgreSQL).&lt;/em>&lt;/p>
&lt;p>&lt;a id="Graph">&lt;/a>&lt;/p>
&lt;h3 id="graph">Graph
&lt;/h3>&lt;p>Graph databases are cool. In a graph database the relationship between objects is a primary feature. Whereas in SQL you need to join an element from one table with another object in another table with some kind of common identifier.&lt;/p>
&lt;p>For most simple use cases a regular SQL database will do fine. But when the number of objects stored (rows) and the number of intermediary tables (joins) become large it gets slow, or expensive, or both.&lt;/p>
&lt;p>I don&amp;rsquo;t have much experience with graph databases but I suspect they are less suited to general tasks and should be reserved for solving specific problems.&lt;/p>
&lt;p>&lt;em>Main contenders: Neo4j. Redis + RedisGraph.&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>PS: Graph databases and GraphQL are completely separate things.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="OtherNiceThings">&lt;/a>&lt;/p>
&lt;h3 id="other-nice-things">Other nice things
&lt;/h3>&lt;p>When researching this post I&amp;rsquo;ve come across things that look promising but are hard to categorize or fall in their own very niche categories.&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://dgraph.io" target="_blank" rel="noopener"
>Dgraph&lt;/a> - A GraphQL and backend in one.&lt;/li>
&lt;li>&lt;a class="link" href="https://prestodb.io" target="_blank" rel="noopener"
>PrestoDB&lt;/a> - An SQL interface on top of whatever database or storage you want to connect.&lt;/li>
&lt;li>&lt;a class="link" href="https://rethinkdb.com" target="_blank" rel="noopener"
>RethinkDB&lt;/a> - A NoSQL database focused on real-time streaming/updating clients.&lt;/li>
&lt;li>&lt;a class="link" href="https://www.foundationdb.org" target="_blank" rel="noopener"
>FoundationDB&lt;/a> - A transactional key-value store by Apple.&lt;/li>
&lt;li>&lt;a class="link" href="https://clickhouse.tech/" target="_blank" rel="noopener"
>ClickHouse&lt;/a> - An SQL database that stores data (on disk) in columns instead of rows. Makes for blazingly fast analytical and aggregation queries.&lt;/li>
&lt;li>&lt;a class="link" href="https://aws.amazon.com/qldb/" target="_blank" rel="noopener"
>Amazon Quantum Ledger Database&lt;/a> - A managed distributed ledger database (aka blockchain).&lt;/li>
&lt;li>&lt;a class="link" href="https://www.enterprisedb.com/products/edb-postgres-advanced-server-secure-ha-oracle-compatible" target="_blank" rel="noopener"
>EDB Postgres Advanced Server&lt;/a> - An Oracle compatible PostgreSQL variant.&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="Landscape">&lt;/a>&lt;/p>
&lt;h1 id="the-landscape">The Landscape
&lt;/h1>&lt;p>&lt;em>How to use these maps:&lt;/em>&lt;/p>
&lt;p>Version compatibility are in parenthesis. I have not mapped every version and how much breaking they are compared to previous versions but included some notes where I know there might be issues.&lt;/p>
&lt;p>&lt;em>API/Protocol/Interface&lt;/em> - This is decided by the framework, tool or driver you want to use. Sometimes it might be easier to choose the framework first and then a fitting database protocol. Or you might be lucky to choose the database features you need first and then select frameworks, tools and drivers that support it.&lt;/p>
&lt;blockquote>
&lt;p>I think interfaces are really important when creating and choosing technology. I had a &lt;a class="link" href="https://speakerdeck.com/stianovrevage/avoiding-lock-in-without-avoiding-managed-services" target="_blank" rel="noopener"
>presentation&lt;/a> about it a while ago and I think it&amp;rsquo;s still relevant.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;em>Engine&lt;/em> - Database implementations that are independent but try to be compatible. If there are alternatives to the &amp;ldquo;original&amp;rdquo; implementation they might have done different tradeoffs with regards to the CAP theorem or solve other specific problems.&lt;/p>
&lt;p>&lt;em>Big three managed&lt;/em> - Available managed services by the big three clouds, Amazon (AWS), Google (GCP) or Microsoft (Azure). Having an option to host in the big three is most likely the cheapest method as well as having a variety of other managed services to build a complete system in a single cloud.&lt;/p>
&lt;p>&lt;em>Vendor managed&lt;/em> - If the database vendor or backing company offers an Official managed service. They are usually hosted on the big three. Potentially a large cost premium over the raw compute power.&lt;/p>
&lt;p>&lt;em>Self-hosted&lt;/em> - Implementations you can run on your own computer or server.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Legend&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;img src="https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/icon-checklist.png"
width="42"
height="48"
srcset="https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/icon-checklist_hu1289597614310538750.png 480w, https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/icon-checklist_hu305759868025016403.png 1024w"
loading="lazy"
alt="icon-checklist"
class="gallery-image"
data-flex-grow="87"
data-flex-basis="210px"
>&lt;/td>
&lt;td>The checklist icon marks potential compatibility issues. For most use cases not a problem.&lt;br>&lt;strong>PS:&lt;/strong> The absence of this icon does not automatically mean compatibility.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;img src="https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/icon-operator.png"
width="50"
height="50"
srcset="https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/icon-operator_hu14757510342261432426.png 480w, https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/icon-operator_hu9633860489137427651.png 1024w"
loading="lazy"
alt="icon-operator"
class="gallery-image"
data-flex-grow="100"
data-flex-basis="240px"
>&lt;/td>
&lt;td>I put the lightning icon on the self-hosted implementations that have what seems to be stable Kubernetes operators available. In short, a Kubernetes operator makes running a stateful system, such as a database, on Kubernetes much easier. It might allow for longer time before migrating to a managed service.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;a id="SQLMap">&lt;/a>&lt;/p>
&lt;h2 id="sql-1">SQL
&lt;/h2>&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-sql.png"
width="1405"
height="947"
srcset="https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-sql_hu3071437159255848473.png 480w, https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-sql_hu14104597638369796258.png 1024w"
loading="lazy"
alt="map-sql"
class="gallery-image"
data-flex-grow="148"
data-flex-basis="356px"
>&lt;/p>
&lt;blockquote>
&lt;p>Compatibility:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://blog.yugabyte.com/postgresql-compatibility-in-yugabyte-db-2-0/" target="_blank" rel="noopener"
>PostgreSQL - Yugabyte&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.cockroachlabs.com/docs/stable/postgresql-compatibility.html" target="_blank" rel="noopener"
>PostgreSQL - CockroachDB&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://mariadb.com/kb/en/mariadb-vs-mysql-compatibility/" target="_blank" rel="noopener"
>MySQL - MariaDB&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Kubernetes Operators:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/CrunchyData/postgres-operator" target="_blank" rel="noopener"
>PostgreSQL (CrunchyData)&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/zalando/postgres-operator" target="_blank" rel="noopener"
>PostgreSQL (Zalando)&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.yugabyte.com/latest/deploy/kubernetes/single-zone/oss/yugabyte-operator/" target="_blank" rel="noopener"
>Yugabyte&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/cockroachdb/cockroach-operator" target="_blank" rel="noopener"
>CockroachDB&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.percona.com/software/percona-kubernetes-operators" target="_blank" rel="noopener"
>Percona PostgreSQL for MySQL &amp;amp; XtraDB&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;a id="NoSQLMap">&lt;/a>&lt;/p>
&lt;h2 id="nosql-1">NoSQL
&lt;/h2>&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-nosql.png"
width="1404"
height="847"
srcset="https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-nosql_hu2273731095068582186.png 480w, https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-nosql_hu12168083053440373248.png 1024w"
loading="lazy"
alt="map-nosql"
class="gallery-image"
data-flex-grow="165"
data-flex-basis="397px"
>&lt;/p>
&lt;blockquote>
&lt;p>PS: There are some &lt;a class="link" href="https://docs.mongodb.com/manual/release-notes/4.0-compatibility/" target="_blank" rel="noopener"
>breaking changes&lt;/a> from MongoDB 3.6 to 4 so make sure the tools you intend to use are compatible with the database version you intend on using.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Kubernetes Operators:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/mongodb/mongodb-kubernetes-operator" target="_blank" rel="noopener"
>MongoDB&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.percona.com/doc/kubernetes-operator-for-psmongodb/index.html" target="_blank" rel="noopener"
>Percona Distribution for MongoDB&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/scylladb/scylla-operator" target="_blank" rel="noopener"
>ScyllaDB&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-overview.html" target="_blank" rel="noopener"
>Elastic Stack&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;a id="KeyValueMap">&lt;/a>&lt;/p>
&lt;h2 id="keyvalue-1">KeyValue
&lt;/h2>&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-keyvalue.png"
width="513"
height="767"
srcset="https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-keyvalue_hu18426603385328126354.png 480w, https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-keyvalue_hu7247583368495670911.png 1024w"
loading="lazy"
alt="map-keyvalue"
class="gallery-image"
data-flex-grow="66"
data-flex-basis="160px"
>&lt;/p>
&lt;blockquote>
&lt;p>Kubernetes Operators:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/spotahome/redis-operator" target="_blank" rel="noopener"
>Redis (Spotahome)&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;a id="TimeseriesMap">&lt;/a>&lt;/p>
&lt;h2 id="timeseries-1">Timeseries
&lt;/h2>&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-timeseries.png"
width="1724"
height="757"
srcset="https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-timeseries_hu2547349071269886546.png 480w, https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-timeseries_hu14200254779532467911.png 1024w"
loading="lazy"
alt="map-timeseries"
class="gallery-image"
data-flex-grow="227"
data-flex-basis="546px"
>&lt;/p>
&lt;blockquote>
&lt;p>Kubernetes Operators:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack" target="_blank" rel="noopener"
>Prometheus-Stack&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/VictoriaMetrics/operator" target="_blank" rel="noopener"
>VictoriaMetrics&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;a id="GraphMap">&lt;/a>&lt;/p>
&lt;h2 id="graph-1">Graph
&lt;/h2>&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-graph.png"
width="1614"
height="761"
srcset="https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-graph_hu18251327852176947982.png 480w, https://demo.stack.jimmycai.com/p/end-of-2020-rough-database-landscape/map-graph_hu10764438276487603028.png 1024w"
loading="lazy"
alt="map-graph"
class="gallery-image"
data-flex-grow="212"
data-flex-basis="509px"
>&lt;/p>
&lt;blockquote>
&lt;p>Kubernetes Operators:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.arangodb.com/docs/stable/deployment-kubernetes-usage.html" target="_blank" rel="noopener"
>ArangoDB&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;a id="FurtherReading">&lt;/a>&lt;/p>
&lt;h1 id="further-reading">Further reading
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://en.wikipedia.org/wiki/Comparison_of_relational_database_management_systems" target="_blank" rel="noopener"
>Wikipedia on RDBMS&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://db-engines.com/en/" target="_blank" rel="noopener"
>DB-engines.com&lt;/a> - Lots of statistics and comparisons between DB engines&lt;/li>
&lt;li>&lt;a class="link" href="https://landscape.cncf.io/" target="_blank" rel="noopener"
>CNCF Landscape&lt;/a> - What&amp;rsquo;s moving in the cloud native landscape, including databases.&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="Conclusion">&lt;/a>&lt;/p>
&lt;h1 id="conclusion">Conclusion
&lt;/h1>&lt;p>Congratulations if you made it this far!&lt;/p>
&lt;p>I did this research primarily to reduce my own analysis paralysis on various projects so I can get-back-to-building. If you learned something as well, great stuff!&lt;/p>
&lt;p>And if you want my advice, just use PostgreSQL unless you really know about some special requirements that necessitates using something else :-)&lt;/p></description></item><item><title>Mini-post: Down-scaling Azure Kubernetes Service (AKS)</title><link>https://demo.stack.jimmycai.com/p/mini-post-down-scaling-azure-kubernetes-service-aks/</link><pubDate>Tue, 04 Jun 2019 00:00:00 +0000</pubDate><guid>https://demo.stack.jimmycai.com/p/mini-post-down-scaling-azure-kubernetes-service-aks/</guid><description>&lt;img src="https://demo.stack.jimmycai.com/p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu.png" alt="Featured image of post Mini-post: Down-scaling Azure Kubernetes Service (AKS)" />&lt;p>We discovered today that some implicit assumptions we had about AKS at smaller scales were incorrect.&lt;/p>
&lt;p>Suddenly new workloads and jobs in our Radix CI/CD could not start due to insufficient resources (CPU &amp;amp; memory).&lt;/p>
&lt;p>Even though it only caused problems in development environments with smaller node sizes it still surprised some of our developers, since we expected the size of development clusters to have enough resources.&lt;/p>
&lt;p>I thought it would be a good chance to go a bit deeper and verify some of our assumptions and also learn more about various components that usually &amp;ldquo;just works&amp;rdquo; and isn&amp;rsquo;t really given much thought.&lt;/p>
&lt;p>First I do a &lt;code>kubectl describe node &amp;lt;node&amp;gt;&lt;/code> on 2-3 of the nodes to get an idea of how things are looking:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">Resource Requests Limits
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-------- -------- ------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cpu 930m &lt;span class="o">(&lt;/span>98%&lt;span class="o">)&lt;/span> 5500m &lt;span class="o">(&lt;/span>585%&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">memory &lt;span class="m">1659939584&lt;/span> &lt;span class="o">(&lt;/span>89%&lt;span class="o">)&lt;/span> 4250M &lt;span class="o">(&lt;/span>228%&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>So we are obviously hitting the roof when it comes to resources. But why?&lt;/p>
&lt;h2 id="node-overhead">Node overhead
&lt;/h2>&lt;p>We use &lt;code>Standard DS1 v2&lt;/code> instances as AKS nodes and they have 1 CPU core and 3.5 GiB memory.&lt;/p>
&lt;p>The output of &lt;code>kubectl describe node&lt;/code> also gives us info on the Capacity (total node size) and Allocatable (resources available to run Pods).&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Capacity:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cpu: 1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> memory: 3500452Ki
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Allocatable:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cpu: 940m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> memory: 1814948Ki
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>So we have lost &lt;strong>60 millicores / 6%&lt;/strong> of CPU and &lt;strong>1685Mi‬B / 48%&lt;/strong> of memory. The next question is if this increases linearly with node size (the percentage of resources lost is the same regardless of node size) or is fixed (always reserves 60 millicores and 1685Mi of memory), or a combination.&lt;/p>
&lt;p>I connect to another cluster that has double the node size (&lt;code>Standard DS2 v2&lt;/code>) and compare:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Capacity:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cpu: 2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> memory: 7113160Ki
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Allocatable:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cpu: 1931m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> memory: 4667848Ki
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>So for this the loss is &lt;strong>69 millicores / 3.5%&lt;/strong> of CPU and &lt;strong>2445MiB / 35%&lt;/strong> of memory.&lt;/p>
&lt;p>So CPU reservations are close to fixed regardless of node size while memory reservations are influenced by node size but luckily not linearly.&lt;/p>
&lt;p>What causes this &amp;ldquo;waste&amp;rdquo;? Reading up on &lt;a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/" target="_blank" rel="noopener"
>kubernetes.io&lt;/a> gives a few clues. Kubelet will reserve CPU and memory resources for itself and other Kubernetes processes. It will also reserve a portion of memory to act as a buffer whenever a Pod is going beyond it&amp;rsquo;s memory limits to avoid risking System OOM, potentially making the whole node unstable.&lt;/p>
&lt;p>To figure out what these are configured to we log in to an actual AKS node&amp;rsquo;s console and run &lt;code>ps ax|grep kube&lt;/code> and the output looks like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="o">/&lt;/span>&lt;span class="n">usr&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">local&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">bin&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubelet&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">enable&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">server&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">labels&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">role&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">agent&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">role&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">agent&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">agentpool&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">nodepool1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">storageprofile&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">managed&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">storagetier&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">Premium_LRS&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">azure&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">com&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">cluster&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">MC_clusters_weekly&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">22&lt;/span>&lt;span class="n">_northeurope&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">volume&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">plugin&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">dir&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">volumeplugins&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">address&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="mf">0.0&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">allow&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">privileged&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">true&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">anonymous&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">auth&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">false&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">authorization&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">mode&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">Webhook&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">azure&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">container&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">azure&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">json&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">cgroups&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">per&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">qos&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">true&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">client&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">ca&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">file&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">certs&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">ca&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">crt&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">cloud&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">azure&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">json&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">cloud&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">provider&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">azure&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">cluster&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">dns&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">10.2&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="mf">0.10&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">cluster&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">domain&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">cluster&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">enforce&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">allocatable&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">pods&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">event&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">qps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">eviction&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">hard&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">memory&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">available&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="mi">750&lt;/span>&lt;span class="n">Mi&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">nodefs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">available&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="o">%&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">nodefs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inodesFree&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="o">%&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">feature&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">gates&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">PodPriority&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">true&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">RotateKubeletServerCertificate&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">true&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">gc&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">high&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">threshold&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">85&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">gc&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">low&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">threshold&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">80&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">pull&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">progress&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">deadline&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">30&lt;/span>&lt;span class="n">m&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">keep&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">terminated&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">pod&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">volumes&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">false&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">kube&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">reserved&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">cpu&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">60&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">memory&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">896&lt;/span>&lt;span class="n">Mi&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">kubeconfig&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="k">var&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">lib&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubelet&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubeconfig&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="nb">max&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">pods&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">110&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">network&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">plugin&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">cni&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">status&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">update&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">frequency&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="n">s&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">non&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">masquerade&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">cidr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">pod&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">infra&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">container&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">image&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">k8s&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gcr&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">pause&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">amd64&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mf">3.1&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">pod&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">manifest&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">path&lt;/span>&lt;span class="o">=/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">kubernetes&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">manifests&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">pod&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="nb">max&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">pids&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">rotate&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">certificates&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">false&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">streaming&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">connection&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">idle&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">timeout&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="n">m&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>To log in to the console of a node, go to the MC_resourcegroup_clustername_region resource-group and select the VM. Then go to &lt;code>Boot diagnostics&lt;/code> and enable it. Go to &lt;code>Reset password&lt;/code> to create yourself a user and then &lt;code>Serial console&lt;/code> to log in and execute commands.&lt;/p>
&lt;/blockquote>
&lt;p>We can see &lt;code>--kube-reserved=cpu=60m,memory=896Mi&lt;/code> and &lt;code>--eviction-hard=memory.available&amp;lt;750Mi&lt;/code> which adds up to &lt;code>1646Mi&lt;/code> which is pretty close to the &lt;code>1685Mi&lt;/code> that was the gap between Capacity and Allocatable.&lt;/p>
&lt;p>We also do this on a &lt;code>Standard DS2 v2&lt;/code> node and get &lt;code>--kube-reserved=cpu=69m,memory=1638Mi&lt;/code> and &lt;code>--eviction-hard=memory.available&amp;lt;750Mi&lt;/code>.&lt;/p>
&lt;p>So we can see that the memory of &lt;code>kube-reserved&lt;/code> grows almost linearly and seems to always be about 20-25% while CPU reservations are almost the same. The memory eviction buffer is always fixed at &lt;code>750Mi&lt;/code> which would mean bigger resource waste as nodes decrease in size.&lt;/p>
&lt;h4 id="cpu">CPU
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: right">Standard DS1 v2&lt;/th>
&lt;th style="text-align: right">Standard DS2 v2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>VM capacity&lt;/td>
&lt;td style="text-align: right">1.000m&lt;/td>
&lt;td style="text-align: right">2.000m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-reserved&lt;/td>
&lt;td style="text-align: right">-60m&lt;/td>
&lt;td style="text-align: right">-69m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Allocatable&lt;/td>
&lt;td style="text-align: right">940m&lt;/td>
&lt;td style="text-align: right">1.931m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Allocatable %&lt;/td>
&lt;td style="text-align: right">94%&lt;/td>
&lt;td style="text-align: right">96.5%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="memory">Memory
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: right">Standard DS1 v2&lt;/th>
&lt;th style="text-align: right">Standard DS2 v2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>VM capacity&lt;/td>
&lt;td style="text-align: right">3.500Mi&lt;/td>
&lt;td style="text-align: right">7.113Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-reserved&lt;/td>
&lt;td style="text-align: right">-896Mi&lt;/td>
&lt;td style="text-align: right">-1.638Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Eviction buf&lt;/td>
&lt;td style="text-align: right">-750Mi&lt;/td>
&lt;td style="text-align: right">-750Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Allocatable&lt;/td>
&lt;td style="text-align: right">1.814Mi&lt;/td>
&lt;td style="text-align: right">4.667Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Allocatable %&lt;/td>
&lt;td style="text-align: right">52%&lt;/td>
&lt;td style="text-align: right">65%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="node-pods-daemonsets">Node pods (DaemonSets)
&lt;/h2>&lt;p>We have some Pods that run on every node, and they are installed by default by AKS. We get the resource limits of these by describing either the pods or the daemonsets.&lt;/p>
&lt;h4 id="cpu-1">CPU
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: right">Standard DS1 v2&lt;/th>
&lt;th style="text-align: right">Standard DS2 v2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Allocatable&lt;/td>
&lt;td style="text-align: right">940m&lt;/td>
&lt;td style="text-align: right">1.931m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/calico-node&lt;/td>
&lt;td style="text-align: right">-250m&lt;/td>
&lt;td style="text-align: right">-250m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/kube-proxy&lt;/td>
&lt;td style="text-align: right">-100m&lt;/td>
&lt;td style="text-align: right">-100m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/kube-svc-redirect&lt;/td>
&lt;td style="text-align: right">-5m&lt;/td>
&lt;td style="text-align: right">-5m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available&lt;/td>
&lt;td style="text-align: right">585m&lt;/td>
&lt;td style="text-align: right">1.576m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available %&lt;/td>
&lt;td style="text-align: right">58%&lt;/td>
&lt;td style="text-align: right">81%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="memory-1">Memory
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th style="text-align: right">Standard DS1 v2&lt;/th>
&lt;th style="text-align: right">Standard DS2 v2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Allocatable&lt;/td>
&lt;td style="text-align: right">1.814Mi&lt;/td>
&lt;td style="text-align: right">4.667Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/kube-svc-redirect&lt;/td>
&lt;td style="text-align: right">-32Mi&lt;/td>
&lt;td style="text-align: right">-32Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available&lt;/td>
&lt;td style="text-align: right">1.782Mi&lt;/td>
&lt;td style="text-align: right">4.635Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available %&lt;/td>
&lt;td style="text-align: right">50%&lt;/td>
&lt;td style="text-align: right">61%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>So for &lt;code>Standard DS1 v2&lt;/code> nodes we have about 0.5 CPU and 1.7GiB memory per node for pods. And for &lt;code>Standard DS2 v2&lt;/code> nodes it&amp;rsquo;s about 1.5 CPU and 4.6GiB memory.&lt;/p>
&lt;h2 id="kube-system-pods">kube-system pods
&lt;/h2>&lt;p>Now lets add some standard Kubernetes pods we need to run. As far as I know these are pretty much fixed for a cluster and not related to node size or count.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Deployment&lt;/th>
&lt;th style="text-align: right">CPU&lt;/th>
&lt;th style="text-align: right">Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>kube-system/kubernetes-dashboard&lt;/td>
&lt;td style="text-align: right">100m&lt;/td>
&lt;td style="text-align: right">50Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/tunnelfront&lt;/td>
&lt;td style="text-align: right">10m&lt;/td>
&lt;td style="text-align: right">64Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/coredns (x2)&lt;/td>
&lt;td style="text-align: right">200m&lt;/td>
&lt;td style="text-align: right">140Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/coredns-autoscaler&lt;/td>
&lt;td style="text-align: right">20m&lt;/td>
&lt;td style="text-align: right">10Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-system/heapster&lt;/td>
&lt;td style="text-align: right">130m&lt;/td>
&lt;td style="text-align: right">230Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sum&lt;/td>
&lt;td style="text-align: right">460m&lt;/td>
&lt;td style="text-align: right">494Mi&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="third-party-pods">Third party pods
&lt;/h2>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Deployment&lt;/th>
&lt;th style="text-align: right">CPU&lt;/th>
&lt;th style="text-align: right">Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>grafana&lt;/td>
&lt;td style="text-align: right">200m&lt;/td>
&lt;td style="text-align: right">500Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>prometheus-operator&lt;/td>
&lt;td style="text-align: right">500m&lt;/td>
&lt;td style="text-align: right">1.000Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>prometheus-alertmanager&lt;/td>
&lt;td style="text-align: right">100m&lt;/td>
&lt;td style="text-align: right">225Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>flux&lt;/td>
&lt;td style="text-align: right">50m&lt;/td>
&lt;td style="text-align: right">64Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>flux-helm-operator&lt;/td>
&lt;td style="text-align: right">50m&lt;/td>
&lt;td style="text-align: right">64Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sum&lt;/td>
&lt;td style="text-align: right">900m&lt;/td>
&lt;td style="text-align: right">1.853Mi&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="radix-platform-pods">Radix platform pods
&lt;/h2>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Deployment&lt;/th>
&lt;th style="text-align: right">CPU&lt;/th>
&lt;th style="text-align: right">Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>radix-api-prod/server (x2)&lt;/td>
&lt;td style="text-align: right">200m&lt;/td>
&lt;td style="text-align: right">400Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-api-qa/server (x2)&lt;/td>
&lt;td style="text-align: right">100m&lt;/td>
&lt;td style="text-align: right">200Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-canary-golang-dev/www&lt;/td>
&lt;td style="text-align: right">40m&lt;/td>
&lt;td style="text-align: right">500Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-canary-golang-prod/www&lt;/td>
&lt;td style="text-align: right">40m&lt;/td>
&lt;td style="text-align: right">500Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-platform-prod/public-site&lt;/td>
&lt;td style="text-align: right">5m&lt;/td>
&lt;td style="text-align: right">10Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-web-console-prod/web&lt;/td>
&lt;td style="text-align: right">10m&lt;/td>
&lt;td style="text-align: right">42Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-web-console-qa/web&lt;/td>
&lt;td style="text-align: right">5m&lt;/td>
&lt;td style="text-align: right">21Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-github-webhook-prod/webhook&lt;/td>
&lt;td style="text-align: right">10m&lt;/td>
&lt;td style="text-align: right">30Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-github-webhook-prod/webhook&lt;/td>
&lt;td style="text-align: right">5m&lt;/td>
&lt;td style="text-align: right">15Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sum&lt;/td>
&lt;td style="text-align: right">415m&lt;/td>
&lt;td style="text-align: right">1.718Mi&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>If we add up the resource usage of these groups of workloads and see the total available resources on our 4 node Standard DS1 v2 clusters we are left with 0.56 CPU cores (14%) and 3GB of memory (22%):&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Workload&lt;/th>
&lt;th style="text-align: right">CPU&lt;/th>
&lt;th style="text-align: right">Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>kube-system&lt;/td>
&lt;td style="text-align: right">460m&lt;/td>
&lt;td style="text-align: right">494Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>third-party&lt;/td>
&lt;td style="text-align: right">900m&lt;/td>
&lt;td style="text-align: right">1.853Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>radix-platform&lt;/td>
&lt;td style="text-align: right">415m&lt;/td>
&lt;td style="text-align: right">1.718Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sum&lt;/td>
&lt;td style="text-align: right">1.760m&lt;/td>
&lt;td style="text-align: right">4.020Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available on 4x DS1&lt;/td>
&lt;td style="text-align: right">2.340m&lt;/td>
&lt;td style="text-align: right">7.128Mi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Available for workloads&lt;/td>
&lt;td style="text-align: right">565m&lt;/td>
&lt;td style="text-align: right">3.063Mi&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Though surprising that we lost this much resources before being able to deploy our actual customer applications, it should still be a bit of headroom.&lt;/p>
&lt;p>Going further I checked the resource requests on 8 customer pods deployed in 4 environments (namespaces). Even though none of them had a resource configuration in their &lt;code>radixconfig.yaml&lt;/code> files they still had resource requests and limits. Not surprising since we use LimitRange to set default resource requests and limits. The surprise was that half of them had 50Mi of memory and the other half 500Mi, seemingly at random.&lt;/p>
&lt;p>It turns out that we did an update to the LimitRange values a few days ago but that only applies to new Pods, so depending on if the Pods got re-created for any reason they may or may not have the old request of 500Mi, which in our case of small clusters will quickly drain the available resources.&lt;/p>
&lt;blockquote>
&lt;p>Read more about LimitRange here: &lt;a class="link" href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/" target="_blank" rel="noopener"
>kubernetes.io&lt;/a> , and here is the commit that eventually trickled down to reduce memory usage: &lt;a class="link" href="https://github.com/equinor/radix-operator/commit/f022fcde993efdf6cbcafb2c6632707a823a2a27" target="_blank" rel="noopener"
>github.com&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h2 id="pod-scheduling">Pod scheduling
&lt;/h2>&lt;p>Depending on the weight between CPU and memory requests and how often things get destroyed and re-created you may find yourself in a situation where you have enough resources in your cluster but new workloads are still Pending. This can happen when one resource type (e.g. CPU) is filled before another (e.g. memory), leading one or more resources to be stranded and unlikely to be utilized.&lt;/p>
&lt;p>Imagine for example a cluster that is already utilized like this:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>CPU&lt;/th>
&lt;th>Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>node0&lt;/td>
&lt;td>94%&lt;/td>
&lt;td>86%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>node1&lt;/td>
&lt;td>80%&lt;/td>
&lt;td>89%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>node2&lt;/td>
&lt;td>98%&lt;/td>
&lt;td>60%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Scheduling a workload that requests 15% CPU and 20% memory cannot be scheduled since there are no nodes fulfilling both requirements. In theory there is probably a CPU intensive Pod on node2 that could be moved to node1 but Kubernetes does not do re-scheduling to optimize utilization. It can do re-scheduling based on Pod priority (&lt;a class="link" href="https://medium.com/@dominik.tornow/the-kubernetes-scheduler-cd429abac02f" target="_blank" rel="noopener"
>medium.com&lt;/a>) and there is an incubator project (&lt;a class="link" href="https://akomljen.com/meet-a-kubernetes-descheduler/" target="_blank" rel="noopener"
>akomljen.com&lt;/a>) that can try to drain nodes with low utilization.&lt;/p>
&lt;p>So for the foreseable future keeping in mind that resources can get stranded and that looking at the sum of cluster resources and sum of cluster resource demand might be misleading.&lt;/p>
&lt;h2 id="calico-node">calico-node
&lt;/h2>&lt;p>The biggest source of waste on our small clusters is &lt;code>calico-node&lt;/code> which is installed on every node and requests 25% of a CPU core while only using 2.5-3% CPU:&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu.png"
width="1291"
height="392"
srcset="https://demo.stack.jimmycai.com/p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu_hu1477443119961749415.png 480w, https://demo.stack.jimmycai.com/p/mini-post-down-scaling-azure-kubernetes-service-aks/calico-node-cpu_hu8581044549333248792.png 1024w"
loading="lazy"
alt="calico-node cpu usage"
class="gallery-image"
data-flex-grow="329"
data-flex-basis="790px"
>&lt;/p>
&lt;p>The request is originally set here &lt;a class="link" href="https://github.com/Azure/aks-engine/blob/master/parts/k8s/containeraddons/kubernetesmasteraddons-calico-daemonset.yaml" target="_blank" rel="noopener"
>github.com&lt;/a> but I have not got into why that number was choosen. Next steps would be to do some benchmarking of &lt;code>calico-node&lt;/code> to smoke out it&amp;rsquo;s performance characteristics to see if it would be safe to lower the resource requests, but that is out of scope for now.&lt;/p>
&lt;h1 id="conclusion">Conclusion
&lt;/h1>&lt;ul>
&lt;li>By increasing node size from &lt;code>Standard DS1 v2&lt;/code> to &lt;code>Standard DS2 v2&lt;/code> we also increase the available CPU from 58% per node to 81% per node. Available memory increases from 50% to 61% per node.&lt;/li>
&lt;li>With a total platform requirement of 3-4GB of memory and 4.6GB available on &lt;code>Standard DS2 v2&lt;/code> we might have more resources for actual workloads on a 1-node &lt;code>Standard DS2 v2&lt;/code> cluster than a 3-node &lt;code>Standard DS1 v2&lt;/code> cluster!&lt;/li>
&lt;li>Beware of stranded resources limiting the utilization you can achieve across a cluster.&lt;/li>
&lt;/ul></description></item><item><title>Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking</title><link>https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/</link><pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate><guid>https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/</guid><description>&lt;img src="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1.png" alt="Featured image of post Disk performance on Azure Kubernetes Service (AKS) - Part 1: Benchmarking" />&lt;p>Understanding the characteristics of disk performance of a platform might be more important than you think. If disk resources are not correctly matched to your workload, your performance will suffer and might lead you to incorrectly diagnose a problem as being related to CPU or memory.&lt;/p>
&lt;p>The defaults might also not give you the performance you expect.&lt;/p>
&lt;p>In this first post on troubleshooting some disk performance issues on Azure Kubernetes Service (AKS) we will benchmark Azure Premium SSD to find how workloads affect performance and which metrics to monitor to know when troubleshooting potential disk issues.&lt;/p>
&lt;p>TLDR:&lt;/p>
&lt;ul>
&lt;li>Disable Azure cache for workloads with high number of random writes&lt;/li>
&lt;li>Use a P15 (256GB) or larger Premium SSD even though you might only need a fraction of it.&lt;/li>
&lt;/ul>
&lt;p>Table of contents&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="#Background" >Background&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#MetricsMethodologies" >Metric Methodologies&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#StorageBackground" >Storage Background&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#WhatToMeasure" >What to measure?&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#HowToMeasureDisk" >How to measure disk&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="HowToMeasureDiskOnAKS" >How to measure disk on Azure Kubernetes Service&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Tests" >Test results&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Test1" >Test 1 - Learning to dislike Azure Cache&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test2" >Test 2 - Disable Azure Cache - enable OS cache&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test3" >Test 3 - Disable OS cache&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test4" >Test 4 - Increase IO depth&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test5" >Test 5 - Larger block size, smaller IO depth&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test6" >Test 6 - Enable OS cache&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test7" >Test 7 - Random writes, small block size&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Test8" >Test 8 - Large block size&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Conclusion" >Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="microsoft-azure">Microsoft Azure
&lt;/h4>&lt;blockquote>
&lt;p>&lt;a class="link" href="https://azure.microsoft.com/en-us/free/" target="_blank" rel="noopener"
>If you don&amp;rsquo;t have a Azure subscription already you can try services for $200 for 30 days.&lt;/a> The VM size &lt;strong>Standard_B2s&lt;/strong> is Burstable, has 2vCPU, 4GB RAM, 8GB temp storage and costs roughly $38 / month. For $200 you can have a cluster of 3-4 B2s nodes plus traffic, loadbalancers and other additional costs.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>See my blog post &lt;a class="link" href="2017-12-23-managed-kubernetes-on-azure.md" >Managed Kubernetes on Microsoft Azure (English)&lt;/a> for information on how to get up and running with Kubernetes on Azure.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;em>I have no affiliation with Microsoft Azure except using them through work.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="corrections">Corrections
&lt;/h2>&lt;p>&lt;strong>February 2020&lt;/strong>: Some of my previous knowledge and assumptions were not correct when applied to a cloud + Docker environment, as &lt;a class="link" href="https://github.com/jnoller/kubernaughty/issues/46" target="_blank" rel="noopener"
>explained by
AKS PM Jesse Noller on GitHub&lt;/a>.&lt;/p>
&lt;p>One of the issues is that even accessing a &amp;ldquo;data disk&amp;rdquo; will incur IOPS on the OS disk, and throttling of the OS disk will also constraint IOPS on the data disks.&lt;/p>
&lt;p>&lt;a id="Background">&lt;/a>&lt;/p>
&lt;h2 id="background">Background
&lt;/h2>&lt;p>I&amp;rsquo;m part of a team at Equinor building an internal PaaS based on Kubernetes running on AKS (Azure managed Kubernetes). We use Prometheus for monitoring each cluster as well as InfluxDB for collecting metrics from k6io which runs continous tests on our public endpoints.&lt;/p>
&lt;p>A couple of weeks ago we discovered some potential problems with both Prometheus and InfluxDB with memory usage and restarts. High CPU usage of type &lt;code>iowait&lt;/code> suggested that there might be some disk issues contributing to the problems.&lt;/p>
&lt;blockquote>
&lt;p>iowait: &amp;ldquo;Percentage of time that the CPU or CPUs were idle during which the system had an outstanding disk I/O request.&amp;rdquo; (&lt;a class="link" href="https://support.hpe.com/hpsc/doc/public/display?docId=c02783994" target="_blank" rel="noopener"
>hpe.com&lt;/a>). You can see &lt;code>iowait&lt;/code> on your Linux system by running &lt;code>top&lt;/code> and looking at the &lt;code>wa&lt;/code> percentage.&lt;/p>
&lt;p>PS: You can have a disk IO bottleneck even with low &lt;code>iowait&lt;/code>, and a high &lt;code>iowait&lt;/code> does not always indicate a disk IO bottleneck (&lt;a class="link" href="https://www.ibm.com/developerworks/community/blogs/AIXDownUnder/entry/iowait_a_misleading_indicator_of_i_o_performance54?lang=en" target="_blank" rel="noopener"
>ibm.com&lt;/a>).&lt;/p>
&lt;/blockquote>
&lt;p>First off we need to benchmark the underlying disk to get an understanding of it&amp;rsquo;s performance limits and characteristics. That is what we will cover in this post.&lt;/p>
&lt;p>&lt;a id="MetricsMethodologies">&lt;/a>&lt;/p>
&lt;h3 id="metric-methodologies">Metric Methodologies
&lt;/h3>&lt;p>There are two helpful methodologies when monitoring information systems. The first one is Utilization, Saturation and Errors (USE) from &lt;a class="link" href="http://www.brendangregg.com/usemethod.html" target="_blank" rel="noopener"
>Brendan Gregg&lt;/a> and the second one is Rate, Errors, Duration (RED) from &lt;a class="link" href="https://www.slideshare.net/weaveworks/monitoring-microservices" target="_blank" rel="noopener"
>Tom Wilkie&lt;/a>. RED is best suited when observing workloads and transactions while USE is best suited for observing resources.&lt;/p>
&lt;p>I&amp;rsquo;ll be using the USE method here. USE can be summarised as:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>For every resource, check utilization, saturation, and errors.&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>resource&lt;/strong>: all physical server functional components (CPUs, disks, busses, &amp;hellip;)&lt;/li>
&lt;li>&lt;strong>utilization&lt;/strong>: the average time that the resource was busy servicing work&lt;/li>
&lt;li>&lt;strong>saturation&lt;/strong>: the degree to which the resource has extra work which it can&amp;rsquo;t service, often queued&lt;/li>
&lt;li>&lt;strong>errors&lt;/strong>: the count of error events&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="StorageBackground">&lt;/a>&lt;/p>
&lt;h3 id="storage-background">Storage Background
&lt;/h3>&lt;p>Disk usage has two dimensions, throughput/bandwidth(BW) and operations per second (IOPS), and the underlying storage system will have upper limits of how much data it can receive (BW) and the number of operations it can perform per second (IOPS).&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Background - harddrive types&lt;/strong>: harddrives come in two types, Solid State Disks (SSD) and spindle (HDD). A SSD disk is a microship capable of permanently storing data while a HDD uses spinning platters to store data. HDDs have a fixed rate of rotation (RPM), typically 5.400 and 7.200 RPM for lower cost drives for home use and higher cost 10.000 and 15.000 RPM drives for server use. Over the last 20 years of HDDs their storage density has increased, but the RPM has largely stayed the same. A disk with twice the density (500GB to 1TB for example) can read twice as much data on a single rotation and thus increase the bandwidth significantly. However, reading or writing a random block still requires waiting for the disk to spin enough to reach the relevant sector on the disk. So IOPS has not increased much for HDDs and is still a low 125-150 IOPS for a 10.000 RPM enterprise disk. A SSD does not have any moving parts so is able to reach MUCH higher IOPS. A low end Samsung 960 EVO with 500GB capacity costs $150 and can achieve a whopping 330.000 IOPS! (&lt;a class="link" href="https://en.wikipedia.org/wiki/IOPS" target="_blank" rel="noopener"
>wikipedia.com&lt;/a>)&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Background - access patterns&lt;/strong>: The way a program uses storage also has a huge impact on the performance one can achieve. Sequential access is when we read or write a large file. When this happens the operating system and harddrive can optimize and &amp;ldquo;merge&amp;rdquo; operations so that we can read or write a much bigger chunk of data at a time. If we can read 1MB at a time 150 times per second we get 150MB/s of bandwidth. However, fully random access where the smallest chunk we read or write is a 4KB block the same 150 IOPS would only give a bandwidth of 0.6MB/s!&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Background - cloud vs physical&lt;/strong>: Now we know what HDDs are limited to a low IOPS and low IOPS combined with a random access pattern gives us a low overall bandwidth. There is a huge gotcha here when it comes to cloud. On Azure when using Premium Managed SSD the IOPS you are given is a factor of the disk size you provision (&lt;a class="link" href="https://azure.microsoft.com/en-us/pricing/details/managed-disks/" target="_blank" rel="noopener"
>microsoft.com&lt;/a>). A 512GB disk is limited to 2.300 IOPS and 150MB/s. With 100% random access that only gives about 9MB/s of bandwidth!&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Background - OS caching&lt;/strong>: To overcome some of the limitations of the underlying disk (mostly IOPS) there are potentially several layers of caching involved. Linux file systems can have &lt;code>writeback&lt;/code> enabled which causes Linux to temporarily store data that is going to be written to disk in memory. This can give a big performance increase when there are sudden spikes of writes exceeding the performance of the underlying disk. It also increases the chance that operations can be &lt;code>merged&lt;/code> where several write operations to areas of the disk that are nearby can be executed as one. This caching works best for sudden peaks and will not necessarily be enough if there is continous random writes to disk. This caching also means that even though an application thinks it has saved some data to disk it can be lost in the case of a power outage or other failure. Applications can also explicitly request &lt;code>direct&lt;/code> access where every operation is persisted to disk before receiving a confirmation. This is a trade-off between performance and durability that needs to be decided based on the application itself and the environment.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Background - Azure caching&lt;/strong>: Azure also provides read and write cache for its &lt;code>disks&lt;/code> which is enabled by default. As we will see soon for our use case it&amp;rsquo;s not a good idea to use.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="WhatToMeasure">&lt;/a>&lt;/p>
&lt;h2 id="what-to-measure">What to measure?
&lt;/h2>&lt;blockquote>
&lt;p>These metrics are collected by the Prometheus &lt;code>node-exporter&lt;/code> and follows it&amp;rsquo;s naming. I&amp;rsquo;ve also created a dashboard that is available on &lt;a class="link" href="https://grafana.com/dashboards/9852" target="_blank" rel="noopener"
>Grafana.com&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>With the USE methodology as a guideline and the two separate but related &amp;ldquo;resources&amp;rdquo;, bandwidth and IOPS we can look for some useful metrics.&lt;/p>
&lt;p>Utilization:&lt;/p>
&lt;ul>
&lt;li>&lt;code>rate(node_disk_written_bytes_total)&lt;/code> - Write bandwidth. The maximum is given by Azure and is 25MB/s for our disk size.&lt;/li>
&lt;li>&lt;code>rate(node_disk_writes_completed_total)&lt;/code> - Write operations. The maximum is given by Azure and is 120 IOPS for our disk size.&lt;/li>
&lt;li>&lt;code>rate(node_disk_io_time_seconds_total)&lt;/code> - Disk active time in percent. The time the disk was busy servicing requests. 100% means fully utilized.&lt;/li>
&lt;/ul>
&lt;p>Saturation:&lt;/p>
&lt;ul>
&lt;li>&lt;code>rate(node_cpu_seconds_total{mode=&amp;quot;iowait&amp;quot;}&lt;/code> - CPU iowait. The percentage of time a CPU core is blocked from doing useful work because it&amp;rsquo;s waiting for an IO operation to complete (typically disk, but can also be network).&lt;/li>
&lt;/ul>
&lt;p>Useful calculated metrics:&lt;/p>
&lt;ul>
&lt;li>&lt;code>rate(node_disk_write_time_seconds_total) / rate(node_disk_writes_completed_total)&lt;/code> - Write latency. How long from a write is requested until it&amp;rsquo;s completed.&lt;/li>
&lt;li>&lt;code>rate(node_disk_written_bytes_total) / rate(node_disk_writes_completed_total)&lt;/code> - Write size. How big the &lt;strong>average&lt;/strong> write operation is. 4KB is minimum and indicates 100% random access while 512KB is maximum and indicates sequential access.&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="HowToMeasureDisk">&lt;/a>&lt;/p>
&lt;h2 id="how-to-measure-disk">How to measure disk
&lt;/h2>&lt;p>The best tool for measuring disk performance is &lt;code>fio&lt;/code>, even though it might seem a bit intimidating at first due to it&amp;rsquo;s insane number of options.&lt;/p>
&lt;p>Installing &lt;code>fio&lt;/code> on Ubuntu:&lt;/p>
&lt;pre>&lt;code>apt-get install fio
&lt;/code>&lt;/pre>
&lt;p>&lt;code>fio&lt;/code> executes &lt;code>jobs&lt;/code> described in a file. Here is the top of our jobs file:&lt;/p>
&lt;pre>&lt;code>[global]
ioengine=libaio # sync|libaio|mmap
group_reporting
thread
size=10g # Size of test file
cpus_allowed=1 # Only use this CPU core
runtime=300s # Run test for 5 minutes
[test1]
filename=/tmp/fio-test-file
direct=1 # If value is true, use non-buffered I/O. Non-buffered I/O usually means O_DIRECT
readwrite=write # read|write|randread|randwrite|readwrite|randrw
iodepth=1 # How many operations to queue to the disk
blocksize=4k
&lt;/code>&lt;/pre>
&lt;p>The fields we will be changing for the various tests are &lt;code>direct&lt;/code>, &lt;code>readwrite&lt;/code>, &lt;code>iodepth&lt;/code> and &lt;code>blocksize&lt;/code>. Save the contents in a file named &lt;code>jobs.fio&lt;/code> and we run a test with &lt;code>fio --sector test1 jobs.fio&lt;/code> and wait until the test completes.&lt;/p>
&lt;blockquote>
&lt;p>PS: To run these tests on higher performance hardware and better caching you might want to set &lt;code>runtime&lt;/code> to &lt;code>0&lt;/code> to have the test run continously and monitor the metrics until performance reaches a steady-state.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="HowToMeasureDiskOnAKS">&lt;/a>&lt;/p>
&lt;h2 id="how-to-measure-disk-on-azure-kubernetes-service">How to measure disk on Azure Kubernetes Service
&lt;/h2>&lt;p>For this testing we use a standard Prometheus installation collecting data from &lt;code>node-exporter&lt;/code> and visualizing data in Grafana. The dashboard I created for the testing can be found here: &lt;a class="link" href="https://grafana.com/dashboards/9852" target="_blank" rel="noopener"
>https://grafana.com/dashboards/9852&lt;/a>.&lt;/p>
&lt;p>By default Kubernetes will schedule a Pod to any node that has enough memory and CPU for our workload. Since one of the tests we are going to run are on the OS disk we do not want the Pod to run on the same node as any other disk-intensive application, such as Prometheus.&lt;/p>
&lt;p>Look at which Pods are running with &lt;code>kubectl get pods -o wide&lt;/code> and look for a node that does not have any disk-intensive application.&lt;/p>
&lt;p>Then we tag that node with &lt;code>kubectl label nodes aks-nodepool1-37707184-2 tag=disktest&lt;/code>. This allows us later to specify that we want to run our testing Pod on that specific node.&lt;/p>
&lt;hr>
&lt;p>A StorageClass in Kubernetes is a specification of a underlying disk that Pods can request usage of through &lt;code>volumeClaimTemplates&lt;/code>. AKS comes with a default StorageClass &lt;code>managed-premium&lt;/code> that has caching enabled. Most of these tests require the Azure cache disabled so create a new StorageClass &lt;code>managed-premium-retain-nocache&lt;/code>:&lt;/p>
&lt;pre>&lt;code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: managed-premium-retain-nocache
provisioner: kubernetes.io/azure-disk
reclaimPolicy: Retain
parameters:
storageaccounttype: Premium_LRS
kind: Managed
cachingmode: None
&lt;/code>&lt;/pre>
&lt;p>You can add it to your cluster with:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2019-02-23-disk-performance-on-aks-part-1/storageclass.yaml
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>Next we create a StatefulSet that uses a &lt;code>volumeClaimTemplate&lt;/code> to request a 250GB Azure disk. This provisions a P15 Azure Premium SSD with 125MB/s bandwidth and 1100 IOPS:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2019-02-23-disk-performance-on-aks-part-1/ubuntu-statefulset.yaml
&lt;/code>&lt;/pre>
&lt;p>Follow the progress of the Pod creation with &lt;code>kubectl get pods -w&lt;/code> and wait until it is &lt;code>Running&lt;/code>.&lt;/p>
&lt;hr>
&lt;p>When the Pod is &lt;code>Running&lt;/code> we can start a shell on it with &lt;code>kubectl exec -it disk-test-0 bash&lt;/code>&lt;/p>
&lt;p>Once inside &lt;code>bash&lt;/code> on the Pod, we install &lt;code>fio&lt;/code>:&lt;/p>
&lt;pre>&lt;code>apt-get update &amp;amp;&amp;amp; apt-get install -y fio wget
&lt;/code>&lt;/pre>
&lt;p>And save the contents of in the Pod:&lt;/p>
&lt;pre>&lt;code>wget https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2019-02-23-disk-performance-on-aks-part-1/jobs.fio
&lt;/code>&lt;/pre>
&lt;p>Now we can run the different test sections one by one. &lt;strong>PS: If you don&amp;rsquo;t specify a section &lt;code>fio&lt;/code> will run all the tests &lt;em>simultaneously&lt;/em>, which is not what we want.&lt;/strong>&lt;/p>
&lt;pre>&lt;code>fio --section=test1 jobs.fio
fio --section=test2 jobs.fio
fio --section=test3 jobs.fio
fio --section=test4 jobs.fio
fio --section=test5 jobs.fio
fio --section=test6 jobs.fio
fio --section=test7 jobs.fio
fio --section=test8 jobs.fio
fio --section=test9 jobs.fio
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="Tests">&lt;/a>&lt;/p>
&lt;h2 id="test-results">Test results
&lt;/h2>&lt;p>&lt;a id="Test1">&lt;/a>&lt;/p>
&lt;h3 id="test-1---learning-to-dislike-azure-cache">Test 1 - Learning to dislike Azure Cache
&lt;/h3>&lt;p>&lt;em>Sequential write, 4K block size, Azure Cache enabled, OS cache disabled. See &lt;a class="link" href="https://demo.stack.jimmycai.com/attachments/2019-02-23-disk-performance-on-aks-part-1/test1.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>I run the first tests on the OS disk of a Kubernetes node. The OS disks have Azure caching enabled.&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1.png"
width="1697"
height="1232"
srcset="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1_hu1499039068193334488.png 480w, https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test1_hu13957841185659198382.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="330px"
>&lt;/p>
&lt;p>The first 1-2 minutes of the test I get very good performance of 45MB/s and ~11.500 IOPS but that drops to 0 very quickly as the cache is full and busy writing things to the underlying disk. When that happens everything freezes and I cannot even execute shell commands. After stopping the test the system still hangs for a bit while the cache empties.&lt;/p>
&lt;p>The maximum latency measured by &lt;code>fio&lt;/code> was 108751k usec. Or about 108 seconds!&lt;/p>
&lt;blockquote>
&lt;p>For the first try of these tests a 20-30 second period of very fast writes (250MB/s) caused a 7-8 minutes hang while the cache emptied. Trying again caused another pattern of lower peak performance with shorter hangs in between. Very unpredictable.
I&amp;rsquo;m not sure what to make of this. It&amp;rsquo;s not acceptable that a Kubernetes node becomes unresponsive for many minutes following a short burst of writing. There are scattered recommendations online of disabling caching for write-heavy applications. Since I have not found any way to measure the Azure cache itself, the results are unpredictable and potentially very impactful as well as making it very hard to use the metrics we do have to evaluate application and storage behaviour I&amp;rsquo;ve concluded that it&amp;rsquo;s best to use data disks with caching disabled for our workloads (you cannot disable caching on an AKS node OS disk).&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test2">&lt;/a>&lt;/p>
&lt;h3 id="test-2---disable-azure-cache---enable-os-cache">Test 2 - Disable Azure Cache - enable OS cache
&lt;/h3>&lt;p>&lt;em>Sequential write, 4K block size. &lt;strong>Change: Azure cache disabled, OS caching enabled.&lt;/strong> See &lt;a class="link" href="https://demo.stack.jimmycai.com/attachments/2019-02-23-disk-performance-on-aks-part-1/test2.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test2.png"
width="1703"
height="1226"
srcset="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test2_hu9653487243883720288.png 480w, https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test2_hu12369574169741432704.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="333px"
>&lt;/p>
&lt;p>If we swap the Azure cache for the Linux OS cache we see that &lt;code>iowait&lt;/code> increases while the writing occurs. The application sees high write performance until the number of &lt;code>Dirty bytes&lt;/code> reaches a threshold of about 3.7GB of memory. The performance of the underlying disk is 125MB/s and 250 IOPS. Here we are throttled by the 125MB/s limit of the Azure P15 Premium SSD.&lt;/p>
&lt;p>Also notice that on sequential writes of 4K with OS caching the actual blocks written to disk is 512K which saves us a lot of IOPS. This will become important later.&lt;/p>
&lt;p>&lt;a id="Test3">&lt;/a>&lt;/p>
&lt;h3 id="test-3---disable-os-cache">Test 3 - Disable OS cache
&lt;/h3>&lt;p>&lt;em>Sequential write, 4K block size. &lt;strong>Change: OS caching disabled.&lt;/strong> See &lt;a class="link" href="https://demo.stack.jimmycai.com/attachments/2019-02-23-disk-performance-on-aks-part-1/test3.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test3.png"
width="1698"
height="1233"
srcset="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test3_hu9187350742741251895.png 480w, https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test3_hu14645523390977373096.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="330px"
>&lt;/p>
&lt;blockquote>
&lt;p>By disabling the OS cache (&lt;code>direct=1&lt;/code>) the results are consistent and predictable. There is no &lt;code>iowait&lt;/code> since the application does not have multiple writes pending at the same time. Because of the 2-3ms latency of the disks we are not able to get more than about 400 IOPS. This gives us a meager 1.5MB/s even though the disk is limited to 1100 IOPS and 125MB/s. To reach that we need multiple simultaneous writes or a bigger IO depth (queue). &lt;code>Disk active time&lt;/code> is also 0% which indicates that the disk is not saturated.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test4">&lt;/a>&lt;/p>
&lt;h3 id="test-4---increase-io-depth">Test 4 - Increase IO depth
&lt;/h3>&lt;p>&lt;em>Sequential write, 4K block size, OS caching disabled. &lt;strong>Change: IO depth 16.&lt;/strong> See &lt;a class="link" href="https://demo.stack.jimmycai.com/attachments/2019-02-23-disk-performance-on-aks-part-1/test4.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test4.png"
width="1698"
height="1237"
srcset="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test4_hu5722226652359263237.png 480w, https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test4_hu2996385930643672719.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="329px"
>&lt;/p>
&lt;blockquote>
&lt;p>For this test we only increase the IO depth from 1 to 16. IO depth is the number of write operations &lt;code>fio&lt;/code> will execute simultaneously. Since we are using &lt;code>direct&lt;/code> these will be queued by the OS for writing. We are now able to hit the performance limit of 1100 IOPS. &lt;code>Disk active time&lt;/code> is now steady at 100% indicating that we have saturated the disk.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test5">&lt;/a>&lt;/p>
&lt;h3 id="test-5---larger-block-size-smaller-io-depth">Test 5 - Larger block size, smaller IO depth
&lt;/h3>&lt;p>&lt;em>Sequential write, OS caching disabled. &lt;strong>Change: 128K block size, IO depth 1.&lt;/strong> See &lt;a class="link" href="https://demo.stack.jimmycai.com/attachments/2019-02-23-disk-performance-on-aks-part-1/test5.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test5.png"
width="1699"
height="1229"
srcset="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test5_hu14954921568768718351.png 480w, https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test5_hu3268361891850381379.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;blockquote>
&lt;p>We increase the block size to 128KB and reduce the IO depth to 1 again. The write latency for larger blocks increase to ~5ms which gives us 200 IOPS and 28MB/s. The disk is not saturated.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test6">&lt;/a>&lt;/p>
&lt;h3 id="test-6---enable-os-cache">Test 6 - Enable OS cache
&lt;/h3>&lt;p>&lt;em>Sequential write, 256K block size, IO depth 1. &lt;strong>Change: OS caching enabled.&lt;/strong> See &lt;a class="link" href="https://demo.stack.jimmycai.com/attachments/2019-02-23-disk-performance-on-aks-part-1/test6.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test6.png"
width="1697"
height="1230"
srcset="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test6_hu5250198874418650337.png 480w, https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test6_hu6421477374997710977.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="331px"
>&lt;/p>
&lt;blockquote>
&lt;p>We have now enabled the OS cache/buffer (&lt;code>direct=0&lt;/code>). We can see that the writes hitting the disk are now merged to 512KB blocks. We are hitting the 125MB/s limit with about 250 IOPS. Enabling the cache also has other effects: CPU suddenly shows significant IO wait. The write latency shoots through the roof. Also note that the writing continued for 30-40 seconds after the test was done. &lt;strong>This also means that the bandwidth and IOPS that &lt;code>fio&lt;/code> sees and reports is higher than what is actually hitting the disk.&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test7">&lt;/a>&lt;/p>
&lt;h3 id="test-7---random-writes-small-block-size">Test 7 - Random writes, small block size
&lt;/h3>&lt;p>&lt;em>IO depth 1, OS caching enabled. &lt;strong>Change: Random write, 4K block size.&lt;/strong> See &lt;a class="link" href="https://demo.stack.jimmycai.com/attachments/2019-02-23-disk-performance-on-aks-part-1/test7.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test7.png"
width="1702"
height="1239"
srcset="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test7_hu9566364161109018395.png 480w, https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test7_hu821024261791687868.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="329px"
>&lt;/p>
&lt;blockquote>
&lt;p>Here we go from sequential writes to random writes. We are limited by IOPS. The average size of the blocks actually written to disks, and the IOPS required to hit the bandwidth limit is actually varying a bit throughout the test. The time taken to empty the cache is about as long as I ran the test (4-5 minutes).&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Test8">&lt;/a>&lt;/p>
&lt;h3 id="test-8---large-block-size">Test 8 - Large block size
&lt;/h3>&lt;p>&lt;em>Random write, OS caching enabled. &lt;strong>Change: 256K block size, IO depth 16.&lt;/strong> See &lt;a class="link" href="https://demo.stack.jimmycai.com/attachments/2019-02-23-disk-performance-on-aks-part-1/test8.md" >full fio test results&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test8.png"
width="1699"
height="1235"
srcset="https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test8_hu1300341601469829731.png 480w, https://demo.stack.jimmycai.com/p/disk-performance-on-azure-kubernetes-service-aks-part-1-benchmarking/test8_hu923727430283491984.png 1024w"
loading="lazy"
alt="graph"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="330px"
>&lt;/p>
&lt;blockquote>
&lt;p>Increasing the block size to 256K makes us bandwidth limited to 125MB/s.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Conclusion">&lt;/a>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>Access patterns and block sizes have a tremendous impact on the amount of data we are able to write to disk.&lt;/p></description></item><item><title>Managed Kubernetes on Microsoft Azure (English)</title><link>https://demo.stack.jimmycai.com/p/managed-kubernetes-on-microsoft-azure-english/</link><pubDate>Fri, 29 Dec 2017 00:00:00 +0000</pubDate><guid>https://demo.stack.jimmycai.com/p/managed-kubernetes-on-microsoft-azure-english/</guid><description>&lt;img src="https://demo.stack.jimmycai.com/p/managed-kubernetes-on-microsoft-azure-english/minecraft-k8s.png" alt="Featured image of post Managed Kubernetes on Microsoft Azure (English)" />&lt;p>&lt;em>A few days ago I wrote a walkthrough of &lt;a class="link" href="https://demo.stack.jimmycai.com/2017/12/25/managed-kubernetes-on-azure.html" >setting up Azure Container Service (AKS) in Norwegian&lt;/a>. Someone asked me for an English version of that, and here it is.&lt;/em>&lt;/p>
&lt;p>Kubernetes(K8s) is becoming the de-facto standard for deploying container-based applications and workloads. Microsoft is currently in preview of their managed Kubernetes offering (Azure Kubernetes Service, AKS) which makes it easy to create a Kubernetes cluster and deploy workloads without the skill and time required to manage day-to-day operations of a Kubernetes-cluster, which today can be complex and time consuming.&lt;/p>
&lt;p>In this post we will set up a Kubernetes cluster from scratch using Azure CLI.&lt;/p>
&lt;p>Table of contents&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="#Background" >Background&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Dockercontainers" >Docker containers&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Containerorchestration" >Container orchestration&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#QuickstartAKS" >Getting started with Azure Kubernetes - AKS&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Caveats" >Caveats&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Preparations" >Preparations&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#AzureLogin" >Azure login&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#ActivateContainerService" >Activate ContainerService&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#CreateResourceGroup" >Create a resource group&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#CreateK8sCluster" >Create a Kubernetes cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#InstallKubectl" >Install kubectl&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#InspectCluster" >Inspect cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#StartNginx" >Start some nginx containere&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#NginxService" >Making nginx available with a service&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#ScaleCluster" >Scale cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#DeleteCluster" >Delete cluster&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Bonusmaterial" >Bonus material&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#HelmIntro" >Deploying services with Helm&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#HelmMinecraft" >Deploy MineCraft with Helm&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#KubernetesDashboard" >Kubernetes Dashboard&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Conclusion" >Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="microsoft-azure">Microsoft Azure
&lt;/h4>&lt;blockquote>
&lt;p>&lt;a class="link" href="https://azure.microsoft.com/en-us/free/" target="_blank" rel="noopener"
>If you don&amp;rsquo;t have a Azure subscription already you can try services for $200 for 30 days.&lt;/a> The VM size &lt;strong>Standard_B2s&lt;/strong> is Burstable, has 2vCPU, 4GB RAM, 8GB temp storage and costs roughly $38 / month. For $200 you can have a cluster of 3-4 B2s nodes plus traffic, loadbalancers and other additional costs.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;em>We have no affiliation with Microsoft Azure except their sponsorship of our startup &lt;a class="link" href="http://www.datadynamics.no/" target="_blank" rel="noopener"
>DataDynamics&lt;/a> with cloud services for 24 months in their &lt;a class="link" href="https://bizspark.microsoft.com/" target="_blank" rel="noopener"
>BizSpark program&lt;/a>.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Background">&lt;/a>&lt;/p>
&lt;h2 id="background">Background
&lt;/h2>&lt;p>&lt;a id="Dockercontainers">&lt;/a>&lt;/p>
&lt;h3 id="docker-containers">Docker containers
&lt;/h3>&lt;p>&lt;em>We will not do a deep dive on Docker containers in this post, but here is a summary for those who are not familiar with it.&lt;/em>&lt;/p>
&lt;p>Docker is a way to package software so that it can run on the most popular platforms without worrying about installation, dependencies and to a certain degree, configuration.&lt;/p>
&lt;p>In addition, a Docker container uses the operating system of the host machine when it runs. Because of this it&amp;rsquo;s possible to run many more containers on the same host machine compared to running virtual machines.&lt;/p>
&lt;p>Here is a incomplete and rough comparison between a Docker container and a virtual machine:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Virtual machine&lt;/th>
&lt;th>Docker container&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Image size&lt;/td>
&lt;td>from 200MB to many GB&lt;/td>
&lt;td>from 10MB to 3-400MB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Startup time&lt;/td>
&lt;td>60 seconds +&lt;/td>
&lt;td>1-10 seconds&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Memory usage&lt;/td>
&lt;td>256MB-512MB-1GB +&lt;/td>
&lt;td>2MB +&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Security&lt;/td>
&lt;td>Good isolation between VMs&lt;/td>
&lt;td>Not as good isolation between containers&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Building image&lt;/td>
&lt;td>Minutes&lt;/td>
&lt;td>Seconds&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> The numbers for virtual machines is taken from memory. I tried starting a MySQL virtual appliance on my laptop but VMware Player refuses to run because of Windows Hyper-V incompatibility. VMware Workstation refuses to run because of license issues and Oracle VirtualBox repeatedly gives me a nasty bluescreen. Hooray!&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong> The smallest and fastest Docker images are built on Alpine Linux. For the webserver Nginx the Alpine-based image is 15MB compared to 108MB for the normal Debian-based image. PostgreSQL:Alpine is 38MB compared to 287MB with &amp;ldquo;full&amp;rdquo; OS. Last version of MySQL is 343MB but will in version 8 support Alpine Linux as well.&lt;/p>
&lt;/blockquote>
&lt;p>To recap, some of the advantages of Docker containers are:&lt;/p>
&lt;ul>
&lt;li>Compatibility across platforms, Linux, Windows, MacOS.&lt;/li>
&lt;li>10-100x smaller size. Faster to download, build and upload.&lt;/li>
&lt;li>Memory usage only for application and not base OS.
&lt;ul>
&lt;li>Advantage when developing. Ability to run 10-20-30 containers on a development laptop.&lt;/li>
&lt;li>Advantage in production. Can reduce hardware/cloud costs considerably.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Near instant startup. Makes dynamic scaling of applications easier.&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://store.docker.com/editions/community/docker-ce-desktop-windows" target="_blank" rel="noopener"
>Download Docker for Windows here.&lt;/a>&lt;/p>
&lt;p>To start a MySQL database container from Windows CMD or Powershell:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">docker run --name mysql -p 3306:3306 -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Stop the container with:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">docker kill mysql
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>You can search for already built Docker images on &lt;a class="link" href="https://hub.docker.com/" target="_blank" rel="noopener"
>Docker Hub&lt;/a>. It&amp;rsquo;s also possible to create private Docker repositories for your own software that you don&amp;rsquo;t want to be publicly available.&lt;/p>
&lt;p>&lt;a id="Containerorchestration">&lt;/a>&lt;/p>
&lt;h4 id="container-orchestration">Container orchestration
&lt;/h4>&lt;p>Now that Docker container images has become the preferred way to package and distribute software on the Linux platform, there has emerged a need for systems to coordinate running and deploying these containers. Similar to the ecosystem of products VMware has built up around development and operation of virtual machines.&lt;/p>
&lt;p>Container orchestration systems have the responsibility for:&lt;/p>
&lt;ul>
&lt;li>Load balancing.&lt;/li>
&lt;li>Service discovery.&lt;/li>
&lt;li>Health checks.&lt;/li>
&lt;li>Automatic scaling and restarting of host nodes and containers.&lt;/li>
&lt;li>Zero downtime upgrades (rolling deploys).&lt;/li>
&lt;/ul>
&lt;p>Until recently the ecosystem around container orchestration has been fragmented, and the most popular alternatives have been:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://kubernetes.io/" target="_blank" rel="noopener"
>Kubernetes&lt;/a> (Originaly from Google, now managed by CNCF, the Cloud Native Computing Foundation)&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.docker.com/engine/swarm/" target="_blank" rel="noopener"
>Swarm&lt;/a> (From the maker of Docker)&lt;/li>
&lt;li>&lt;a class="link" href="http://mesos.apache.org/" target="_blank" rel="noopener"
>Mesos&lt;/a> (From Apache Software Foundation)&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/coreos/fleet" target="_blank" rel="noopener"
>Fleet&lt;/a> (From CoreOS)&lt;/li>
&lt;/ul>
&lt;p>But the last year there has been a convergence towards Kubernetes as the preferred solution.&lt;/p>
&lt;ul>
&lt;li>7 February
&lt;ul>
&lt;li>&lt;a class="link" href="https://coreos.com/blog/migrating-from-fleet-to-kubernetes.html" target="_blank" rel="noopener"
>CoreOS announces that they are removing Fleet from Container Linux and recommends Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>27 July
&lt;ul>
&lt;li>&lt;a class="link" href="https://azure.microsoft.com/en-us/blog/announcing-cncf/" target="_blank" rel="noopener"
>Microsoft joins the CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>9 August
&lt;ul>
&lt;li>&lt;a class="link" href="https://techcrunch.com/2017/08/09/aws-joins-the-cloud-native-computing-foundation/" target="_blank" rel="noopener"
>Amazon Web Services join the CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>29 August
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.geekwire.com/2017/now-vmware-pivotal-cncf-becoming-hub-enterprise-tech/" target="_blank" rel="noopener"
>VMware and Pivotal joins the CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>17 September
&lt;ul>
&lt;li>&lt;a class="link" href="https://techcrunch.com/2017/09/13/oracle-joins-the-cloud-native-computing-foundation-as-a-platinum-member/" target="_blank" rel="noopener"
>Oracle joins the CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>17 October
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.theregister.co.uk/2017/10/17/docker_ee_kubernetes_support/" target="_blank" rel="noopener"
>Docker announces native support for Kubernetes in addition to it&amp;rsquo;s own Swarm product&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>24 October
&lt;ul>
&lt;li>&lt;a class="link" href="https://azure.microsoft.com/en-us/blog/introducing-azure-container-service-aks-managed-kubernetes-and-azure-container-registry-geo-replication/" target="_blank" rel="noopener"
>Microsoft Azure announces the managed Kubernetes service AKS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>29 November
&lt;ul>
&lt;li>&lt;a class="link" href="https://aws.amazon.com/blogs/aws/amazon-elastic-container-service-for-kubernetes/" target="_blank" rel="noopener"
>Amazon Web Services announces the managed Kubernetes service EKS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Especially the last two news items are important. Deploying and running your own Kubernetes-installation requires time and skills (&lt;a class="link" href="https://stripe.com/blog/operating-kubernetes" target="_blank" rel="noopener"
>Read how Stripe used 5 months to trust running Kubernetes in production, just for batch jobs.&lt;/a>)&lt;/p>
&lt;p>Until now the choice has been running your own Kubernetes cluster or using Google Container Engine which has been &lt;a class="link" href="https://cloudplatform.googleblog.com/2014/11/unleashing-containers-and-kubernetes-with-google-compute-engine.html" target="_blank" rel="noopener"
>using Kubernetes since 2014&lt;/a>. Many of us feel a certain discomfort by locking ourselves to one provider. But this is now changing when you can develop infrastructure on Kubernetes and choose between the 3 large cloud providers in addition to running your own cluster if wanted. &lt;strong>*&lt;/strong>&lt;/p>
&lt;p>&lt;strong>*&lt;/strong> Kubernetes is a fast moving project, and features might be available on the different platforms on different timelines.&lt;/p>
&lt;p>&lt;a id="QuickstartAKS">&lt;/a>&lt;/p>
&lt;h2 id="getting-started-with-azure-kubernetes---aks">Getting started with Azure Kubernetes - AKS
&lt;/h2>&lt;p>&lt;a id="Caveats">&lt;/a>&lt;/p>
&lt;h3 id="caveats">Caveats
&lt;/h3>&lt;blockquote>
&lt;p>This guide is based on the documentation on &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough" target="_blank" rel="noopener"
>Microsoft.com&lt;/a>. Setting up a Azure Kubernetes cluster did not work in the beginning of December, but today, 23. December, it seems to work fairly well. But, upgrading the cluster from Kubernetes 1.7 to 1.8 for example does NOT work.&lt;/p>
&lt;p>AKS is in Preview and Azure are working continuously to make AKS stable and to support as many Kubernetes-features as possible. Amazon Web Services has a similar closed invite-only Preview currently while working on stability and features.&lt;/p>
&lt;p>Both Azure and AWS expresses expectations about their Kubernetes offerings will be ready for production in 2018.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Preparations">&lt;/a>&lt;/p>
&lt;h3 id="preparations">Preparations
&lt;/h3>&lt;p>You need Azure-CLI (version 2.0.21 or newer) to execute the &lt;code>az&lt;/code> commands:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://aka.ms/InstallAzureCliWindows" target="_blank" rel="noopener"
>Download Azure-CLI here&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest" target="_blank" rel="noopener"
>Information about Azure-CLI on MacOS and Linux here&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>All commands executed in Windows PowerShell.&lt;/p>
&lt;p>&lt;a id="AzureLogin">&lt;/a>&lt;/p>
&lt;h3 id="azure-login">Azure login
&lt;/h3>&lt;p>Log on to Azure:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az login
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>You will get a link to open in your browser together with an authentication code. Enter the code on the webpage and &lt;code>az login&lt;/code> will save the login information so that you will not have to authenticate again on the same machine.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> The login information gets saved in &lt;code>C:\Users\Username\.azure\&lt;/code>. You have to make sure nobody can access these files. They will then have full access to your Azure account.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="ActivateContainerService">&lt;/a>&lt;/p>
&lt;h3 id="activate-containerservice">Activate ContainerService
&lt;/h3>&lt;p>Since AKS is in Preview/Beta, you explicitly have to activate it in your subscription to get access to the &lt;code>aks&lt;/code> subcommands.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az provider register -n Microsoft.ContainerService
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">az provider show -n Microsoft.ContainerService
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="CreateResourceGroup">&lt;/a>&lt;/p>
&lt;h3 id="create-a-resource-group">Create a resource group
&lt;/h3>&lt;p>Here we create a resource group named &amp;ldquo;my_aks_rg&amp;rdquo; in Azure region West Europe.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az group create --name my_aks_rg --location westeurope
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong>
To see a list of all available Azure regions, use the command &lt;code>az account list-locations --output table&lt;/code>. &lt;strong>PS&lt;/strong> AKS might not be available in all regions yet!&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="CreateK8sCluster">&lt;/a>&lt;/p>
&lt;h3 id="create-kubernetes-cluster">Create Kubernetes cluster
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks create --resource-group my_aks_rg --name my_cluster --node-count 3 --generate-ssh-keys --node-vm-size Standard_B2s --node-osdisk-size 128 --kubernetes-version 1.8.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;code>--node-count&lt;/code>
&lt;ul>
&lt;li>Number of agent(host) nodes available to run containers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--generate-ssh-keys&lt;/code>
&lt;ul>
&lt;li>Creates and prints a SSH key which can be used for SSHing directly to the agent nodes.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--node-vm-size&lt;/code>
&lt;ul>
&lt;li>Which size Azure VMs the agent nodes should be created as. To see available sizes use &lt;code>az vm list-sizes -l westeurope --output table&lt;/code> and &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes" target="_blank" rel="noopener"
>Microsofts webpages&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--node-osdisk-size&lt;/code>
&lt;ul>
&lt;li>Disk size of the agent nodes in GB. &lt;strong>PS&lt;/strong> Containers can be stopped and moved to another host if Kubernetes finds it necessary or if a agent node disappears. All data saved locally in the container will be gone. If saving data permanently use Kubernetes PersistentVolumes and not the local agent node or container disks.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--kubernetes-version&lt;/code>
&lt;ul>
&lt;li>Which Kubernetes version to install. Azure does NOT necessarily install the last version by default, and currently upgrading with &lt;code>az aks upgrade&lt;/code> does not work. Latest version available right now is 1.8.2. It&amp;rsquo;s recommended to use the latest available version since there is a lot of changes from version to version. The documentation is also much better for newer versions.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Save the output of the command in a file in a secure location. It contains keys that can be used to connect to the cluster with SSH. Even though that should not in theory be necessary.&lt;/p>
&lt;p>&lt;a id="InstallKubectl">&lt;/a>&lt;/p>
&lt;h3 id="install-kubectl">Install kubectl
&lt;/h3>&lt;p>&lt;code>kubectl&lt;/code> is the client which performs all operations against your Kubernetes cluster. Azure CLI can install &lt;code>kubectl&lt;/code> for you:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks install-cli
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>After &lt;code>kubectl&lt;/code> is installed we need to get login information so that &lt;code>kubectl&lt;/code> can communicate with the Kubernetes cluster.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks get-credentials --resource-group my_aks_rg --name my_cluster
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>The login information is saved in &lt;code>C:\Users\Username\.kube\config&lt;/code>. Keep these files secure as well.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong> When you have several Kubernetes clusters you can change which one &lt;code>kubectl&lt;/code> talks to with &lt;code>kubectl config get-contexts&lt;/code> and &lt;code>kubectl config set-context my_cluster&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="InspectCluster">&lt;/a>&lt;/p>
&lt;h3 id="inspect-cluster">Inspect cluster
&lt;/h3>&lt;p>To check that the cluster and &lt;code>kubectl&lt;/code> works we start with a couple of commands.&lt;/p>
&lt;p>See all agent nodes and status:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get nodes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME STATUS AGE VERSION
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-0 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-1 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-2 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>See all services, pods and deployments:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get all --all-namespaces
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system po/kubernetes-dashboard-6fc8cf9586-frpkn 1/1 Running 0 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system svc/kubernetes-dashboard 10.0.161.132 &amp;lt;none&amp;gt; 80/TCP 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system deploy/kubernetes-dashboard 1 1 1 1 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME DESIRED CURRENT READY AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system rs/kubernetes-dashboard-6fc8cf9586 1 1 1 3d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>This is just some of the output from this command. You do not have to know what the resources in the &lt;code>kube-system&lt;/code> namespace does. That is part of the intention when Microsoft is managing our cluster for us.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Namespaces&lt;/strong>
In Kubernetes there is something called Namespaces. Resources in one namespace does not have automatic access to resources in another namespace. The services that runs Kubernetes itself use the namespace &lt;code>kube-system&lt;/code>. The &lt;code>kubectl&lt;/code> command by default only shows you resources in the &lt;code>default&lt;/code> namespace, unless you specify &lt;code>--all-namespaces&lt;/code> or &lt;code>--namespace=xx&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="StartNginx">&lt;/a>&lt;/p>
&lt;h3 id="start-some-nginx-containers">Start some nginx containers
&lt;/h3>&lt;blockquote>
&lt;p>An instance of a running container in Kubernetes is called a &lt;strong>Pod&lt;/strong>.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;code>nginx&lt;/code> is a fast and flexible web server.&lt;/p>
&lt;/blockquote>
&lt;p>Now that the clsuter is up we can start rolling out services and deployments on it.&lt;/p>
&lt;p>Lets start with creating a Deployment consiting of 3 containers all running the &lt;code>nginx:mainline-alpine&lt;/code> image from &lt;a class="link" href="https://hub.docker.com/r/_/nginx/" target="_blank" rel="noopener"
>Docker hub&lt;/a>.&lt;/p>
&lt;p>&lt;strong>nginx-dep.yaml&lt;/strong> looks like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">apiVersion: apps/v1beta2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kind: Deployment
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: nginx-deployment
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> replicas: 3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> selector:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> matchLabels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> template:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> containers:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - name: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> image: nginx:mainline-alpine
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ports:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - containerPort: 80
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Load this into the cluster with &lt;code>kubectl create&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-dep.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>This command creates the resources described in the file. &lt;code>kubectl&lt;/code> can read files either from your local disk or from a web URL.&lt;/p>
&lt;blockquote>
&lt;p>After making changes to a resource definition (&lt;code>.yaml&lt;/code> file), you can update the resources in the cluster with &lt;code>kubetl replace -f resource.yaml&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>We can verify that the Deployment is ready:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get deploy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment 3 3 3 3 10m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>We can also get the actual Pods that are running:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get pods
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-dqwx5 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-xwzpw 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-z5tfk 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Logger&lt;/strong> We can view logs from one pod with &lt;code>kubectl logs nginx-deployment-569477d6d8-xwzpw&lt;/code>. But since we in this case don&amp;rsquo;t know which Pod ends up getting an incomming request we can view logs from all the Pods which have &lt;code>app=nginx&lt;/code> label: &lt;code>kubectl logs -lapp=nginx&lt;/code>. The use of &lt;code>app=nginx&lt;/code> is our choice in &lt;code>nginx-dep.yaml&lt;/code> when we configured &lt;code>spec.template.metadata.labels: app: nginx&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="NginxService">&lt;/a>&lt;/p>
&lt;h3 id="making-nginx-available-with-a-service">Making nginx available with a service
&lt;/h3>&lt;p>To send traffic to our new Pods we need to create a &lt;strong>Service&lt;/strong>. A service consists of one or more Pods which are chosen based on different criteria, for example which labels they have and whether the Pods are Running and Ready.&lt;/p>
&lt;p>Lets create a service which forwards traffic to all Pods with label &lt;code>app: nginx&lt;/code> and are listening to port 80. In addition we make the service available via a LoadBalancer:&lt;/p>
&lt;p>&lt;strong>nginx-svc.yaml&lt;/strong> looks like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">apiVersion: v1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kind: Service
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> type: LoadBalancer
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ports:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - port: 80
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: http
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> targetPort: 80
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> selector:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>We tell Kubernetes to create our service with &lt;code>kubectl create&lt;/code> as usual:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-svc.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>We can then wait and see which IP-address Azure assigns our service:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get svc -w
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx 10.0.24.11 13.95.173.255 80:31522/TCP 15m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> It can take a few minutes for Azure to allocate and assign a Public IP for us. In the mean time &lt;code>&amp;lt;pending&amp;gt;&lt;/code> will appear under EXTERNAL-IP.&lt;/p>
&lt;/blockquote>
&lt;p>A simple &lt;strong>Welcome to nginx&lt;/strong> webpage should now be available on http://13.95.173.255 (&lt;em>remember to replace with your own External-IP&lt;/em>).&lt;/p>
&lt;p>We can also delete the service and deployment afterwards:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl delete svc nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete deploy nginx-deployment
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="ScaleCluster">&lt;/a>&lt;/p>
&lt;h3 id="scaling-the-cluster">Scaling the cluster
&lt;/h3>&lt;p>If we want to change the number of agent nodes running Pods we can do that via Azure-CLI:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks scale --name my_cluster --resource-group my_aks_rg --node-count 5
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>Currently all nodes will be created with the same size as when we created the cluster. AKS will probably get support for &lt;a class="link" href="https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools" target="_blank" rel="noopener"
>&lt;strong>node-pools&lt;/strong>&lt;/a> next year. That will allow for creating different groups of nodes with different size and operating systems, both Linux and Windows.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="DeleteCluster">&lt;/a>&lt;/p>
&lt;h3 id="delete-cluster">Delete cluster
&lt;/h3>&lt;p>You can delete the whole cluster like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks delete --name my_cluster --resource-group my_aks_rg --yes
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="Bonusmaterial">&lt;/a>&lt;/p>
&lt;h2 id="bonus-material">Bonus material
&lt;/h2>&lt;p>Here is some bonus material if you want to go a bit further with Kubernetes.&lt;/p>
&lt;p>&lt;a id="HelmIntro">&lt;/a>&lt;/p>
&lt;h3 id="deploying-services-with-helm">Deploying services with Helm
&lt;/h3>&lt;p>&lt;a class="link" href="https://helm.sh/" target="_blank" rel="noopener"
>Helm&lt;/a> is a package manager and library of software that is ready to be deployed on a Kubernetes cluster.&lt;/p>
&lt;p>Start by downloading the &lt;a class="link" href="https://github.com/kubernetes/helm/releases" target="_blank" rel="noopener"
>Helm-client&lt;/a>. It will read login information etc. from the same location as &lt;code>kubectl&lt;/code> automatically.&lt;/p>
&lt;p>Install the Helm-server (&lt;strong>Tiller&lt;/strong>) on the Kubernetes cluster and update the package library:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">helm init
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm repo update
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>See available packages (&lt;strong>Charts&lt;/strong>) with &lt;code>helm search&lt;/code>.&lt;/p>
&lt;p>&lt;a id="HelmMinecraft">&lt;/a>&lt;/p>
&lt;h4 id="deploy-minecraft-with-helm">Deploy MineCraft with Helm
&lt;/h4>&lt;p>Lets deploy a MineCraft server installation on our cluster, just because we can :-)&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">helm install --name stians --set minecraftServer.eula=true stable/minecraft
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;code>--set&lt;/code> overrides one or more of the standard values configured in the package. The MineCraft package is made in a way where it does not start without accepting the user license agreement by setting the variable &lt;code>minecraftServer.eula&lt;/code>. All the variables that can be set in the MineCraft package are &lt;a class="link" href="https://github.com/kubernetes/charts/blob/master/stable/minecraft/values.yaml" target="_blank" rel="noopener"
>documented here&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>Then we wait for Azure to assign us a Public IP:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get svc -w
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">stians-minecraft 10.0.237.0 13.95.172.192 25565:30356/TCP 3m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Now we can connect to our MineCraft server on &lt;code>13.95.172.192:25565&lt;/code>!&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/managed-kubernetes-on-microsoft-azure-english/minecraft-k8s.png"
width="856"
height="507"
srcset="https://demo.stack.jimmycai.com/p/managed-kubernetes-on-microsoft-azure-english/minecraft-k8s_hu13197832490752396686.png 480w, https://demo.stack.jimmycai.com/p/managed-kubernetes-on-microsoft-azure-english/minecraft-k8s_hu16352153299487520650.png 1024w"
loading="lazy"
alt="Kubernetes in MineCraft on Kubernetes"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="405px"
>&lt;/p>
&lt;p>&lt;a id="KubernetesDashboard">&lt;/a>&lt;/p>
&lt;h3 id="kubernetes-dashboard">Kubernetes Dashboard
&lt;/h3>&lt;p>Kubernetes also has a graphic web user-interface which makes it a bit easier to see which resources are in the cluster, view logs and even open a remote shell inside a running Pod, among other things.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl proxy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Starting to serve on 127.0.0.1:8001
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>kubectl&lt;/code> encrypts and tunnels the traffic to the Kubernetes API servers. The dashboard is available on &lt;a class="link" href="http://127.0.0.1:8001/ui/" target="_blank" rel="noopener"
>http://127.0.0.1:8001/ui/&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/managed-kubernetes-on-microsoft-azure-english/k8s-dash.png"
width="899"
height="588"
srcset="https://demo.stack.jimmycai.com/p/managed-kubernetes-on-microsoft-azure-english/k8s-dash_hu15091082541702636396.png 480w, https://demo.stack.jimmycai.com/p/managed-kubernetes-on-microsoft-azure-english/k8s-dash_hu17318776918264020386.png 1024w"
loading="lazy"
alt="Kubernetes Dashboard"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>&lt;/p>
&lt;p>&lt;a id="Conclusion">&lt;/a>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>I hope you enjoy Kubernetes as much as I have. The learning curve can be a bit steep in the beginning, but it does not take long before you are productive.&lt;/p>
&lt;p>Look at the &lt;a class="link" href="https://v1-8.docs.kubernetes.io/docs/tutorials/" target="_blank" rel="noopener"
>official guides on Kubernetes.io&lt;/a> to learn more about defining different types of resources and services to run on Kubernetes. &lt;strong>PS: There are big changes from version to version so make sure you use the documentation for the correct version!&lt;/strong>&lt;/p>
&lt;p>Kubernetes also have a very active Slack-community on &lt;a class="link" href="http://slack.k8s.io/" target="_blank" rel="noopener"
>kubernetes.slack.com&lt;/a> that is worthwhile to check out.&lt;/p></description></item><item><title>Managed Kubernetes på Microsoft Azure (Norwegian)</title><link>https://demo.stack.jimmycai.com/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/</link><pubDate>Mon, 25 Dec 2017 00:00:00 +0000</pubDate><guid>https://demo.stack.jimmycai.com/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/</guid><description>&lt;img src="https://demo.stack.jimmycai.com/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/minecraft-k8s.png" alt="Featured image of post Managed Kubernetes på Microsoft Azure (Norwegian)" />&lt;p>&lt;em>Update 29. Dec: There is an &lt;a class="link" href="https://demo.stack.jimmycai.com/2017/12/29/managed-kubernetes-on-azure-eng.html" >English version of this post here.&lt;/a>&lt;/em>&lt;/p>
&lt;p>Kubernetes (K8s) er i ferd med å bli de-facto standard for deployments av kontainer-baserte applikasjoner. Microsoft har nå preview av deres managed Kubernetes tjeneste (Azure Kubernetes Service, AKS) som gjør det enkelt å opprette et Kubernetes cluster og rulle ut tjenester uten å måtte ha kompetanse og tid til den daglige driften av selve Kubernetes-clusteret, som per i dag kan være relativt komplisert og tidkrevende.&lt;/p>
&lt;p>I denne posten setter vi opp et Kubernetes cluster fra scratch ved bruk av Azure CLI.&lt;/p>
&lt;p>Table of contents&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="#Bakgrunn" >Bakgrunn&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Dockercontainers" >Docker containers&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Containerorchestration" >Container orchestration&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#OppretteAKS" >Kom i gang med Azure Kubernetes - AKS&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Forbehold" >Forbehold&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Forberedelser" >Forberedelser&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Azureinnlogging" >Azure innlogging&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#AktiverContainerService" >Aktiver ContainerService&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#OpprettResourceGroup" >Opprett en resource group&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#OpprettK8sCluster" >Opprette Kubernetes cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#InstallerKubectl" >Installer kubectl&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#InspiserCluster" >Inspiser cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#StarteNginx" >Starte noen nginx containere&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#NginxService" >Gjøre nginx tilgjengelig med en tjeneste&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#ScaleCluster" >Skalere cluster&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#DeleteCluster" >Slette cluster&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Bonusmateriale" >Bonusmateriale&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#HelmIntro" >Rulle ut tjenester med Helm pakker&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#HelmMinecraft" >MineCraft server med Helm&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#KubernetesDashboard" >Kubernetes Dashboard&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Konklusjon" >Konklusjon&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="microsoft-azure">Microsoft Azure
&lt;/h4>&lt;blockquote>
&lt;p>&lt;a class="link" href="https://azure.microsoft.com/en-us/free/" target="_blank" rel="noopener"
>Hvis du ikke har Azure fra før kan du prøve tjenester for $200 i 30 dager.&lt;/a> VM typen &lt;strong>Standard_B2s&lt;/strong> er Burstable, har 2vCPU, 4GB RAM, 8GB temp storage og koster ~$38 / mnd. For $200 kan du ha et cluster på 3-4 B2s noder plus trafikkostnad, lastbalanserere og andre nødvendige tjenester.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;em>Vi har ingen tilknytning til Microsoft bortsett fra at de sponser vår startup &lt;a class="link" href="http://www.datadynamics.no/" target="_blank" rel="noopener"
>DataDynamics&lt;/a> med cloud-tjenester i 24 mnd i deres &lt;a class="link" href="https://bizspark.microsoft.com/" target="_blank" rel="noopener"
>BizSpark program&lt;/a>.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Bakgrunn">&lt;/a>&lt;/p>
&lt;h2 id="bakgrunn">Bakgrunn
&lt;/h2>&lt;p>&lt;a id="Dockercontainers">&lt;/a>&lt;/p>
&lt;h3 id="docker-containers">Docker containers
&lt;/h3>&lt;p>&lt;em>Vi tar ikke for oss Docker containers i dybden i denne posten, men her er en kort oppsummering for de som ikke er kjent med teknologien.&lt;/em>&lt;/p>
&lt;p>Docker er en måte å pakketere programvare slik at det kan kjøres på samtlige populære platformer uten å måtte bruke mye tid på dependencies, oppsett og konfigurasjon.&lt;/p>
&lt;p>I tillegg bruker en Docker container operativsystemet på vertsmaskinen når den kjører. Dette gjør at en kan kjøre mange flere containere på samme vertsmaskin sammenlignet med virtuelle maskiner.&lt;/p>
&lt;p>Her er en ufullstendig og grov sammenligning mellom en Docker container og en virtuell maskin:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Virtuel maskin&lt;/th>
&lt;th>Docker container&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Image størrelse&lt;/td>
&lt;td>fra 200MB til mange GB&lt;/td>
&lt;td>fra 10MB til 3-400MB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Oppstartstid&lt;/td>
&lt;td>60 sekunder +&lt;/td>
&lt;td>1-10 sekunder&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Minnebruk&lt;/td>
&lt;td>256MB-512MB-1GB +&lt;/td>
&lt;td>2MB +&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sikkerhet&lt;/td>
&lt;td>God isolasjon mellom VM&lt;/td>
&lt;td>Dårligere isolasjon mellom containere&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Bygge image&lt;/td>
&lt;td>Minutter&lt;/td>
&lt;td>Sekunder&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> Tallene for virtuelle maskiner er tatt fra hukommelsen. Jeg forsøkte å starte en MySQL virtuell appliance på min laptop men VMware Player nekter å kjøre pga inkompatibilitet med Windows Hyper-V. VMware Workstation nekter å kjøre pga utgått lisens og Oracle VirtualBox gir en nasty bluescreen gang på gang. Hooray!&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong> De minste og raskeste Docker imagene er bygget på Alpine Linux. For webserveren Nginx er det Alpine-baserte imaget 15MB mot det Debian-baserte imaget på 108MB. PostgreSQL:Alpine er 38MB mot 287MB. Siste versjon av MySQL er 343MB men vil i versjon 8 støtte Alpine Linux også.&lt;/p>
&lt;/blockquote>
&lt;p>Noen av fordelene med Docker containers er altså:&lt;/p>
&lt;ul>
&lt;li>Kompatibilitet på tvers av platformer, Linux, Windows og MacOS.&lt;/li>
&lt;li>10-100x mindre størrelse. Raskere å laste ned, raskere å bygge, raskere å laste opp.&lt;/li>
&lt;li>Minnebruk kun for applikasjon og ikke eget OS.
&lt;ul>
&lt;li>Fordel under utvikling, kan kjøre 10-20-30 Docker containere samtidig på en laptop.&lt;/li>
&lt;li>Fordel i produksjon, kan redusere hardware utgifter betraktelig.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Oppstart på få sekunder. Gjør dynamisk skalering av applikasjoner mye enklere.&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://store.docker.com/editions/community/docker-ce-desktop-windows" target="_blank" rel="noopener"
>Last ned Docker for Windows her.&lt;/a>&lt;/p>
&lt;p>Og start en MySQL database fra Windows CMD eller Powershell:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">docker run --name mysql -p 3306:3306 -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Stop containeren med:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">docker kill mysql
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>En kan søke etter ferdige Docker images på &lt;a class="link" href="https://hub.docker.com/" target="_blank" rel="noopener"
>Docker Hub&lt;/a>. Det er også mulig å lage private Docker repositories for egen programvare som ikke skal være tilgjengelig for omverden.&lt;/p>
&lt;p>&lt;a id="Containerorchestration">&lt;/a>&lt;/p>
&lt;h4 id="container-orchestration">Container orchestration
&lt;/h4>&lt;p>Etter som Docker containers har blitt den foretrukne måten å pakke og distribuere programvare på Linux platformen de siste par årene har det vokst frem et behov for systemer som kan samkjøre drift og utrulling av disse containerene. Ikke ulikt det økosystemet av produkter VMware har bygget opp rundt utvikling og drift av virtuelle maskiner.&lt;/p>
&lt;p>Container orchestration systemene har som oppgave å sørge for:&lt;/p>
&lt;ul>
&lt;li>Lastbalansering.&lt;/li>
&lt;li>Service discovery.&lt;/li>
&lt;li>Health checks.&lt;/li>
&lt;li>Automatisk skalering og restarting av vertsmaskiner og containere.&lt;/li>
&lt;li>Oppgraderinger uten nedetid (rolling deploy).&lt;/li>
&lt;/ul>
&lt;p>Frem til nylig har økosystemet rundt container orchestration vært fragmentert og de mest populære alternativene har vært:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://kubernetes.io/" target="_blank" rel="noopener"
>Kubernetes&lt;/a> (Opprinnelig fra Google, nå styrt av CNCF, Cloud Native Computing Foundation)&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.docker.com/engine/swarm/" target="_blank" rel="noopener"
>Swarm&lt;/a> (Fra produsenten bak Docker)&lt;/li>
&lt;li>&lt;a class="link" href="http://mesos.apache.org/" target="_blank" rel="noopener"
>Mesos&lt;/a> (Fra Apache Software Foundation)&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/coreos/fleet" target="_blank" rel="noopener"
>Fleet&lt;/a> (Fra CoreOS)&lt;/li>
&lt;/ul>
&lt;p>Men det siste året har det vært en konvergens mot Kubernetes som foretrukket løsning.&lt;/p>
&lt;ul>
&lt;li>7 februar
&lt;ul>
&lt;li>&lt;a class="link" href="https://coreos.com/blog/migrating-from-fleet-to-kubernetes.html" target="_blank" rel="noopener"
>CoreOS annonserer at de fjerner Fleet fra Container Linux og anbefaler Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>27 juli
&lt;ul>
&lt;li>&lt;a class="link" href="https://azure.microsoft.com/en-us/blog/announcing-cncf/" target="_blank" rel="noopener"
>Microsoft slutter seg til CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>9 august
&lt;ul>
&lt;li>&lt;a class="link" href="https://techcrunch.com/2017/08/09/aws-joins-the-cloud-native-computing-foundation/" target="_blank" rel="noopener"
>Amazon Web Services slutter seg til CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>29 august
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.geekwire.com/2017/now-vmware-pivotal-cncf-becoming-hub-enterprise-tech/" target="_blank" rel="noopener"
>VMware og Pivotal slutter seg til CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>17 september
&lt;ul>
&lt;li>&lt;a class="link" href="https://techcrunch.com/2017/09/13/oracle-joins-the-cloud-native-computing-foundation-as-a-platinum-member/" target="_blank" rel="noopener"
>Oracle slutter seg til CNCF&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>17 oktober
&lt;ul>
&lt;li>&lt;a class="link" href="https://www.theregister.co.uk/2017/10/17/docker_ee_kubernetes_support/" target="_blank" rel="noopener"
>Docker annonserer native støtte for Kubernetes i tillegg til sitt eget Swarm produkt&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>24 oktober
&lt;ul>
&lt;li>&lt;a class="link" href="https://azure.microsoft.com/en-us/blog/introducing-azure-container-service-aks-managed-kubernetes-and-azure-container-registry-geo-replication/" target="_blank" rel="noopener"
>Microsoft Azure annonserer managed Kubernetes med tjenesten AKS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>29 november
&lt;ul>
&lt;li>&lt;a class="link" href="https://aws.amazon.com/blogs/aws/amazon-elastic-container-service-for-kubernetes/" target="_blank" rel="noopener"
>Amazon Web Services annonserer managed Kubernetes med tjenesten EKS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>De to siste nyhetene er spesielt viktige. Å drifte sin egen Kubernetes-installasjon krever tid og kompetanse. (&lt;a class="link" href="https://stripe.com/blog/operating-kubernetes" target="_blank" rel="noopener"
>Les hvordan Stripe brukte 5 måneder på å bli fortrolig med å drifte sitt eget Kubernetes cluster, bare for batch jobs.&lt;/a>)&lt;/p>
&lt;p>Frem til nå har valget vært mellom å drifte sitt eget Kubernetes cluster eller bruke Google Container Engine som har &lt;a class="link" href="https://cloudplatform.googleblog.com/2014/11/unleashing-containers-and-kubernetes-with-google-compute-engine.html" target="_blank" rel="noopener"
>brukt Kubernetes siden 2014&lt;/a>. Mange av oss føler et visst ubehag ved å låse oss til én tilbyder. Men dette er nå anderledes når en kan utvikle infrastruktur på Kubernetes, og velge tilnærmet fritt &lt;strong>*&lt;/strong> mellom de 3 store cloud-tilbyderene i tillegg til å drifte selv om ønskelig.&lt;/p>
&lt;p>&lt;strong>*&lt;/strong> Kubernetes utvikles raskt, og funksjonalitet blir ofte ikke tilgjengelig på de ulike platformene samtidig.&lt;/p>
&lt;p>&lt;a id="OppretteAKS">&lt;/a>&lt;/p>
&lt;h2 id="opprette-azure-kubernetes-cluster">Opprette Azure Kubernetes Cluster
&lt;/h2>&lt;p>&lt;a id="Forbehold">&lt;/a>&lt;/p>
&lt;h3 id="forbehold">Forbehold
&lt;/h3>&lt;blockquote>
&lt;p>Denne gjennomgangen tar utgangspunkt i dokumentasjonen på &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough" target="_blank" rel="noopener"
>Microsoft.com&lt;/a>. Å sette opp et Azure Kubernetes cluster fungerte ikke i starten av desember, men per dags dato, 23. desember, ser det ut til å fungere relativt bra. Men, oppgradering av cluster fra Kubernetes 1.7 til 1.8 fungerer for eksempel IKKE.&lt;/p>
&lt;p>AKS er i Preview og Azure jobber kontinuerlig med å gjøre AKS stabilt og støtte så mange Kubernetes-funksjoner som mulig. Amazon Web Services har tilsvarende en lukket invite-only Preview per dags dato mens de også jobber med stabilitet og funksjonalitet.&lt;/p>
&lt;p>Både Azure og AWS uttrykker forventning om at deres Kubernetes tjenester skal være klare for produksjonsmiljø ila 2018.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Forberedelser">&lt;/a>&lt;/p>
&lt;h3 id="forberedelser">Forberedelser
&lt;/h3>&lt;p>Du behøver Azure-CLI (versjon 2.0.21 eller nyere) for å utføre kommandoene:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://aka.ms/InstallAzureCliWindows" target="_blank" rel="noopener"
>Last ned Azure-CLI her&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest" target="_blank" rel="noopener"
>Informasjon om Azure-CLI på MacOS og Linux finner du her&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Alle kommandoer gjøres i Windows PowerShell.&lt;/p>
&lt;p>&lt;a id="Azureinnlogging">&lt;/a>&lt;/p>
&lt;h3 id="azure-innlogging">Azure innlogging
&lt;/h3>&lt;p>Logg på Azure:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az login
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Du får en link som du åpner i din browser samt en autentiseringskode. Skriv koden på nettsiden og &lt;code>az login&lt;/code> lagrer påloggingsinformasjonen slik at du ikke behøver å autentisere igjen på samme maskin.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> Pålogingsinformasjonen lagres i &lt;code>C:\Users\Brukernavn\.azure\&lt;/code>. Du må selv passe på at ingen kopierer disse filene. Da får de full tilgang til din Azure konto.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="AktiverContainerService">&lt;/a>&lt;/p>
&lt;h3 id="aktiver-containerservice">Aktiver ContainerService
&lt;/h3>&lt;p>Siden AKS er i Preview/Beta må du eksplisitt aktivere det for å få tilgang til &lt;code>aks&lt;/code> kommandoene.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az provider register -n Microsoft.ContainerService
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">az provider show -n Microsoft.ContainerService
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="OpprettResourceGroup">&lt;/a>&lt;/p>
&lt;h3 id="opprett-en-resource-group">Opprett en resource group
&lt;/h3>&lt;p>Her oppretter vi en resource group med navn &amp;ldquo;min_aks_rg&amp;rdquo; i Azure region West Europe.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az group create --name min_aks_rg --location westeurope
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong>
For å se en liste over tilgjengelige Azure regioner, bruk kommandoen &lt;code>az account list-locations --output table&lt;/code>. &lt;strong>PS&lt;/strong> Det kan hende AKS ikke er tilgjengelig i alle regioner enda.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="OpprettK8sCluster">&lt;/a>&lt;/p>
&lt;h3 id="opprette-kubernetes-cluster">Opprette Kubernetes cluster
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks create --resource-group min_aks_rg --name mitt_cluster --node-count 3 --generate-ssh-keys --node-vm-size Standard_B2s --node-osdisk-size 256 --kubernetes-version 1.8.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;code>--node-count&lt;/code>
&lt;ul>
&lt;li>Antall vertsmaskiner tilgjengelig for å kjøre containers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--generate-ssh-keys&lt;/code>
&lt;ul>
&lt;li>Oppretter og outputter en SSH key som kan brukes for å SSHe direkte til vertsmaskinene.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--node-vm-size&lt;/code>
&lt;ul>
&lt;li>Hvilken type Azure VM clusteret skal bestå av. For å se tilgjengelige størrelser bruk &lt;code>az vm list-sizes -l westeurope --output table&lt;/code> og &lt;a class="link" href="https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes" target="_blank" rel="noopener"
>Microsofts nettsider.&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--node-osdisk-size&lt;/code>
&lt;ul>
&lt;li>Disk størrelse på vertsmaskiner i GB. &lt;strong>PS&lt;/strong> Conteinere kan bli stoppet og flyttet til en annen host ved behov eller hvis en vertsmaskin forsvinner. Alle data lagret lokalt i conteineren blir da borte. Hvis en skal lagre ting permanent må en bruke PersistentVolumes og ikke lokal disk på vertsmaskin.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>--kubernetes-version&lt;/code>
&lt;ul>
&lt;li>Hvilken Kubernetes versjon som skal installeres. Azure installerer IKKE den siste versjonen som standard, og per dags dato fungerer ikke &lt;code>az aks upgrade&lt;/code> tilstrekkelig. Siste tilgjengelige versjon per dags dato er 1.8.2. Det er en fordel å bruke siste versjon da det skjer store forbedringer i Kubernetes fra versjon til versjon. Dokumentasjon er også mye bedre for nyere versjoner.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Lagre teksten som kommandoen spytter ut i en fil på en trygg plass. Den inneholder nøkler som kan brukes for å kople til clusteret med SSH. Selv om det i teorien ikke skal være nødvendig.&lt;/p>
&lt;p>&lt;a id="InstallerKubectl">&lt;/a>&lt;/p>
&lt;h3 id="installer-kubectl">Installer kubectl
&lt;/h3>&lt;p>&lt;code>kubectl&lt;/code> er klienten som gjør alle operasjoner mot ditt Kubernetes cluster. Azure CLI kan installere &lt;code>kubectl&lt;/code> for deg:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks install-cli
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Etter &lt;code>kubectl&lt;/code> er installert behøver vi å få påloggingsinformasjon slik at &lt;code>kubectl&lt;/code> kan kommunisere med Kubernetes clusteret.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks get-credentials --resource-group min_aks_rg --name mitt_cluster
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Påloggingsinformasjonen lagres i &lt;code>C:\Users\Brukernavn\.kube\config&lt;/code>. Hold disse filene hemmelig også.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Protip&lt;/strong> Når en har flere ulike Kubernetes clusters kan en bytte hvilken &lt;code>kubectl&lt;/code> skal snakke til med &lt;code>kubectl config get-contexts&lt;/code> og &lt;code>kubectl config set-context mitt_cluster&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="InspiserCluster">&lt;/a>&lt;/p>
&lt;h3 id="inspiser-cluster">Inspiser cluster
&lt;/h3>&lt;p>For å se at clusteret og &lt;code>kubectl&lt;/code> virker begynner vi med noen kommandoer.&lt;/p>
&lt;p>Se alle vertsmaskiner og status:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get nodes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME STATUS AGE VERSION
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-0 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-1 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">aks-nodepool1-16970026-2 Ready 15m v1.8.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Se alle tjenester, pods, deployments:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get all --all-namespaces
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system po/kubernetes-dashboard-6fc8cf9586-frpkn 1/1 Running 0 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system svc/kubernetes-dashboard 10.0.161.132 &amp;lt;none&amp;gt; 80/TCP 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system deploy/kubernetes-dashboard 1 1 1 1 3d
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAMESPACE NAME DESIRED CURRENT READY AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kube-system rs/kubernetes-dashboard-6fc8cf9586 1 1 1 3d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Jeg har bare tatt et lite utdrag fra denne kommandoen. Du behøver ikke å forstå hva alle ressursene i &lt;code>kube-system&lt;/code> namespacet gjør. Det er hensikten at du skal slippe det når Microsoft står for management av selve clusteret.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Namespaces&lt;/strong>
I Kubernetes er det noe som heter Namespaces. Ressurser i ett namespace har ikke automatisk tilgang til ressurser i et annet namespace. Tjenestene som Kubernetes selv benytter installeres i namespacet &lt;code>kube-system&lt;/code>. Kommandoen &lt;code>kubectl&lt;/code> viser deg vanligvis bare ressurser i &lt;code>default&lt;/code> namespace med mindre du spesifiserer &lt;code>--all-namespaces&lt;/code> eller &lt;code>--namespace=xx&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="StarteNginx">&lt;/a>&lt;/p>
&lt;h3 id="starte-noen-nginx-containere">Starte noen nginx containere
&lt;/h3>&lt;blockquote>
&lt;p>En instans av en kjørende container kalles i Kubernetes for en &lt;strong>Pod&lt;/strong>.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;code>nginx&lt;/code> er en rask og fleksibel webserver.&lt;/p>
&lt;/blockquote>
&lt;p>Nå som clusteret er oppe å kjøre kan vi begynne å rulle ut tjenster og deployments på det.&lt;/p>
&lt;p>Vi begynner med å lage en Deployment bestående av 3 containere som alle kjører &lt;code>nginx:mainline-alpine&lt;/code> imaget fra &lt;a class="link" href="https://hub.docker.com/r/_/nginx/" target="_blank" rel="noopener"
>Docker hub&lt;/a>.&lt;/p>
&lt;p>&lt;strong>nginx-dep.yaml&lt;/strong> ser slik ut:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">apiVersion: apps/v1beta2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kind: Deployment
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: nginx-deployment
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> replicas: 3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> selector:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> matchLabels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> template:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> containers:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - name: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> image: nginx:mainline-alpine
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ports:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - containerPort: 80
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Last denne inn på clusteret med &lt;code>kubectl create&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-dep.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Denne kommandoen oppretter ressursene beskrevet i filen. &lt;code>kubectl&lt;/code> kan lese filer enten lokalt fra din maskin eller fra en URL.&lt;/p>
&lt;blockquote>
&lt;p>Etter du har gjort endringer i en ressurs-definisjon (&lt;code>.yaml&lt;/code> fil) kan du oppdatere ressursene i clusteret med &lt;code>kubectl replace -f ressurs.yaml&lt;/code>&lt;/p>
&lt;/blockquote>
&lt;p>Vi kan verifisere at Deployment er klar:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get deploy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment 3 3 3 3 10m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Vi kan også hente de faktiske Pods som er startet:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get pods
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-dqwx5 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-xwzpw 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx-deployment-569477d6d8-z5tfk 1/1 Running 0 10m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Logger&lt;/strong> Vi kan se logger fra én pod med &lt;code>kubectl logs nginx-deployment-569477d6d8-xwzpw&lt;/code>. Men siden vi i dette tilfellet ikke vet hvilken Pod som ender opp med å få innkommende forespørsler kan vi se logger fra alle Pods som har &lt;code>app=nginx&lt;/code> label: &lt;code>kubectl logs -lapp=nginx&lt;/code>. At vi her bruker &lt;code>app=nginx&lt;/code> har vi selv bestemt i &lt;code>nginx-dep.yaml&lt;/code> når vi satt &lt;code>spec.template.metadata.labels: app: nginx&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="NginxService">&lt;/a>&lt;/p>
&lt;h3 id="gjøre-nginx-tilgjengelig-med-en-tjeneste">Gjøre nginx tilgjengelig med en tjeneste
&lt;/h3>&lt;p>For å kommunisere med våre nye Pods behøver vi å opprette en tjeneste (&lt;strong>Service&lt;/strong>). En tjeneste består av en eller flere Pods som velges basert på ulike kriterier, blant annet hvilke labels de har og om Podene det gjelder er Running og Ready.&lt;/p>
&lt;p>Nå lager vi en tjeneste som ruter trafikk til alle Pods som har label &lt;code>app: nginx&lt;/code> og som lytter på port 80. I tillegg gjør vi tjenesten tilgjengelig via en LoadBalancer:&lt;/p>
&lt;p>&lt;strong>nginx-svc.yaml&lt;/strong> ser slik ut:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">apiVersion: v1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kind: Service
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">metadata:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> labels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">spec:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> type: LoadBalancer
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ports:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - port: 80
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> name: http
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> targetPort: 80
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> selector:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> app: nginx
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Vi ber Kubernetes om å opprette tjeneten vår med &lt;code>kubectl create&lt;/code> som vanlig:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl create -f https://raw.githubusercontent.com/StianOvrevage/gh-pages-blog/master/static/attachments/2017-12-23-managed-kubernetes-on-azure/nginx-svc.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Deretter kan vi se hvilken IP-adresse tjenesten vår har fått av Azure:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get svc -w
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nginx 10.0.24.11 13.95.173.255 80:31522/TCP 15m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>PS&lt;/strong> Det kan ta et par minutter for Azure å tildele tjenesten vår en Public IP, i mellomtiden vil det stå &lt;code>&amp;lt;pending&amp;gt;&lt;/code> under EXTERNAL-IP.&lt;/p>
&lt;/blockquote>
&lt;p>En enkel &lt;strong>Welcome to nginx&lt;/strong> webside skal nå være tilgjengelig på http://13.95.173.255 (&lt;em>husk å bytt ut med din egen External-IP&lt;/em>).&lt;/p>
&lt;p>&lt;strong>Vi har nå en lastbalansert &lt;code>nginx&lt;/code> tjeneste med 3 servere klar til å ta imot trafikk.&lt;/strong>&lt;/p>
&lt;p>For ordens skyld kan vi slette tjeneste og deployment etterpå:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">kubectl delete svc nginx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl delete deploy nginx-deployment
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="ScaleCluster">&lt;/a>&lt;/p>
&lt;h3 id="skalere-cluster">Skalere cluster
&lt;/h3>&lt;p>Hvis en ønsker å endre antall vertsmaskiner/noder som kjører Pods kan en gjøre det via Azure-CLI:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks scale --name mitt_cluster --resource-group min_aks_rg --node-count 5
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>For øyeblikket blir alle noder opprettet med samme størrelse som når clusteret ble opprettet. AKS vil antageligvis få støtte for &lt;a class="link" href="https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools" target="_blank" rel="noopener"
>&lt;strong>node-pools&lt;/strong>&lt;/a> i løpet av neste år. Da kan en opprette grupper av noder med forskjellig størrelse og operativsystem, både Linux og Windows.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="DeleteCluster">&lt;/a>&lt;/p>
&lt;h3 id="slette-cluster">Slette cluster
&lt;/h3>&lt;p>En kan slette hele clusteret slik:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">az aks delete --name mitt_cluster --resource-group min_aks_rg --yes
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;a id="Bonusmateriale">&lt;/a>&lt;/p>
&lt;h2 id="bonusmateriale">Bonusmateriale
&lt;/h2>&lt;p>Her er litt bonusmateriale dersom du ønsker å gå enda litt videre med Kubernetes.&lt;/p>
&lt;p>&lt;a id="HelmIntro">&lt;/a>&lt;/p>
&lt;h3 id="rulle-ut-tjenester-med-helm">Rulle ut tjenester med Helm
&lt;/h3>&lt;p>&lt;a class="link" href="https://helm.sh/" target="_blank" rel="noopener"
>Helm&lt;/a> er en pakke-behandler og et bibliotek av programvare som er klart for å rulles ut i et Kubernetes-cluster.&lt;/p>
&lt;p>Start med å laste ned &lt;a class="link" href="https://github.com/kubernetes/helm/releases" target="_blank" rel="noopener"
>Helm-klienten&lt;/a>. Den henter påloggingsinformasjon osv fra samme sted som &lt;code>kubectl&lt;/code> automatisk.&lt;/p>
&lt;p>Installer Helm-serveren (Tiller) på Kubernetes clusteret og oppdater pakke-biblioteket:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">helm init
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm repo update
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Se tilgjengelige pakker (&lt;strong>Charts&lt;/strong>) med: &lt;code>helm search&lt;/code>.&lt;/p>
&lt;p>&lt;a id="HelmMinecraft">&lt;/a>&lt;/p>
&lt;h4 id="rulle-ut-minecraft-med-helm">Rulle ut MineCraft med Helm
&lt;/h4>&lt;p>La oss rulle ut en MineCraft installasjon på clusteret vårt, fordi vi kan :-)&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">helm install --name stian-sin --set minecraftServer.eula=true stable/minecraft
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>&lt;code>--set&lt;/code> overstyrer en eller flere av standardverdiene som er satt i pakken. MineCraft pakken er laget slik at den ikke starter uten å ha sagt seg enig i brukervilkårene i variabelen &lt;code>minecraftServer.eula&lt;/code>. Alle variablene som kan overstyres i MineCraft pakken er &lt;a class="link" href="https://github.com/kubernetes/charts/blob/master/stable/minecraft/values.yaml" target="_blank" rel="noopener"
>dokumentert her&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>Så venter vi litt på at Azure skal tildele en Public IP:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl get svc -w
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">stian-sin-minecraft 10.0.237.0 13.95.172.192 25565:30356/TCP 3m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Og vipps kan vi kople til Minecraft på &lt;code>13.95.172.192:25565&lt;/code>.&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/minecraft-k8s.png"
width="856"
height="507"
srcset="https://demo.stack.jimmycai.com/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/minecraft-k8s_hu13197832490752396686.png 480w, https://demo.stack.jimmycai.com/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/minecraft-k8s_hu16352153299487520650.png 1024w"
loading="lazy"
alt="Kubernetes in MineCraft on Kubernetes"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="405px"
>&lt;/p>
&lt;p>&lt;a id="KubernetesDashboard">&lt;/a>&lt;/p>
&lt;h3 id="kubernetes-dashboard">Kubernetes Dashboard
&lt;/h3>&lt;p>Kubernetes har også et grafisk web-grensesnitt som gjør det litt lettere å se hvilke ressurser som er i clusteret, se logger og åpne remote-shell inne i en kjørende Pod, blant annet.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;gt; kubectl proxy
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Starting to serve on 127.0.0.1:8001
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>kubectl&lt;/code> krypterer og tunnelerer trafikken inn til Kubernetes&amp;rsquo; API servere. Dashboardet er tilgjengelig på &lt;a class="link" href="http://127.0.0.1:8001/ui/" target="_blank" rel="noopener"
>http://127.0.0.1:8001/ui/&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/k8s-dash.png"
width="899"
height="588"
srcset="https://demo.stack.jimmycai.com/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/k8s-dash_hu15091082541702636396.png 480w, https://demo.stack.jimmycai.com/p/managed-kubernetes-p%C3%A5-microsoft-azure-norwegian/k8s-dash_hu17318776918264020386.png 1024w"
loading="lazy"
alt="Kubernetes Dashboard"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>&lt;/p>
&lt;p>&lt;a id="Konklusjon">&lt;/a>&lt;/p>
&lt;h2 id="konklusjon">Konklusjon
&lt;/h2>&lt;p>Jeg håper du har fått mersmak for Kubernetes. Lærekurven kan være litt bratt i begynnelsen men det tar ikke så veldig lang tid før du er produktiv.&lt;/p>
&lt;p>Se på de &lt;a class="link" href="https://v1-8.docs.kubernetes.io/docs/tutorials/" target="_blank" rel="noopener"
>offisielle guidene på Kubernetes.io&lt;/a> for å lære mer om hvordan du definerer forskjellige typer ressurser og tjenester for å kjøre på Kubernetes. &lt;strong>PS: Det gjøres store endringer fra versjon til versjon så sørg for å bruke dokumentasjonen for riktig versjon!&lt;/strong>&lt;/p>
&lt;p>Kubernetes har også et veldig aktivt Slack-miljø på &lt;a class="link" href="http://slack.k8s.io/" target="_blank" rel="noopener"
>kubernetes.slack.com&lt;/a>. Der er det også en kanal for norske Kubernetes brukere; &lt;strong>#norw-users&lt;/strong>.&lt;/p></description></item><item><title>Next generation monitoring with OpenTSDB</title><link>https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/</link><pubDate>Mon, 02 Jun 2014 19:56:40 +0000</pubDate><guid>https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/</guid><description>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure1.png" alt="Featured image of post Next generation monitoring with OpenTSDB" />&lt;h3 id="2021-update-the-specific-tools-discussed-in-this-blog-post-should-be-considered-obsolete-by-todays-standards-you-should-investigate-prometheus-influxdb-and-timescaledb-for-your-monitoring-needs">2021 Update: The specific tools discussed in this blog post should be considered obsolete by todays standards. You should investigate Prometheus, InfluxDB and TimescaleDB for your monitoring needs.
&lt;/h3>&lt;p>In this paper we will provide a step by step guide on how to install a single-instance of &lt;strong>OpenTSDB&lt;/strong> using the latest versions of the underlying technology, &lt;strong>Hadoop&lt;/strong> and &lt;strong>HBase&lt;/strong>. We will also provide some background on the state of existing monitoring solutions.&lt;/p>
&lt;p>&lt;a id="Abstract">&lt;/a>&lt;/p>
&lt;p>Table of contents&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="#Abstract" >Abstract&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Background" >Background&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Performanceproblems" >Performance problems - Welcome to I/O-hell&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Scaling" >Scaling problems&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Loss" >Loss of detail&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#flexibility" >Lack of flexibility&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#revolution" >The monitoring revolution&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Debian" >Setting up a single node OpenTSDB instance on Debian 7 Wheezy&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Hardware" >Hardware requirements&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Operating" >Operating system requirements&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#preparations" >Pre-setup preparations&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#java" >Installing java from packages&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#HBase" >Installing HBase&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#snappy" >Install snappy&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#native" >Building native libhadoop and libsnappy&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#ConfiguringHBase" >Configuring HBase&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#compression" >Testing HBase and compression&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#StartingHBase" >Starting HBase&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#InstallingOpenTSDB" >Installing OpenTSDB&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#ConfiguringOpenTSDB" >Configuring OpenTSDB&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#HBasetables" >Creating HBase tables&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#StartingOpenTSDB" >Starting OpenTSDB&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Feeding" >Feeding data into OpenTSDB&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#tcollector" >tcollector&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#peritus-tc-tools" >peritus-tc-tools&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#collectd-opentsdb" >collectd-opentsdb&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#MonitoringOpenTSDB" >Monitoring OpenTSDB&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a class="link" href="#Performancecomparison" >Performance comparison&lt;/a>
&lt;ul>
&lt;li>&lt;a class="link" href="#Collection" >Collection&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Storage" >Storage&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="#Conclusion" >Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="Background">&lt;/a>&lt;/p>
&lt;h2 id="background">Background
&lt;/h2>&lt;p>Since its inception in 1999 &lt;a class="link" href="http://oss.oetiker.ch/rrdtool/" target="_blank" rel="noopener"
>&lt;strong>rrdtool&lt;/strong>&lt;/a> (the underlying storage mechanism of once universal &lt;strong>MRTG&lt;/strong>) has been the base of many popular monitoring solutions; &lt;strong>Cacti&lt;/strong>, &lt;strong>collectd&lt;/strong>, &lt;strong>Ganglia&lt;/strong>, &lt;strong>Munin&lt;/strong>, &lt;strong>Observium&lt;/strong>, &lt;strong>OpenNMS&lt;/strong> and &lt;strong>Zenoss&lt;/strong>, to name a few.&lt;/p>
&lt;p>There are a number of problems with the current approach and we will highlight some of these here.&lt;/p>
&lt;p>Please note that this includes &lt;strong>Graphite&lt;/strong> and its backend &lt;strong>Whisper&lt;/strong>, which is based on the &lt;a class="link" href="http://graphite.readthedocs.org/en/0.9.10/whisper.html" target="_blank" rel="noopener"
>same basic design as rrdtool&lt;/a> and has &lt;a class="link" href="http://dieter.plaetinck.be/on-graphite-whisper-and-influxdb.html" target="_blank" rel="noopener"
>some of the same limitations&lt;/a>.&lt;/p>
&lt;p>&lt;a id="Performanceproblems">&lt;/a>&lt;/p>
&lt;h3 id="performance-problems---welcome-to-io-hell">Performance problems - Welcome to I/O-hell
&lt;/h3>&lt;p>When MRTG and rrdtool was created the preservation of disk space was more important than preservation of disk operations and the default collection interval was 5 minutes (which many are still using). The way rrdtool is designed it requires quite a few random reads and writes per datapoint. It also re-reads, computes the average, and writes old data again according to the RRA rules defined which causes additional I/O load. In 2014 memory is cheap, disk storage is cheap and CPU is fairly cheap. Disk I/O operations (IOPS) however are still very expensive in terms of hardware. The recent maturing of SSD provides extreme amounts of IOPS for a reasonable price, but the drive sizes are fractional. The result is that in order to scale IOPS-wise you currently need many low-space SSDs to get the required space, or many low-IOPS spindle drives to get the required IOPS:&lt;/p>
&lt;p>&lt;a class="link" href="http://www.newegg.com/Product/Product.aspx?Item=N82E16820147251" target="_blank" rel="noopener"
>Samsung EVO 840 1TB SSD&lt;/a> - 98.000 IOPS - 470 USD&lt;/p>
&lt;p>&lt;a class="link" href="http://www.newegg.com/Product/Product.aspx?Item=N82E16822148844" target="_blank" rel="noopener"
>Seagate Barracuda 3TB&lt;/a> - 240 IOPS - 110 USD&lt;/p>
&lt;p>You would need $44.880 (408 drives) worth of spindle drives in order to match a single SSD drive in terms of I/O-performance. On the other hand a $2.000 array of spindle drives would get you a net ~54 TB of space. The cost of SSD to reach the same volume would be $25.380. Not to mention the cost of servers, power, provisioning, etc.&lt;/p>
&lt;p>&lt;strong>Note: This is the cheapest available bulk consumer drives and comparable OEM drives (&lt;a class="link" href="http://h30094.www3.hp.com/product/sku/10350615/mfg_partno/632494-B21" target="_blank" rel="noopener"
>SSD&lt;/a>, &lt;a class="link" href="http://h30094.www3.hp.com/product/sku/10389145/mfg_partno/628061-B21" target="_blank" rel="noopener"
>spindle&lt;/a>) for a HP server will be 6 to 30 times more expensive.&lt;/strong>&lt;/p>
&lt;p>In rrdtool version 1.4, released in 2009, &lt;strong>rrdcached&lt;/strong> was introduced as a caching daemon for buffering multiple data updates and reducing the number of random I/O operations by writing several related datapoints in sequence. It took a couple of years before this new feature was implemented in most of the common open source monitoring solutions.&lt;/p>
&lt;p>For a good introduction into the internals of rrdtool/rrdcached updates and the problems with I/O scaling look at presentation by Sebastian Harl, &lt;a class="link" href="http://www.netways.de/index.php?id=2815" target="_blank" rel="noopener"
>How to Escape the I/O Hell&lt;/a>&lt;/p>
&lt;p>&lt;a id="Scaling">&lt;/a>&lt;/p>
&lt;h3 id="scaling-problems">Scaling problems
&lt;/h3>&lt;p>Most of today&amp;rsquo;s monitoring systems do not easily scale-out. Scale-out, or scaling horizontally, is when you can add new nodes in response to increased load. Scaling up by replacing existing hardware with state-of-the-art hardware is both expensive and only buys you limited time before the next even more expensive necessary hardware upgrade. Many systems offer distributed polling but none offer the option of spreading out the disk load. For example; you can &lt;a class="link" href="http://community.zenoss.org/docs/DOC-2485" target="_blank" rel="noopener"
>scale Zenozz for High Availability&lt;/a> but not performance.&lt;/p>
&lt;p>&lt;a id="Loss">&lt;/a>&lt;/p>
&lt;h3 id="loss-of-detail">Loss of detail
&lt;/h3>&lt;p>Current RRD based systems will aggregate old data into averages in order to save storage space. Most technicians do not have the in depth knowledge in order to tune the rules for aggregation and will leave the default values as is. Using cacti as an example and looking at the &lt;a class="link" href="http://docs.cacti.net/manual:088:8_rrdtool#rrd_files" target="_blank" rel="noopener"
>cacti documentation&lt;/a> we see that in a very short time, 2 months, data is averaged to a single data point PER DAY. For systems such as Internet backbones where traffic vary a lot from bottom (30% utilization for example) to peak (90% utilization for example) during a day only the average of 60% is shown in the graphs. This in turn makes troubleshooting by comparing old data difficult. It makes trending based on peaks/bottoms impossible and it may also lead to wrong or delayed strategic decisions on where to invest in added capacity.&lt;/p>
&lt;p>&lt;a id="flexibility">&lt;/a>&lt;/p>
&lt;h3 id="lack-of-flexibility">Lack of flexibility
&lt;/h3>&lt;p>In order to collect, store and graph new kinds of metrics an operator would need a certain level of programming skills and experience with the internals of the monitoring system. Adding new metrics to the systems would range from hours to weeks depending on the skill and experience of the operator. Creating new graphs based on existing metrics is also very difficult on most systems. And not within reach for the average operator.&lt;/p>
&lt;p>&lt;a id="revolution">&lt;/a>&lt;/p>
&lt;h2 id="the-monitoring-revolution">The monitoring revolution
&lt;/h2>&lt;p>We are currently at the beginning of a monitoring revolution. The advent of cloud computing and big data has created a need for measuring lots of metrics for thousands of machines at small intervals. This has sparked the creation of completely new monitoring components. One of the components where we now have improved alternatives is for efficient metric storage.&lt;/p>
&lt;p>The first is &lt;strong>&lt;a class="link" href="http://opentsdb.net/" target="_blank" rel="noopener"
>OpenTSDB&lt;/a>&lt;/strong>, a &amp;ldquo;Scalable, Distributed, Time Series Database&amp;rdquo; that begun development at &lt;a class="link" href="https://www.stumbleupon.com/" target="_blank" rel="noopener"
>StumbleUpon&lt;/a> in 2011 and aimed at solving some of the problems with existing monitoring systems. OpenTSDB is built in top of Apache HBase which is a scalable and performant database that builds on top of Apache Hadoop. Hadoop is a series of tools for building large and scalable distributed systems. Back in 2010 Facebook already had &lt;a class="link" href="http://hadoopblog.blogspot.no/2010/05/facebook-has-worlds-largest-hadoop.html" target="_blank" rel="noopener"
>2000 machines in a Hadoop cluster&lt;/a> with 21PB (that is 21.000.000 GB) of combined storage.&lt;/p>
&lt;p>The second is an interesting newcommer, &lt;a class="link" href="http://influxdb.com/" target="_blank" rel="noopener"
>&lt;strong>InfluxDB&lt;/strong>&lt;/a>, that began development in 2013 and has the goal of offering scalability and performance without the requirements of HBase/Hadoop.&lt;/p>
&lt;p>In addition to advances in performance these alternatives also decouple storage of metrics and display of graphs and abstract the interaction in simple and well-defined APIs. This makes it easy for developers to create improved frontends rapidly and this has already resulted in several very attractive open-source frontends such as &lt;strong>&lt;a class="link" href="https://github.com/Ticketmaster/Metrilyx-2.0" target="_blank" rel="noopener"
>Metrilyx&lt;/a>&lt;/strong> (OpenTSDB), &lt;strong>&lt;a class="link" href="http://grafana.org/" target="_blank" rel="noopener"
>Grafana&lt;/a>&lt;/strong> (InfluxDB, Graphite, &lt;a class="link" href="https://github.com/grafana/grafana/pull/211" target="_blank" rel="noopener"
>soon OpenTSDB&lt;/a>), &lt;strong>&lt;a class="link" href="http://www.statuswolf.com/" target="_blank" rel="noopener"
>StatusWolf&lt;/a>&lt;/strong> (OpenTSDB), &lt;strong>&lt;a class="link" href="https://github.com/hakobera/influga" target="_blank" rel="noopener"
>Influga&lt;/a>&lt;/strong> (InfluxDB).&lt;/p>
&lt;p>&lt;a id="Debian">&lt;/a>&lt;/p>
&lt;h2 id="setting-up-a-single-node-opentsdb-instance-on-debian-7-wheezy">Setting up a single node OpenTSDB instance on Debian 7 Wheezy
&lt;/h2>&lt;p>In the rest of this paper we will set up a single node OpenTSDB instance. OpenTSDB builds on top of HBase and Hadoop and scales to very large setups easily. But it also delivers substantial performance on a single node which is deployed in &lt;strong>less than an hour&lt;/strong>. There are plenty of guides on installing a Hadoop cluster but here we will focus on the natural first step of getting a single node running using &lt;strong>recent releases&lt;/strong> of the relevant software:&lt;/p>
&lt;ul>
&lt;li>OpenTSDB 2.0.0 - Released 2014-05-05&lt;/li>
&lt;li>HBase 0.98.2 - Released 2014-05-01&lt;/li>
&lt;li>Hadoop 2.4.0 - Released 2014-04-07&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>If you later require to deploy a larger cluster consider using a framework such as &lt;a class="link" href="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html" target="_blank" rel="noopener"
>&lt;strong>Cloudera CDH&lt;/strong>&lt;/a> or &lt;a class="link" href="http://hortonworks.com/hdp/" target="_blank" rel="noopener"
>&lt;strong>Hortonworks HDP&lt;/strong>&lt;/a> which are open-source platforms which package Apache Hadoop components and provides a fully tested environment and easy-to-use graphical frontends for configuration and management. It is &lt;a class="link" href="http://opentsdb.net/setup-hbase.html" target="_blank" rel="noopener"
>recommended to have at least 5 machines&lt;/a> in a HBase cluster supporting OpenTSDB.&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;blockquote>
&lt;p>This guide assumes you are somewhat familiar with using a Linux shell/command prompt.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="Hardware">&lt;/a>&lt;/p>
&lt;h4 id="hardware-requirements">Hardware requirements
&lt;/h4>&lt;ul>
&lt;li>CPU cores: Max (Limit to 50% of your available CPU resources)&lt;/li>
&lt;li>RAM: Min 16 GB&lt;/li>
&lt;li>Disk 1 - OS: 10 GB - Thin provisioned&lt;/li>
&lt;li>Disk 2 - Data: 100 GB - Thin provisioned&lt;/li>
&lt;/ul>
&lt;p>&lt;a id="Operating">&lt;/a>&lt;/p>
&lt;h4 id="operating-system-requirements">Operating system requirements
&lt;/h4>&lt;p>This guide is based on a recently installed Debian 7 Wheezy &lt;strong>64bit&lt;/strong> installed without any extra packages. See the &lt;a class="link" href="https://www.debian.org/releases/stable/amd64/" target="_blank" rel="noopener"
>official documentation&lt;/a> for more information.&lt;/p>
&lt;p>All commands are entered as &lt;strong>root&lt;/strong> user unless otherwise noted.&lt;/p>
&lt;p>&lt;a id="preparations">&lt;/a>&lt;/p>
&lt;h4 id="pre-setup-preparations">Pre-setup preparations
&lt;/h4>&lt;p>We start by installing a few tools that we will need later.&lt;/p>
&lt;pre>&lt;code>apt-get install wget make gcc g++ cmake maven
&lt;/code>&lt;/pre>
&lt;p>Create a new ext3 partition on the data disk &lt;strong>/dev/sdb&lt;/strong>:&lt;/p>
&lt;pre>&lt;code>(echo &amp;quot;n&amp;quot;; echo &amp;quot;p&amp;quot;; echo &amp;quot;&amp;quot;; echo &amp;quot;&amp;quot;; echo &amp;quot;&amp;quot;; echo &amp;quot;t&amp;quot;; echo &amp;quot;83&amp;quot;; echo &amp;quot;w&amp;quot;) | fdisk /dev/sdb
mkfs.ext3 /dev/sdb1
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>ext3 is the &lt;a class="link" href="https://wiki.apache.org/hadoop/DiskSetup" target="_blank" rel="noopener"
>recommended filesystem for Hadoop&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>Create a mountpoint &lt;strong>/mnt/data1&lt;/strong> and add it to the file system table and mount the disk:&lt;/p>
&lt;pre>&lt;code>mkdir /mnt/data1
echo &amp;quot;/dev/sdb1 /mnt/data1 ext3 auto,noexec,noatime,nodiratime 0 1&amp;quot; | tee -a /etc/fstab
mount /mnt/data1
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>Using &lt;strong>noexec&lt;/strong> for the data partition will increase security as nothing on the data partition will be allowed to ever execute.
&lt;br />
Using &lt;strong>noatime&lt;/strong> and &lt;strong>nodiratime&lt;/strong> increases performance since the read access timestamps are not updated on every file access.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="java">&lt;/a>&lt;/p>
&lt;h4 id="installing-java-from-packages">Installing java from packages
&lt;/h4>&lt;p>Installing java on Linux can be quite challenging due to licensing issues, but thanks to the guys over at &lt;a class="link" href="https://launchpad.net/" target="_blank" rel="noopener"
>Launchpad.net&lt;/a> who are providing a repository with a custom java package this can now be done quite easy.&lt;/p>
&lt;p>We start by adding the launchpad java repository to our &lt;em>&lt;strong>/etc/apt/sources.list&lt;/strong>&lt;/em> file:&lt;/p>
&lt;pre>&lt;code>echo &amp;quot;deb http://ppa.launchpad.net/webupd8team/java/ubuntu precise main&amp;quot; | tee -a /etc/apt/sources.list
echo &amp;quot;deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu precise main&amp;quot; | tee -a /etc/apt/sources.list
&lt;/code>&lt;/pre>
&lt;p>Add the signing key and download information from the new repository:&lt;/p>
&lt;pre>&lt;code>apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys EEA14886
apt-get update
&lt;/code>&lt;/pre>
&lt;p>Run the java installer:&lt;/p>
&lt;pre>&lt;code>apt-get install oracle-java7-installer
&lt;/code>&lt;/pre>
&lt;p>Follow the instructions on screen to complete the Java 7 installation.&lt;/p>
&lt;p>&lt;a id="HBase">&lt;/a>&lt;/p>
&lt;h3 id="installing-hbase">Installing HBase
&lt;/h3>&lt;p>OpenTSDB has its own HBase installation tutorial &lt;a class="link" href="http://opentsdb.net/setup-hbase.html" target="_blank" rel="noopener"
>here&lt;/a>. It is very brief and does not use the latest versions or snappy compression.&lt;/p>
&lt;p>Download and unpack HBase:&lt;/p>
&lt;pre>&lt;code>cd /opt
wget http://apache.vianett.no/hbase/hbase-0.98.2/hbase-0.98.2-hadoop2-bin.tar.gz
tar xvfz hbase-0.98.2-hadoop2-bin.tar.gz
export HBASEDIR=`pwd`/hbase-0.98.2-hadoop2/
&lt;/code>&lt;/pre>
&lt;p>Increase the system-wide limitations of open files and processes from the default of 1000 to 32000 by adding a few lines to &lt;em>&lt;strong>/etc/security/limits.conf&lt;/strong>&lt;/em>:&lt;/p>
&lt;pre>&lt;code>echo &amp;quot;root - nofile 32768&amp;quot; | tee -a /etc/security/limits.conf
echo &amp;quot;root soft/hard nproc 32000&amp;quot; | tee -a /etc/security/limits.conf
echo &amp;quot;* - nofile 32768&amp;quot; | tee -a /etc/security/limits.conf
echo &amp;quot;* soft/hard nproc 32000&amp;quot; | tee -a /etc/security/limits.conf
&lt;/code>&lt;/pre>
&lt;p>The settings above will only take effect if we also add a line to &lt;em>&lt;strong>/etc/pam.d/common-session&lt;/strong>&lt;/em>:&lt;/p>
&lt;pre>&lt;code>echo &amp;quot;session required pam_limits.so&amp;quot; | tee -a /etc/pam.d/common-session
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="snappy">&lt;/a>&lt;/p>
&lt;h4 id="install-snappy">Install snappy
&lt;/h4>&lt;p>&lt;a class="link" href="https://code.google.com/p/snappy/" target="_blank" rel="noopener"
>Snappy&lt;/a> is a compression algorithm that values speed over compression ratio and this makes it a good choice for high throughput applications such as Hadoop/HBase. Due to licensing issues Snappy does not ship with HBase and need to be installed on top.&lt;/p>
&lt;p>The installation process is a bit complicated and has caused headache for many people (me included). Here we will show a method of installing snappy and getting it to work with the latest version of HBase and Hadoop.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Compression algorithms in HBase&lt;/strong>
Compression is the method of reducing the size of a file or text without losing any of the contents. There are many compression algorithms available and some focus on being able to create the smallest compressed file at the cost of time and CPU usage while other achieve &lt;em>reasonable&lt;/em> compression ratio while being very fast.
&lt;br /> &lt;br />
Out of the box HBase supports gz(gzip/zlib), snappy and lzo. Only gz is included due to licensing issues.
Unfortunately gz is a slow and costly algorithm compared to snappy and lzo. In a test performed by Yahoo (see &lt;a class="link" href="http://www.slideshare.net/Hadoop_Summit/singh-kamat-june27425pmroom210c" target="_blank" rel="noopener"
>slides here&lt;/a>, page 8) gz achieves 64% compression in 32 seconds. lzo 47% in 4.8 seconds and snappy 42% in 4.0 seconds. lz4 is another protocol &lt;a class="link" href="http://search-hadoop.com/m/KFLWV1PFVhp1" target="_blank" rel="noopener"
>considered for inclusion&lt;/a> that is even faster (2.4 seconds) but requires much more memory.
&lt;br /> &lt;br />
&lt;em>For more information look at the &lt;a class="link" href="https://hbase.apache.org/book/compression.html" target="_blank" rel="noopener"
>Apache HBase Handbook - Appendix C - Compression&lt;/a>&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="native">&lt;/a>&lt;/p>
&lt;h4 id="building-native-libhadoop-and-libsnappy">Building native libhadoop and libsnappy
&lt;/h4>&lt;p>In order to use compression we need the common Hadoop library, libhadoop.so, and the snappy library, libsnappy.so. HBase ships without libhadoop.so and the libhadoop.so that ships in the Hadoop Package is only for 32 bit OS. So we need to compile these files ourself.&lt;/p>
&lt;p>Start by downloading and installing ProtoBuf. Hadoop requres version 2.5+ which is not available as a Debian package unfortunately.&lt;/p>
&lt;pre>&lt;code>wget --no-check-certificate https://protobuf.googlecode.com/files/protobuf-2.5.0.tar.gz
tar zxvf protobuf-2.5.0.tar.gz
cd protobuf-2.5.0
./configure; make; make install
export LD_LIBRARY_PATH=/usr/local/lib/
&lt;/code>&lt;/pre>
&lt;p>Download and compile Hadoop:&lt;/p>
&lt;pre>&lt;code>apt-get install zlib1g-dev
wget http://apache.uib.no/hadoop/common/hadoop-2.4.0/hadoop-2.4.0-src.tar.gz
tar zxvf hadoop-2.4.0-src.tar.gz
cd hadoop-2.4.0-src/hadoop-common-project/
mvn package -Pdist,native -Dskiptests -Dtar -Drequire.snappy -DskipTests
&lt;/code>&lt;/pre>
&lt;p>Copy the newly compiled native libhadoop library into /usr/local/lib, then create the folder in which HBase looks for it and create a shortcut from there to /usr/local/lib/libhadoop.so:&lt;/p>
&lt;pre>&lt;code>cp hadoop-common/target/native/target/usr/local/lib/libhadoop.* /usr/local/lib
mkdir -p $HBASEDIR/lib/native/Linux-amd64-64/
cd $HBASEDIR/lib/native/Linux-amd64-64/
ln -s /usr/local/lib/libhadoop.so* .
&lt;/code>&lt;/pre>
&lt;p>Install snappy from Debian packages:&lt;/p>
&lt;pre>&lt;code>apt-get install libsnappy-dev
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="ConfiguringHBase">&lt;/a>&lt;/p>
&lt;h4 id="configuring-hbase">Configuring HBase
&lt;/h4>&lt;p>Now we need to do some basic configuration before we can start HBase. The configuration files are in $HBASEDIR/conf/.&lt;/p>
&lt;p>&lt;a id="hbase-env.sh">&lt;/a>&lt;/p>
&lt;h4 id="confhbase-envsh">&lt;strong>conf/hbase-env.sh&lt;/strong>
&lt;/h4>&lt;p>A shell script setting various environment variables related to how HBase and Java should behave. The file contains a lot of options and they are all documented by comments so feel free to look around in it.&lt;/p>
&lt;p>Start by setting the JAVA_HOME, which points to where Java is installed:&lt;/p>
&lt;pre>&lt;code>export JAVA_HOME=/usr/lib/jvm/java-7-oracle/
&lt;/code>&lt;/pre>
&lt;p>Then increase the size of the &lt;a class="link" href="http://pubs.vmware.com/vfabric52/index.jsp?topic=/com.vmware.vfabric.em4j.1.2/em4j/conf-heap-management.html" target="_blank" rel="noopener"
>Java Heap&lt;/a> from the default of 1000 which is a bit low:&lt;/p>
&lt;pre>&lt;code>export HBASE_HEAPSIZE=8000
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="Background">&lt;/a>&lt;/p>
&lt;h4 id="confhbase-sitexml">&lt;strong>conf/hbase-site.xml&lt;/strong>
&lt;/h4>&lt;p>An XML file containing HBase specific configuration parameters.&lt;/p>
&lt;pre>&lt;code>&amp;lt;configuration&amp;gt;
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;/mnt/data1/hbase&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;hbase.zookeeper.property.dataDir&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;/mnt/data1/zookeeper&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="compression">&lt;/a>&lt;/p>
&lt;h4 id="testing-hbase-and-compression">Testing HBase and compression
&lt;/h4>&lt;p>Now that we have installed snappy and configured HBase we can verify that HBase is working and that the compression is loaded by doing:&lt;/p>
&lt;pre>&lt;code>$HBASEDIR/bin/hbase org.apache.hadoop.hbase.util.CompressionTest /tmp/test.txt snappy
&lt;/code>&lt;/pre>
&lt;p>This should output some lines with information and end with &lt;strong>SUCCESS&lt;/strong>.&lt;/p>
&lt;p>&lt;a id="StartingHBase">&lt;/a>&lt;/p>
&lt;h4 id="starting-hbase">Starting HBase
&lt;/h4>&lt;p>HBase ships with scripts for starting and stopping it, namely start-hbase.sh and stop-hbase.sh. You start HBase with&lt;/p>
&lt;pre>&lt;code>$HBASEDIR/bin/start-hbase.sh
&lt;/code>&lt;/pre>
&lt;p>Then look at the log to ensure it has started without any serious errors:&lt;/p>
&lt;pre>&lt;code>tail -fn100 $HBASEDIR/bin/../logs/hbase-root-master-opentsdb.log
&lt;/code>&lt;/pre>
&lt;p>If you want HBase to start automatically on boot you can use a process management tool such as &lt;a class="link" href="http://mmonit.com/monit/" target="_blank" rel="noopener"
>Monit&lt;/a> or simply put it in &lt;em>&lt;strong>/etc/rc.local&lt;/strong>&lt;/em>:&lt;/p>
&lt;pre>&lt;code>/opt/hbase-0.98.2-hadoop2/bin/start-hbase.sh
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="InstallingOpenTSDB">&lt;/a>&lt;/p>
&lt;h3 id="installing-opentsdb">Installing OpenTSDB
&lt;/h3>&lt;p>Start by installing gnuplot, which is used by the native webui to draw graphs:&lt;/p>
&lt;pre>&lt;code>apt-get install gnuplot
&lt;/code>&lt;/pre>
&lt;p>Then download and install OpenTSDB:&lt;/p>
&lt;pre>&lt;code>wget https://github.com/OpenTSDB/opentsdb/releases/download/v2.0.0/opentsdb-2.0.0_all.deb
dpkg -i opentsdb-2.0.0_all.deb
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="ConfiguringOpenTSDB">&lt;/a>&lt;/p>
&lt;h4 id="configuring-opentsdb">Configuring OpenTSDB
&lt;/h4>&lt;p>The configuration file is &lt;em>&lt;strong>/etc/opentsdb/opentsdb.conf&lt;/strong>&lt;/em>. It has some of the basic configuration parameters but not nearly all of them. &lt;a class="link" href="http://opentsdb.net/docs/build/html/user_guide/configuration.html" target="_blank" rel="noopener"
>Here is the official documentation with all configuration parameters&lt;/a>.&lt;/p>
&lt;p>The defaults are reasonable but we need to make a few tweaks, the first is to add this:&lt;/p>
&lt;pre>&lt;code>tsd.core.auto_create_metrics = true
&lt;/code>&lt;/pre>
&lt;p>This will make OpenTSDB accept previously unseen metrics and add them to the database. This is very useful in the beginning when feeding data into OpenTSDB. Without this you will have to use the command &lt;em>&lt;strong>mkmetric&lt;/strong>&lt;/em> for each metric you will store and get errors that might be hard to trace if the metric you create do not match what is actually sent.&lt;/p>
&lt;p>Then we will add support for chunked requests via the HTTP API:&lt;/p>
&lt;pre>&lt;code>tsd.http.request.enable_chunked = true
tsd.http.request.max_chunk = 16000
&lt;/code>&lt;/pre>
&lt;p>Some tools and plugins (such as our own &lt;a class="link" href="https://github.com/PeritusConsulting/collectd-opentsdb" target="_blank" rel="noopener"
>improved collectd to OpenTSDB plugin&lt;/a>) send multiple data points in a single HTTP request for increased efficiency and requires this setting to be enabled.&lt;/p>
&lt;p>&lt;a id="HBasetables">&lt;/a>&lt;/p>
&lt;h4 id="creating-hbase-tables">Creating HBase tables
&lt;/h4>&lt;p>Before we start OpenTSDB we need to create the necessary tables in HBase:&lt;/p>
&lt;pre>&lt;code>env COMPRESSION=SNAPPY HBASE_HOME=$HBASEDIR /usr/share/opentsdb/tools/create_table.sh
&lt;/code>&lt;/pre>
&lt;p>&lt;a id="StartingOpenTSDB">&lt;/a>&lt;/p>
&lt;h4 id="starting-opentsdb">Starting OpenTSDB
&lt;/h4>&lt;p>Since version 2.0.0 OpenTSDB ships as a Debian package and includes SysV init scripts. To start OpenTSDB as a daemon running in the background we run:&lt;/p>
&lt;pre>&lt;code>service opentsdb start
&lt;/code>&lt;/pre>
&lt;p>And then check the logs for any errors or other relevant information:&lt;/p>
&lt;pre>&lt;code>tail -f /var/log/opentsdb/opentsdb.log
&lt;/code>&lt;/pre>
&lt;p>If the server is started successfully the last line of the log should say:&lt;/p>
&lt;pre>&lt;code>13:42:30.900 INFO [TSDMain.main] - Ready to serve on /0.0.0.0:4242
&lt;/code>&lt;/pre>
&lt;p>And you can now browse to your new OpenTSDB in a browser using http://hostname:4242 !&lt;/p>
&lt;p>&lt;a id="Feeding">&lt;/a>&lt;/p>
&lt;h3 id="feeding-data-into-opentsdb">Feeding data into OpenTSDB
&lt;/h3>&lt;p>It is not within the scope of this paper to go into details about how to feed data into OpenTSDB but we will give a quick introduction here to get you started.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>A note on metric naming in OpenTSDB&lt;/strong>
&lt;br /> &lt;br />
Each datapoint has a metric name such as &lt;em>&lt;strong>df.bytes.free&lt;/strong>&lt;/em> and one or more tags such as &lt;em>&lt;strong>host=server1&lt;/strong>&lt;/em> and &lt;em>&lt;strong>mount=/mnt/data1&lt;/strong>&lt;/em>. This is closer to the proposed &lt;a class="link" href="http://metrics20.org/" target="_blank" rel="noopener"
>Metrics 2.0&lt;/a> standard for naming metrics than the traditional naming of &lt;em>&lt;strong>df.bytes.free.server1.mnt-data&lt;/strong>&lt;/em>. This makes it possible to create aggregates across tags and combine data easily using the tags.
&lt;br /> &lt;br />
OpenTSDB stores each datapoint with a given metric and tags in one HBase row per hour. But due to a HBase issue it still has to scan every row that matches the metric, ignoring the tags. Even though it will only return the data also matching the tags. This results in very much data being read and it will be very slow to read if there is a large number of data points for a given metric. The default for the collectd-opentsdb plugin is to use the read plugin name as metric, and other values as tags. In my case this results in 72.000.000 datapoints per hour for this metric. When generating a graph all of this data has to be read and evaluated before drawing a graph. 24 hours of data is over 1.7 billion datapoints for this single metric and results in a read performance of 5-15 &lt;strong>minutes&lt;/strong> for a simple graph.
&lt;br /> &lt;br />
A solution to this is to use &lt;em>shift-to-metric&lt;/em>, as &lt;a class="link" href="http://opentsdb.net/docs/build/html/user_guide/writing.html" target="_blank" rel="noopener"
>mentioned in the OpenTSDB user guide&lt;/a>. Shift-to-metric is simply moving one or more data identifiers from tags to the metric in order to reduce the cardinality (number of values) for a metric, and hence the time required to read out the data we want. We have modified the collectd-opentsdb java plugin in order to shift the tags to metrics, and this increases read-performance by ~1000x down to 10-100ms. Read the section about collectd below for more information on our modified plugin.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a id="tcollector">&lt;/a>&lt;/p>
&lt;h4 id="tcollector">tcollector
&lt;/h4>&lt;p>&lt;a class="link" href="http://opentsdb.net/docs/build/html/user_guide/utilities/tcollector.html" target="_blank" rel="noopener"
>tcollector&lt;/a> is the default agent for collecting and sending data from a Linux server to a OpenTSDB server. It is based on Python and plugins / addons can be written in any language. It ships with the most common plugins to collect information about disk usage and performance, cpu and memory statistics and also for some specific systems such as mysql, mongodb, riak, varnish, postgresql and others. tcollector is very lightweight and features advanced de-duplication in order to reduce unneeded network traffic.&lt;/p>
&lt;p>The commands for installing dependencies and downloading tcollector are&lt;/p>
&lt;pre>&lt;code>aptitude install git python
cd /opt
git clone git://github.com/OpenTSDB/tcollector.git
&lt;/code>&lt;/pre>
&lt;p>Configuration is in the startup script &lt;em>&lt;strong>tcollector/startstop&lt;/strong>&lt;/em>, you will need to uncomment and set the value of TSD_HOST to point to your OpenTSDB server.&lt;/p>
&lt;p>To start it run&lt;/p>
&lt;pre>&lt;code>/opt/tcollector/startstop start
&lt;/code>&lt;/pre>
&lt;p>This is also the command you want to add to &lt;em>&lt;strong>/etc/rc.local&lt;/strong>&lt;/em> in order to have the agent automatically start at boot. Logfiles are saved in &lt;em>&lt;strong>/var/log/tcollector.log&lt;/strong>&lt;/em> and they are rotated automatically.&lt;/p>
&lt;p>&lt;a id="peritus-tc-tools">&lt;/a>&lt;/p>
&lt;h4 id="peritus-tc-tools">peritus-tc-tools
&lt;/h4>&lt;p>We have developed a set of &lt;strong>tcollector&lt;/strong> plugins for collecting statistics from&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;a class="link" href="https://www.isc.org/downloads/dhcp/" target="_blank" rel="noopener"
>ISC DHCPd server&lt;/a>&lt;/strong>, about number of DHCP events and DHCP pool sizes&lt;/li>
&lt;li>&lt;strong>&lt;a class="link" href="http://www.opensips.org/" target="_blank" rel="noopener"
>OpenSIPS&lt;/a>&lt;/strong>, total number of subscribers and registered user agents&lt;/li>
&lt;li>&lt;strong>&lt;a class="link" href="http://atmail.com/" target="_blank" rel="noopener"
>Atmail&lt;/a>&lt;/strong>, number of users, admins, sent and received emails, logins and errors&lt;/li>
&lt;/ul>
&lt;p>As well as a high performance replacement for &lt;strong>&lt;a class="link" href="http://oss.oetiker.ch/smokeping/" target="_blank" rel="noopener"
>smokeping&lt;/a>&lt;/strong> called &lt;strong>tc-ping&lt;/strong>.&lt;/p>
&lt;p>These plugins are available for download from our &lt;strong>&lt;a class="link" href="https://github.com/PeritusConsulting/peritus-tc-tools" target="_blank" rel="noopener"
>GitHub page&lt;/a>&lt;/strong>.&lt;/p>
&lt;p>&lt;a id="collectd-opentsdb">&lt;/a>&lt;/p>
&lt;h4 id="collectd-opentsdb">collectd-opentsdb
&lt;/h4>&lt;p>&lt;a class="link" href="http://collectd.org/" target="_blank" rel="noopener"
>collectd&lt;/a> is the &lt;em>system statistics collection daemon&lt;/em> and is a widely used system for collecting metrics from various sources. There are several options for sending data from collectd to OpenTSDB but one way that works well is to use the &lt;a class="link" href="https://github.com/auxesis/collectd-opentsdb" target="_blank" rel="noopener"
>collectd-opentsdb java write plugin&lt;/a>.&lt;/p>
&lt;p>Since collectd is a generic metric collection tool the original collectd-opentsdb plugin will use the plugin name (such as &lt;strong>snmp&lt;/strong>) as the metric, and use tags such as &lt;strong>host=servername&lt;/strong>, &lt;strong>plugin_instance=ifHcInOctets&lt;/strong> and &lt;strong>type_instance=FastEthernet0/1&lt;/strong>.&lt;/p>
&lt;p>As mentioned in the &lt;em>&lt;strong>note on metric naming in OpenTSDB&lt;/strong>&lt;/em> this can be very inefficient when data needs to be read again resulting in read performance potentially thousands of times slower than optimal (&amp;lt;100ms). To alleviate this we have modified the original collectd-opentsdb plugin to store all metadata as part of the metric. This gives metric names such as ifHCInBroadcastPkts.sw01.GigabitEthernet0 and very good read performance.&lt;/p>
&lt;p>The modified collectd-opentsdb plugin can be downloaded from our &lt;a class="link" href="https://github.com/PeritusConsulting/collectd-opentsdb" target="_blank" rel="noopener"
>GitHub repository&lt;/a>.&lt;/p>
&lt;p>&lt;a id="MonitoringOpenTSDB">&lt;/a>&lt;/p>
&lt;h4 id="monitoring-opentsdb">Monitoring OpenTSDB
&lt;/h4>&lt;p>To monitor OpenTSDB itself install tcollector as described above on the OpenTSDB server and set &lt;em>&lt;strong>TSD_HOST&lt;/strong>&lt;/em> to &lt;em>&lt;strong>localhost&lt;/strong>&lt;/em> in &lt;em>&lt;strong>/opt/tcollector/startstop&lt;/strong>&lt;/em>.&lt;/p>
&lt;p>You can then go to http://opentsdb-server:4242/#start=1h-ago&amp;amp;end=1s-ago&amp;amp;m=sum:rate:tsd.rpc.received%7Btype=*%7D&amp;amp;o=&amp;amp;yrange=%5B0:%5D&amp;amp;wxh=1200x600 to view a graph of amount of data received in the last hour.&lt;/p>
&lt;p>&lt;a id="Performancecomparison">&lt;/a>&lt;/p>
&lt;h3 id="performance-comparison">Performance comparison
&lt;/h3>&lt;p>Lastly we include a little performance comparison between the latest version of OpenTSDB+HBase+Hadoop, a previous version of OpenTSDB+HBase+Hadoop that we have used for a while as well as rrdcached which ran in production for 4 years at a client.&lt;/p>
&lt;p>The workload is gathering and storing metrics from 150 Cisco switches with 8200 ports/interfaces every 5 seconds. This equals about 15.000 points per second.&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure1.png"
width="735"
height="338"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure1_hu13479656331014733159.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure1_hu12430936122100692572.png 1024w"
loading="lazy"
alt="Figure 1 - Data received by OpenTSDB per second"
class="gallery-image"
data-flex-grow="217"
data-flex-basis="521px"
>&lt;/p>
&lt;p>&lt;a id="Collection">&lt;/a>&lt;/p>
&lt;h4 id="collection">Collection
&lt;/h4>&lt;p>Even though it is not the primary focus, we include some data about collection performance for completeness. Collection is done using the latest version of &lt;a class="link" href="http://collectd.org/" target="_blank" rel="noopener"
>collectd&lt;/a> and the builtin SNMP plugin.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>NB #1:&lt;/strong> There is a &lt;a class="link" href="https://github.com/collectd/collectd/issues/610" target="_blank" rel="noopener"
>memory leak&lt;/a> in the way collectd&amp;rsquo;s SNMP plugin uses the underlying libsnmp library and you might need to schedule a restart of the collectd service as a workaround for that if handling large workloads.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;strong>NB #2:&lt;/strong> Due to &lt;a class="link" href="http://comments.gmane.org/gmane.comp.monitoring.collectd/5061" target="_blank" rel="noopener"
>limitations in the libnetsnmp library&lt;/a> you will run into problems if polling many (1000+) devices with a single collectd instance. A workaround is to run multiple collectd instances with fewer hosts. &lt;a class="link" href="https://github.com/collectd/collectd/issues/610" target="_blank" rel="noopener"
>memory leak&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>Figure 2 shows that collection through SNMP polling consumes about 2200Mhz. We optimized some of the data types and definitions in collectd when moving to OpenTSDB and achieved a 20% performance increase in the polling as seen in Figure 3.&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure2.png"
width="785"
height="616"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure2_hu11520703687699822583.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure2_hu6507568741892228121.png 1024w"
loading="lazy"
alt="Figure 2 - CPU Usage - SNMP polling and writing to RRDcached"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="305px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure3.png"
width="787"
height="625"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure3_hu8758563371351857519.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure3_hu7312580424747238338.png 1024w"
loading="lazy"
alt="Figure 3 - CPU Usage - SNMP polling and sending to OpenTSDB"
class="gallery-image"
data-flex-grow="125"
data-flex-basis="302px"
>&lt;/p>
&lt;p>Writing to the native rrdcached write plugin consumes 1300Mhz while our modified collectd-opentsdb plugin consumes 1450Mhz. It is probably possible to create a much more efficient write plugin with more advanced knowledge of concurrency and using a lower level language such as C.&lt;/p>
&lt;p>&lt;a id="Storage">&lt;/a>&lt;/p>
&lt;h4 id="storage">Storage
&lt;/h4>&lt;p>When considering storage performance we will look at CPU usage and disk IOPS since these are the primary drivers of cost in today&amp;rsquo;s datacenters.&lt;/p>
&lt;h4 id="collectd--rrdcached">collectd + rrdcached
&lt;/h4>&lt;p>CPU usage - 1300Mhz, see Figure 2 above.&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure4.png"
width="661"
height="286"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure4_hu9422720109705725120.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure4_hu2897764835097661964.png 1024w"
loading="lazy"
alt="Figure 4 - Disk write IOPS - Fluctuating between 10 and 170 IOPS during the 1 hour flush period."
class="gallery-image"
data-flex-grow="231"
data-flex-basis="554px"
>&lt;/p>
&lt;h4 id="opentsdb--hbase-096--hadoop-1">OpenTSDB + Hbase 0.96 + Hadoop 1
&lt;/h4>&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure5.png"
width="791"
height="616"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure5_hu17014411734833306734.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure5_hu7917395545853379940.png 1024w"
loading="lazy"
alt="Figure 5 - CPU usage - 1700Mhz baseline with peaks of 7000Mhz during Java Garbage Collection (GC) (untuned)."
class="gallery-image"
data-flex-grow="128"
data-flex-basis="308px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure6.png"
width="800"
height="285"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure6_hu14624923996941413689.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure6_hu10993571528499142312.png 1024w"
loading="lazy"
alt="Figure 6 - Disk write IOPS - 5 IOPS average with peaks of 25 IOPS during Java GC. We also see that disk read IOPS are much higher and this is due to regular compaction of the database and can be tuned. Reads in general can be reduced by increasing caching with more RAM if necessary."
class="gallery-image"
data-flex-grow="280"
data-flex-basis="673px"
>&lt;/p>
&lt;h4 id="opentsdb--hbase-098--hadoop-2">OpenTSDB + HBase 0.98 + Hadoop 2
&lt;/h4>&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure7.png"
width="925"
height="478"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure7_hu8040901172863881962.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure7_hu1188736229991262180.png 1024w"
loading="lazy"
alt="Figure 7 - CPU usage - 1200Mhz baseline with peaks of 5000-6000Mhz during Java GC (untuned)."
class="gallery-image"
data-flex-grow="193"
data-flex-basis="464px"
>&lt;/p>
&lt;p>&lt;img src="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure8.png"
width="767"
height="395"
srcset="https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure8_hu10785440613668549146.png 480w, https://demo.stack.jimmycai.com/p/next-generation-monitoring-with-opentsdb/Figure8_hu12590607926021679589.png 1024w"
loading="lazy"
alt="Figure 8 - Disk write IOPS - &amp;lt; 5 IOPS average with peaks of 25 IOPS during Java GC. Much less read IOPS during compaction compared to HBase 0.96."
class="gallery-image"
data-flex-grow="194"
data-flex-basis="466px"
>&lt;/p>
&lt;p>&lt;a id="Conclusion">&lt;/a>&lt;/p>
&lt;h4 id="conclusion">Conclusion
&lt;/h4>&lt;p>Even without tuning, a single instance OpenTSDB installation is able to handle significant amounts of data before running into IO problems. This comes at a cost of CPU, currently OpenTSDB will consume &amp;gt; 300% the amount of CPU cycles compared to rrdcached for storage. But this is offset by a 85-95% reduction in disk load. In absolute terms for our particular set up (one 2 year old HP DL360p Gen8 running VMware vSphere 5.5) CPU usage increased from 15% to 25% while reducing IOPS load from 70% to &amp;lt; 10%.&lt;/p>
&lt;p>&lt;em>Fine tuning of parameters (such as Java GC) as well as detailed analysis of memory usage is outside the scope of this brief paper and detailed information may be found elsewhere (&lt;a class="link" href="https://hbase.apache.org/book/performance.html" target="_blank" rel="noopener"
>51&lt;/a>,&lt;a class="link" href="http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html" target="_blank" rel="noopener"
>52&lt;/a>,&lt;a class="link" href="http://www.cubrid.org/blog/textyle/428187" target="_blank" rel="noopener"
>53&lt;/a>) for those interested.&lt;/em>&lt;/p>
&lt;hr></description></item></channel></rss>